var tipuesearch = {"pages":[{"title":"Contact","text":"Electrical Engineer with multi-faceted background encompassing software engineering, control systems development & buildings design/construction. Developed specialties in interactive data science & visualisation, desktop & web full stack development and technical writing. Proven ability to work apart of multi-disciplinary teams to design, develop, and integrate software solutions to increase efficiency & quality and reduce human-error. Curriculum Vitae Always looking to meet new people & help let computers do the hard work for you. This site is just meant to function as half blog, half showcase of a few things I like to do. Please look around, and if you'd like to contact me about anything at all, please do! Your Name: Your Email: Message: Send","tags":"misc","url":"https://jackmckew.dev/pages/contact.html","loc":"https://jackmckew.dev/pages/contact.html"},{"title":"CV/Professional","text":"Electrical Engineer with multi-faceted background encompassing software engineering, control systems development & buildings design/construction. Developed specialties in interactive data science & visualisation, desktop & web full stack development and technical writing. Proven ability to work apart of multi-disciplinary teams to design, develop, and integrate software solutions to increase efficiency & quality and reduce human-error. Curriculum Vitae Honour's Thesis: \"In Home Appliance Scheduler Using Home Area Network\" Experience My CV and honours thesis are linked below (PDF download). My professional experience thus far is: Current Founder & Principal Consultant at Cyberlytica Freelance digital services company specialising in data services. Services include data analytics, visualisation & automation. Full stack development, packaging and distribution of solutions. Current Graduate Electrical Engineer at AECOM (since Jan 2019) 2017 - 2018 Undergraduate Electrical Engineer at AECOM Development of scientific computing software for environmental data sets. Saved approximately 900 working hours through automation. Pioneered and lead an Australia wide Microsoft Excel training session. Lighting design, lightning protection system design, PLC network design. Reticulation design, switchboard & switch room design. Construction support for defence & education projects and site inspections. Electrical drafting in AutoCAD and Revit. Tools: Python, git, VBA, AutoCAD, Revit, MySQL, Plotly, Bokeh, Pandas 2016 - 2018 Undergraduate Electrical Engineer at Hunter H2O PLC, SCADA & HMI design for water network infrastructure. Telemetry system & radio network design. Cable schedules & electrical protection schemes. Tools: Python, SCADAPack Workbench, ViewX, Unity Pro 2014 - 2018 Timber Expert & Forklift Operator at Bunnings Warehouse Used safety procedures to reduce risk within an unrestricted workzone. Detailed advice to customers on building materials & techniques. Customer service, cashier, in store loss prevention. Previous Projects Air Quality Toolkit AECOM Internal Developed, packaged and distributed software to automate tasks reducing a week long task to seconds. Developed re-sampling tooling such that high frequency atmospheric data can be used in models. Built interactive data visualisation tooling to communicate data with clients. Responsibilities : Full stack development, packaging, distribution Vulnerability Mapping Tool Hunter Joint Organisation By utilising statistical data and region-specific data, Jack was able to develop software that determines the vulnerability of communities. By visualising this data in an online, interactive form, this was then distributed to the relevant councils to assist in disaster relief planning. Automated GIS analysis to model vulnerability of communities to natural disasters, saving time and enhancing reproducibility. Compiled various sources of statistical and functional information to model vulnerability. Developed interactive visualisation to assist data driven decision making. Responsibilities : Statistical analysis, automated GIS, data visualisation Mental Health Asset Condition Assessments Victoria Health Worked a part of a multidisciplinary team to audit active mental health wards across Victoria to allow the government to make data driven decisions on maintenance and upgrades. Responsibilities : Building services (electrical \\& fire) audits, data automation Honeysuckle Campus Development University of Newcastle Honeysuckle Campus Jack is part of a multidisciplinary team undertaking the design of the new UoN 1A building in the new Honeysuckle campus. This includes lighting, power, communications, dry fire, mechanical, structural and hydraulics. Jack is responsible for providing electrical engineering support during the design phase of this project Delivered electrical designs & drafting work in a multidisciplinary team for the new UoN 1A building in the new Honeysuckle campus. Worked a part of a multidisciplinary team undertaking the design of the new UoN 1A building in the new Honeysuckle campus. Responsibilities: Lighting design, electrical design \\& electrical drafting in Revit In Home Appliance Scheduler Using Home Area Network Engineering Thesis Designed, developed and constructed a home area network capable of monitoring and controlling appliances in accordance with an optimized schedule. Responsibilities : Embedded development, networking, databases, algorithm design, technical writing, data analysis, Android app development","tags":"misc","url":"https://jackmckew.dev/pages/cv-professional.html","loc":"https://jackmckew.dev/pages/cv-professional.html"},{"title":"Pelican and Javascript - Bouncing Balls in Canvas","text":"Today let's look into building a visualisation of some bouncing balls with Javascript. The inspiration for building this comes from Harry Stevens over at the Washington Post for his amazing piece of data journalism around the coronavirus . Here is a gif of the current version of my bouncing balls using the Canvas API: As soon as I read that article personally, I thought of a few ways to extend the analysis, and such this post was born. A few of the ideas that I've had are: Adding a 'chance' to transmit the infection (eg, 50% of all collisions transmit). This could be a symbolic way of seeing how much of an impact wearing PPE makes on virus outbreaks. Having hot spots in which the circles will be attracted towards. If you've got any ideas of some new parameters that might be interesting to see, leave a comment! Before I could build a similar visualisation, I had to figure out how I could integrate Javascript into this blog. Luckily, the amazing community behind Pelican has built a plethora of plugins to choose from. Pelican Javascript ended up being the plugin of choice. By using this in combination with Pelican Autostatic , this allowed for the article-centric resources and also ensures that a list of source javascript files was included at the end of each post. Without further ado, here is my current implementation of bouncing balls. There is a few bugs in that sometimes they fly out of the box and some get stuck together forever, if anyone has any ideas on how I could fix it, please let me know in the comments! This visualisation is completed using the Canvas API . To interface with the canvas element in HTML, we use the DOM . Another option of doing this is using d3js , for which my understanding is that since Canvas is inbuilt, it is much more powerful at painting more objects. We then instantiate an amount of objects (which will be the balls) within Javascript with the following parameters: radius angle speed colour position These are then all pushed into an array for which we will loop through checking collision with horizontal walls, vertical walls and the other balls. To have a visualisation which moves along with time, we can use the setInterval method (which is apart of the Window object aka the browser). We will be painting the canvas at each interval that we have set with this method. Now we need to paint our canvas with the balls. To do this, we start by clearing the canvas each time, so the last 'timestamp' doesn't stay on our canvas (unless you want to?). The balls are then painted onto the canvas using the canvas arc() Method . At each time step, we need to do a series of steps: Clear the canvas (using canvas clearRect() Method ). The following steps are applied to each individual ball. Check if the ball is colliding with the walls or another ball. If they are, reflect them accordingly. (Since our balls are all the same 'weight', we use a mass elastic collision , which means both balls angle will be rotated by 90 degrees. Move our ball according to the angle it is facing, by it's speed. Paint the ball on canvas. Repeat for all balls. Javascript Source(s): Bouncing_Balls.js","tags":"Javascript","url":"https://jackmckew.dev/pelican-and-javascript-bouncing-balls-in-canvas.html","loc":"https://jackmckew.dev/pelican-and-javascript-bouncing-balls-in-canvas.html"},{"title":"Packaging Python Packages with Poetry","text":"Packaging Python code to easily share code with others! Building upon a previous post on \"How many words have I written in this blog?\" , let's dive into how to share this code with other on PyPI and integrate into this website with a 'word ticker' which updates whenever a new post is uploaded. What is Poetry Package Structure __init__.py Files Wordsum - My First Package Wordsum Package Structure User Interaction Publishing to PyPI Integrating Wordsum Into This Website Update requirements.txt Update pelicanconf.py Update base.html If you are using Python, you've most likely used pip, conda or similar to install packages from other developers such that you aren't reinventing the wheel. This functionality is by far one of my favourite features of the language. If you aren't already aware, (most) of these packages you install with pip live on PyPI , the Python Package Index. This post is for how to structure a package in Python with Poetry and publish it on PyPI (I was amazed how easy this was). What is Poetry A quote from the creator of Poetry : I built Poetry because I wanted a single tool to manage my Python projects from start to finish. I wanted something reliable and intuitive that the community could use and enjoy. - Sébastien Eustace In it's essence, Poetry manages the Python project workflow so you don't have to. Same as venv, virtualenv, conda create virtual environments for a project, Poetry will also create a virtual environment to go with your project. If you are working in VS Code, the Python extension doesn't automatically detect the virtual environment location that Poetry defaults to. To tell VS Code where the virtual environments live for Poetry head to Settings > python.venvFolders and add C:\\\\Users\\\\USERNAME\\\\AppData\\\\Local\\\\pypoetry\\\\Cache\\\\virtualenvs for Windows. Package Structure Python packages require a standard structure (albeit lenient), which Poetry sets up for you when a project is initialized. If we run poetry new test_package we will end up with the structure: 1 2 3 4 5 6 7 8 test-package +-- pyproject.toml +-- README.rst +-- test_package | +-- __init__.py +-- tests | +-- __init__.py | +-- test_test_package.py Inside the top directory of our package we have 2 folders and 2 files. |File|Use| |--|--| |pyproject.toml|Contains all the information about your package, dependancies, versions, etc.| |README.rst|Readme file as to what the project does, any instructions of use, etc (see Pandas-Bokeh for a great example).| |test_package|This is where all our Python code will live for the project.| |tests|Following test driven development, this is where any automated tests live to make sure the code runs as expected.| __init__.py Files What are all these __init__.py files and what are they there for? To be able to import code from another folder, Python requires to see an __init__.py inside a folder to mark it as a package. If we create a function inside our test_package folder: 1 2 3 +-- test_package | +-- __init__.py | +-- function .py Now users can use: import test_package.function or from test_package import function Wordsum - My First Package If you have read this blog previously, I did a post on answering the question \"How many words have I written in this blog?\" which you can reach at: https://jackmckew.dev/counting-words-with-python.html . This was great, I had wrote 2 functions which count how many words were inside markdown files & Jupyter notebooks. Following this, I had a great idea, why not make a 'ticker' of how many words have been written in this blog and display this on the website. Making sure that whenever a new post is added the 'word ticker' increments by how many words were in that post. This in the inspiration behind Wordsum which is also available on PyPI . Meaning you can install it with: 1 pip install wordsum Wordsum Package Structure To make the two functions more extensible, the two functions were further broken into smaller functions and contained in their own 'internal' package (folder). The basic structure we ended up with was: 1 2 3 4 5 6 7 8 9 10 11 12 13 wordsum +-- word_sum.py +-- __init__.py +-- _file_types | +-- __init__.py | +-- jupyter.py | +-- markdown.py +-- _io | +-- __init__.py | +-- read_files.py +-- _util | +-- __init__.py | +-- file_locate.py Now the _ prefix to the folders is to nominate that functions contained in these folders are internal to the package. Meaning the user shouldn't be able to access these functions. The main functions of the package are kept within word_sum.py (which uses the functions in the _xxx folders). User Interaction To make the main functions within word_sum.py accessible to users of the package we can import them in the 'top' __init__.py of the wordsum package. 1 2 3 4 5 __version__ = '0.1.3' from wordsum.word_sum import count_words from wordsum.word_sum import list_supported_formats from wordsum.word_sum import count_files This will allow users to interact with the package like: 1 2 3 4 5 import wordsum if __name__ == \"__main__\" : print ( wordsum . count_words ( './example_files' ,[ '.md' , '.ipynb' ])) wordsum . list_supported_formats () Publishing to PyPI Since we've used Poetry with the development of this package, our pyproject.toml should be a bit more fleshed out. Wordsum's pyproject.toml ended up as: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 [tool.poetry] name = \"wordsum\" version = \"0.1.3\" description = \"Counting words within a folder of files recursively.\" readme = \"README.md\" repository = \"https://github.com/JackMcKew/wordsum\" authors = [\"JackMcKew <jackmckew2@gmail.com>\"] [tool.poetry.dependencies] python = \"&#94;3.6\" mypy = \"&#94;0.770\" nbformat = \"&#94;5.0.4\" black = \"&#94;19.10b0\" flake8 = \"&#94;3.7.9\" [tool.poetry.dev-dependencies] pytest = \"&#94;5.2\" black = {version = \"&#94;19.10b0\", allow-prereleases = true} [tool.poetry.urls] issues = \"https://github.com/JackMcKew/wordsum/issues\" [build-system] requires = [\"poetry>=0.12\"] build-backend = \"poetry.masonry.api\" All that is left to do is to sign up for an account on PyPI and run: 1 poetry publish This will ask for your PyPI credentials, build the package (a step done by setuptools previously) and upload the package for you. Now users can install your package with: 1 pip install wordsum Integrating Wordsum Into This Website Previously I've written a blog post on how I moved from Wordpress to Pelican ( https://jackmckew.dev/migrating-from-wordpress-to-pelican.html ). This goes into detail about how this site utilizes a continuous integration (CI) service (TravisCI), that rebuilds the site each time a new file is pushed to the GitHub repository. Following this, Netlify fires up and pushes the freshly built website out to the internet. This site uses the theme Flex as a basis, except a local 'fork' of Flex is retained in the repository such that I can make edit without disrupting other users of Flex. So there was only 3 files that I needed to edit: pelicanconf.py , requirements.txt and a html file for the theme. pelicanconf.py contains all the instructions to provide to pelican when building the site, requirements.txt contains the list of packages required for TravisCI to use and the template html file is how it is to represented on the web. Update requirements.txt First off we add wordsum to the virtual environment for the project and freeze it within requirements.txt with 1 2 pip install wordsum pip freeze requirements.txt Since most CI services use Ubuntu as a base, pywin32 is installed be default as a dependancy on wordsum, this can be removed with pip uninstall pywin32 Update pelicanconf.py This file contains the code that will run when pelican content is called upon the folder to build this website. To interface with wordsum we add the code: 1 2 3 4 import wordsum WORD_TICKER = wordsum . count_words ( './content' ,[ '.md' , '.ipynb' ]) WORD_TICKER = f \" { WORD_TICKER : , } \" This creates a variable WORD_TICKER that can be used later on to show the number of words counted across all markdown files & Jupyter notebooks. The line WORD_TICKER = f\"{WORD_TICKER:,}\" adds thousand separators to numbers in Python with f-strings. For example this converts 123456 to 123,456. Update base.html Now we just need to show our word ticker on the website. In the Flex theme, all pages inherit from a base.html file. To squeeze our new metrics onto the page we add the lines: 1 2 3 4 {% if WORD_TICKER %} < p > Number of Posts: {{ articles|count }} </ p > < p > Number of Words: {{ WORD_TICKER}} </ p > {% endif %} First we check if the variable WORD_TICKER is used to ensure we only include the metrics if the variable has been set. Followed by two paragraph markers which will show: How many posts are on the website How many words are used in all of those posts","tags":"Python","url":"https://jackmckew.dev/packaging-python-packages-with-poetry.html","loc":"https://jackmckew.dev/packaging-python-packages-with-poetry.html"},{"title":"Counting Words with Python","text":"{% notebook 2020/markdown-word-counter/notebooks/markdown-counter.ipynb %}","tags":"Python","url":"https://jackmckew.dev/counting-words-with-python.html","loc":"https://jackmckew.dev/counting-words-with-python.html"},{"title":"Python Bytes Awesome Package List","text":"Python Bytes is a weekly, short & sweet podcast by Michael Kennedy & Brian Okken . After having the podcast recommended numerous times by friends & colleagues, I decided to download every episode thus far on the 14th of September 2019. Over the next 174 days, whenever I was commuting, I'd listen to 171 episodes of Python bytes, learnt a stack of new things and found new amazing python packages. IMPORTANT NOTE : This list has been moved to it's own repository on Github so other listeners can add other awesome packages to this list! Pull requests are 100% open and I'm looking forward to seeing your contributions! https://github.com/JackMcKew/awesome-python-bytes This post is intended to list out the packages I'd noted down & their application. Total disclaimer, I haven't tried out all of these packages personally and I'm certain there is a plethora of other packages mentioned that I have not captured here, please reach out if theres anything to add! I've attempted to sort these into a directory of sorts pending on what you're interested in looking at, and whether I found out about them through Python Bytes or elsewhere (they will have a link to the episode if directly from Python Bytes ). Table Of Contents Web Development Wagtail Wooey Anvil Vue.py GeoDjango Django Bootcamp Security Osmedeus Mongo Audit Data Science Great Expectations PDF Plumber PyJanitor Pandas Vet NB2XLS Data Visualisation Pylustrator Chartify Panel Holoviz Cartoframes Sand Dance Machine Learning PyTorch Yellow Brick Thinc Keras Gym Spinning up Jax Gensim Databases GeoAlchemy Sandman 2 Command Line Interfaces (CLIs) Python Fire Clize Typer Guided User Interfaces (GUIs) Gooey Eel GUI QUICK Great Examples of Tkinter Python Development Attrs PyOxidiser Python Date Utils Pycel Doc Assemble Game Development Panda3D PursuedPyBear Interesting Tidbits Web Development Wagtail https://pythonbytes.fm/episodes/show/70/have-you-seen-my-log-it-s-cute Wagtail is a content management system (CMS) (like Wordpress), written in Python , based off Django . Gallery of sites made with wagtail Wooey https://pythonbytes.fm/episodes/show/62/wooey-and-gooey-are-simple-python-guis A Django app that creates automatic web UIs for Python scripts. Live example at: https://wooey.herokuapp.com/ Anvil https://pythonbytes.fm/episodes/show/106/fluent-query-apis-on-python-collections Full stack web apps with nothing but Python . Vue.py https://pythonbytes.fm/episodes/show/140/becoming-a-10x-developer-sorta use Vue.js with pure Python vue.py provides Python bindings for Vue.js . It uses brython to run Python in the browser. Live example at: https://stefanhoelzl.github.io/vue.py/examples/todo_mvc/ GeoDjango https://docs.djangoproject.com/en/3.0/ref/contrib/gis/ GeoDjango intends to be a world-class geographic Web framework. Its goal is to make it as easy as possible to build GIS Web applications and harness the power of spatially enabled data. Django Bootcamp https://github.com/vitorfs/bootcamp Bootcamp is an open source enterprise social network of open purpose, on which you can build for your own ends. Example at: https://trybootcamp.vitorfs.com/ Security Osmedeus Fully automated offensive security framework for reconnaissance and vulnerability scanning Mongo Audit https://mongoaud.it/ mongoaudit is an automated pentesting tool that lets you know if your MongoDB instances are properly secured Data Science Great Expectations https://pythonbytes.fm/episodes/show/115/dataclass-csv-reader-and-nina-drops-by Great Expectations is a leading tool for validating, documenting, and profiling, your data to maintain quality and improve communication between teams. PDF Plumber https://pythonbytes.fm/episodes/show/26/how-have-you-automated-your-life-or-cli-with-python Plumb a PDF for detailed information about each char, rectangle, line, et cetera — and easily extract text and tables. PyJanitor https://pythonbytes.fm/episodes/show/108/spilled-data-call-the-pyjanitor pyjanitor is a project that extends Pandas with a verb-based API, providing convenient data cleaning routines for repetitive tasks. Pandas Vet https://pythonbytes.fm/episodes/show/167/cheating-at-kaggle-and-uwsgi-in-prod pandas-vet is a plugin for flake8 that provides opinionated linting for pandas code. NB2XLS https://github.com/ideonate/nb2xls Convert Jupyter notebooks to Excel Spreadsheets (xlsx), through a new 'Download As' option or via nbconvert on the command line. Data Visualisation Pylustrator https://pythonbytes.fm/episodes/show/137/advanced-python-testing-and-big-time-diffs Pylustrator offers an interactive interface to find the best way to present your data in a figure for publication. Added formatting an styling can be saved by automatically generated code. To compose multiple figures to panels, pylustrator can compose different subfigures to a single figure. Chartify https://pythonbytes.fm/episodes/show/109/cpython-byte-code-explorer Chartify is a Python library that makes it easy for data scientists to create charts. Panel Holoviz Panel provides tools for easily composing widgets, plots, tables, and other viewable objects and controls into control panels, apps, and dashboards. Panel works with visualizations from Bokeh , Matplotlib , HoloViews , and other Python plotting libraries, making them instantly viewable either individually or when combined with interactive widgets that control them. Panel works equally well in Jupyter Notebooks, for creating quick data-exploration tools, or as standalone deployed apps and dashboards, and allows you to easily switch between those contexts as needed. Examples at: https://panel.holoviz.org/ Cartoframes A Python package for integrating CARTO maps, analysis, and data services into data science workflows. Python data analysis workflows often rely on the de facto standards pandas and Jupyter notebooks. Integrating CARTO into this workflow saves data scientists time and energy by not having to export datasets as files or retain multiple copies of the data. Instead, CARTOframes give the ability to communicate reproducible analysis while providing the ability to gain from CARTO's services like hosted, dynamic or static maps and Data Observatory augmentation. Sand Dance https://github.com/microsoft/SandDance Visually explore, understand, and present your data. Machine Learning PyTorch https://pythonbytes.fm/episodes/show/80/dan-bader-drops-by-and-we-found-30-new-python-projects Tensors and Dynamic neural networks in Python with strong GPU acceleration Yellow Brick https://pythonbytes.fm/episodes/show/74/contributing-to-open-source-effectively Yellowbrick extends the Scikit-Learn API to make model selection and hyperparameter tuning easier. Under the hood, it's using Matplotlib . Thinc https://pythonbytes.fm/episodes/show/167/cheating-at-kaggle-and-uwsgi-in-prod A refreshing functional take on deep learning, compatible with your favorite libraries. From the makers of spaCy , Prodigy & FastAPI Keras Gym https://github.com/KristianHolsheimer/keras-gym Plug-n-play reinforcement learning with OpenAI Gym and Keras Spinning up https://spinningup.openai.com/en/latest/ Deep reinforcement learning educational resource Jax https://github.com/google/jax JAX is Autograd and XLA, brought together for high-performance machine learning research. Autograd & XLA are both optimisers, this package makes the applications run quicker Gensim https://radimrehurek.com/gensim/ Gensim is an open-source library for unsupervised topic modeling and natural language processing, using modern statistical machine learning. Databases GeoAlchemy https://pythonbytes.fm/episodes/show/77/you-don-t-have-to-be-a-workaholic-to-win Using SQLAlchemy with Spatial Databases. GeoAlchemy 2 provides extensions to SQLAlchemy for working with spatial databases. GeoAlchemy 2 focuses on PostGIS . PostGIS 1.5 and PostGIS 2 are supported. Sandman 2 https://github.com/jeffknupp/sandman2 Automatically generate a RESTful API service for your legacy database. No code required! Command Line Interfaces (CLIs) Python Fire https://pythonbytes.fm/episodes/show/17/google-s-python-is-on-fire-and-simon-says-you-have-cpu-load-pythonically `Python Fire is a library for automatically generating command line interfaces (CLIs) from absolutely any Python` object. Clize https://pythonbytes.fm/episodes/show/167/cheating-at-kaggle-and-uwsgi-in-prod Clize is an argument parser for Python . You can use Clize as an alternative to argparse if you want an even easier way to create command-line interfaces. Typer https://pythonbytes.fm/episodes/show/164/use-type-hints-to-build-your-next-cli-app Typer , build great CLIs. Easy to code. Based on Python type hints. Guided User Interfaces (GUIs) Gooey https://pythonbytes.fm/episodes/show/62/wooey-and-gooey-are-simple-python-guis I personally love Gooey and have it installed in almost every project lately. Gooey turns (almost) any Python command line program into a full GUI application with one line. I have also done a tutorial blog post on Gooey as well at: https://jackmckew.dev/making-executable-guis-with-python-gooey-pyinstaller.html Eel GUI https://pythonbytes.fm/episodes/show/61/on-being-a-senior-engineer Eel is a little Python library for making simple Electron-like offline HTML/JS GUI apps, with full access to Python capabilities and libraries. QUICK https://pythonbytes.fm/episodes/show/166/misunderstanding-software-clocks-and-time A real quick GUI generator for click . Inspired by Gooey , the GUI generator for classical Python argparse -based command line programs. Great Examples of Tkinter https://pythonbytes.fm/episodes/show/63/we-re-still-on-a-desktop-gui-kick A few great examples of what is possible with Tkinter. - https://github.com/victordomingos/PT-Tracking/ - - https://github.com/victordomingos/RepService/ - - https://github.com/victordomingos/ContarDinheiro.py - Python Development Attrs https://pythonbytes.fm/episodes/show/11/django-2.0-is-dropping-python-2-entirely-pipenv-for-profile-functionality-and-pythonic-home-automation Python Classes Without Boilerplate PyOxidiser https://pythonbytes.fm/episodes/show/114/what-should-be-in-the-python-standard-library PyOxidizer is a utility for producing binaries that embed Python . The over-arching goal of PyOxidizer is to make complex packaging and distribution problems simple so application maintainers can focus on building applications instead of toiling with build systems and packaging tools. Python Date Utils https://pythonbytes.fm/episodes/show/136/a-python-kernel-rather-than-cleaning-the-batteries The dateutil module provides powerful extensions to the standard datetime module, available in Python . Pycel https://pythonbytes.fm/episodes/show/171/chilled-out-python-decorators-with-pep-614 A library for compiling excel spreadsheets to Python code & visualizing them as a graph Doc Assemble docassemble is a free, open-source expert system for guided interviews and document assembly. It provides a web site that conducts interviews with users. Based on the information gathered, the interviews can present users with documents in PDF, RTF, or DOCX format, which users can download or e-mail. Game Development Panda3D https://pythonbytes.fm/episodes/show/116/so-you-want-python-in-a-3d-graphics-engine Panda3D is an open-source, completely free-to-use engine for realtime 3D games, visualizations, simulations, experiments PursuedPyBear PursuedPyBear, also known as ppb, exists to be an educational resource. Most obviously used to teach computer science, it can be a useful tool for any topic that a simulation can be helpful. Interesting Tidbits There was one episode that referenced some amazing examples of GUIs built in Tkinter, unfortunately I have been unable to find it again. My note that I had down was 63 GUIs in Tkinter . EDIT: Thank you Anton Alekseev for helping me find this! Tkinter Examples Using --prompt to name your virtualenv for easy identification later on is something I use widely now. https://pythonbytes.fm/episodes/show/168/race-your-donkey-car-with-python Python Graph Gallery is an amazing resource for examples of already made data visualisations. Type hints for busy programmers is a great resource for understanding what type hints are and why you should use them. https://pythonbytes.fm/episodes/show/160/your-json-shall-be-streamed","tags":"Python","url":"https://jackmckew.dev/python-bytes-awesome-package-list.html","loc":"https://jackmckew.dev/python-bytes-awesome-package-list.html"},{"title":"Book Review: Courage To Be Disliked","text":"The courage to be disliked is a book around the work of a 19th century psychologist who determined that happiness lies within the individual and doesn't depend on the past whatsoever. I really enjoyed reading this book, and would recommend it to anyone at all walks & stages of life. The book is structured as a dialogue between a curious youth and a philosopher that the youth had sought out to answer some questions. This was intentional by the authors to try and answer questions that the reader could come up with while reading, I found this really effective and found myself having a question and then it immediately would come up moments later while reading. What I took out of this book was: Comparing yourself to others is only of detriment to your own mental health and well being There is a dangerous belief in believing your past defines you & your future Competition is mostly fictional and that life isn't a race, everyone walks their own path Other people don't pay as much attention to you as you may possibly believe All problems in the world are from interpersonal relationships Not to take on anyone elses tasks, and not let anyone take on your tasks Don't focus on the past or the future, focus on the here and now Overall, the book is broken into five 'chapters' resembling separate nights in which the youth would visit the philosopher, discuss topics, leave with concepts to contemplate and return on the next night with questions. First Night Focusing on the past helps nobody The key point that I took away from this chapter was most things are generally explained in the world as a cause & effect. The standpoint of the psychologist is that rather than focusing on how something in the past affected an outcome, it's rather in the view point of the current goals. This chapter goes into an example of a recluse youth who wouldn't go out for anything, rather preferring to stay at home, inside. The youth argues that this could be due to his upbringing and things that had happened in the recluse's life, while the philosopher returns with the reasoning behind the recluse's nature is because of a possible current goal that person may have. The philosopher argues that the recluse youth may desire attention and affection from their family, and thus by not going into the outside world, that protects the youths situation from changing. Putting it in the frame of which the reasoning is dependent on the current goal of the person in question. Second Night All problems are interpersonal relationship problems This chapter I personally strongly agreed with. The viewpoint in which all problems derive from one human interaction with another and how this is closely tied with the 'feeling of inferiority' causing dilemmas in the world. I resonate with the explanation of the feeling of inferiority being when one compares themself to another, and feels as though they are not equal and the other is superior in one form or another. All of these feeling of inferiority, while they are common, are subjective. By recognising these feelings as subjective, this puts the power to control how we perceive them in the hands of the individual. Although they can easily turn from inferiority to superiority, which can be as detrimental. Third Night Don't take on other people's tasks, and don't let other people take your tasks This chapter delved into a way of life in which one does not live to satisfy the expectations of others and a viewpoint which denies the desire for recognition for others. These concepts are portrayed through the situation in which a child tries to live up to the parents expectations, specifically the scenario where the child tries to live their life along the lines set by the parents. A common scenario of this is what to do for a living, where to go to school and many more factors. The psychological concept to combat this is by denying to live by the expectations of others and not doing things for the sake of receiving recognition from others later on. By seeking the recognition of others, it is as if trying to satisfy the expectations of everyone which can only lead to disappointment. But if you lead such a selfish life, will that not only cause more interpersonal relationship problems? The psychologist standpoint is this in to 'not behave without regards for others' and this is achieved through the separation of tasks. Separation of tasks This concept is portrayed through a child-parent relationship and the topic of studying. A common scenario for this is a heavy handed approach by forcing the child to study either by incentive or punishment, which inevitably may lead to the child resenting the practice which is perceived as favourable. The solution that is presented is if the parent leaves the task in the child's responsibility, but only seeks to provide assistance, if and when the child seeks it. Which hopefully leads to the child seeing benefit in their own actions and enjoying a beneficial action. Fourth Night Self value, horizontal relationships and individualism Self Value By delving into a typical 'self-centred' viewpoint in which someone is the centre of their own world and the viewpoint in which everyone else is to live up to their expectations (detrimental as mentioned earlier), the philosopher attempts to present the standpoint of living for the benefit of the community. Rather than looking at things with expectations for others, by living as if how your own personal actions can be of benefit for others subsequently can lead to a more fulfilling life. Horizontal Relationships Once the concept of living for the benefit of the community (thus resolving interpersonal relationship problems) is achieved, this is further enhanced by the concept of horizontal relationships, wherein one is not above or below any other. As soon as one puts themself in a 'vertical' relationship, wherein one is believed to be superior/inferior to the other, this is only going to cause further interpersonal relationship problems and the cycle continues. By rather living as if everyone is equal, and lives to the benefit of others, this concentrates a fulfilling lifestyle and is beneficial for everyone involved. Fifth Night Live for the now, not the past or the future When you take in all the points & concepts raised above, this are only enhanced further when you take in the concept that the past & the future should not matter whatsoever on the actions you can change now. Therefore by living for the moment, not letting the past be the cause and effect of who you are, living for the benefit of others and treating everybody equal, I find these great steps to leading a more fulfilling, happy life.","tags":"Book Reviews","url":"https://jackmckew.dev/book-review-courage-to-be-disliked.html","loc":"https://jackmckew.dev/book-review-courage-to-be-disliked.html"},{"title":"3D Gradient Descent in Python","text":"{% notebook 2020/3d-gradient/notebooks/3d-gradient.ipynb cells[20:] %}","tags":"Python","url":"https://jackmckew.dev/3d-gradient-descent-in-python.html","loc":"https://jackmckew.dev/3d-gradient-descent-in-python.html"},{"title":"Getting Started in Python","text":"Resources & tips for getting started in Python Glossary One of the most useful skills in my opinion in all walks of life is the ability to find information & answers. A very useful tool for doing this is the internet & search engines like Google. In my opinion being able to search the internet for answers to your programming questions is a skill in itself, knowing the right words or phrases to use is a massive part of that, so I will begin with a glossary of terminology for things that may make it easier to answer your questions. Term Meaning Directory In Windows, a directory is commonly referred to as a folder. Module A module in software is a piece of software responsible for one specific functionality. In Python, they are separate files with a .py extension containing functions, classes and/or variables. Package A package in Python is a directory of packages and/or modules. It is more or less a directory with one special __init__.py file inside it. Runtime / Environment The runtime or environment in Python is the context in which the code runs. Variable A variable is a location that stores data temporarily within a program which can be modified, stored and/or displayed. Object An object is a combination of data, whether in variables or structures. Class A class refers to a set of related objects with common properties. Function A named section of a program that has a specific purpose. Algorithms A set of guidelines which describe how to perform a task. API Application Programming Interface, is a structure in which you can interact with another piece of software. Bug A bug is an unexpected error which causes a malfunction. Frontend The front-end is how the user interacts with the software, like Windows. Backend The back-end is all the tasks that happen in the background with the user's interaction Argument An argument is a value or variable passed into a function. Boolean Boolean is an expression used with True or False statements, operators include AND, OR, NOT. Command Line Interface The user interface based on text, typically used on Windows through cmd.exe (Command Prompt) Shell A shell is a program that takes in commands (like Command Prompt) to interact with the operating system. Compilation Compilation is converting programming (code) into instructions the computer can understand. List An ordered sequence of elements, denoted with square brackets [ ] Dictionary An ordered (in 3.6+) collection of data, with a key and a value, denoted with curly brackets Exception If a situation comes up that the program cannot handle, it'll throw an exception. Loop A loop allows code to be executed repeated, either with a for (finite amount of times) or while (indefinite). Iteration Repetition of a process, you iterate over a loop numerous times. https://github.com/ A website/service where lots of open source packages live and great resource for finding examples of real-use code. Editor There is so many editors out there it is difficult to find the right one for you. Mu https://codewith.mu/ Mu is a simple Python editor, designed for beginners. VS Code https://code.visualstudio.com/ While not specifically for Python, it has an amazing Python extension and is my go-to editor. Sublime Text https://www.sublimetext.com/ A self proclaimed \"A sophisticated text editor for code, markup and prose\". PyCharm https://www.jetbrains.com/pycharm/ A more fleshed out full suite for professional developers. All the bells and whistles included. Resources There is an abundance of resources on the internet for getting started with Python (including this one!), these are my personal recommendations. Automate The Boring Stuff http://automatetheboringstuff.com/ Computers are made to do the boring things for us, so make the most of it! This book goes through many practical examples of how you can use Python to automate the boring stuff out of your life. Code Academy https://www.codecademy.com/catalog/language/python While the Python 3 course on Code Academy isn't free, the Python 2.7 version is and you can always learn the differences later on. Python Bytes Podcast https://pythonbytes.fm/ Weekly half an hour podcast on exciting packages & tools in the Python space. Great to listen to on your commute! Github https://github.com/ Github is a website/service where lots of open source projects & packages live. Lots of information can be found here on the projects themselves and in the issues raised/answered previously. Cheat Sheet If you're anything like me, you'd appreciate a nicely laid out cheat sheet. https://perso.limsi.fr/pointal/_media/python:cours:mementopython3-english.pdf Real Python https://realpython.com/ Real Python is an amazing website, with lots of beginner friendly tutorials on more topics than I can imagine. Packages The best part about Python is the community and abundance of free, open source packages that you can utilise to not re-invent the wheel when solving problems. I did a blog post on useful packages in 2019, check it out at: https://jackmckew.dev/episode-10-python-package-cheat-sheet.html If there is anything I have missed, please feel free to drop a comment below and I will update this post!","tags":"Python","url":"https://jackmckew.dev/getting-started-in-python.html","loc":"https://jackmckew.dev/getting-started-in-python.html"},{"title":"Book Review: The Power Of Habits","text":"Power of Habits by Charles Duhigg was one of the most useful books I have read. While it doesn't explicitly step the reader through how to change habits, it thoroughly goes through many examples of where habits have changed other people's lives, case studies if you will. By then being able to recognise the habitual changes these people had to make, this allows the reader to (maybe) identify and be able to change some of their own habits to better their own life. Cue, Routine, Reward A concept that is repeatedly drawn parallel to habitual behaviours in the case study examples is that of Cue, Routine, Reward. Cue: Something that 'triggers' a habitual behaviour, this could be hunger, boredom or even just a bright colour. Routine: The process that follows the cue. Reward: The end goal of this habit to satisfy a need/desire for something An example of this in my own life is that when I'm at work sitting at my desk tapping away at my jobs for the day, I'll finish one of my tasks for the day (I love bullet journalling to maintain these), I won't want to start the next task straight away so I'll typically go to the kitchen and fill up my drink bottle. Here the process is as follows: Cue: Finish a task at work Routine: Walk to the kitchen and fill up water bottle Reward: A moment to stretch my legs and get a drink While this is a good habit that I have tried to instill myself, can also be replaced by any bad habit (eg, walk to the kitchen and eat a bag of chips) Case Study - Alcoa A fantastic case study that was presented in the book that resonated with me was the changing of the CEO at Alcoa. Alcoa is one of the biggest manufacturers of aluminium in the United States. When the CEO had changed, Alcoa was a dangerous place to work with a high staff injury rate, turns out working with molten metal all day can be a dangerous activity, who would've knew. The new CEO made it his goal that the number 1 priority of Alcoa going forward would be safety. Profits, revenue, etc were not going to be the focus of Alcoa anymore (upsetting lots of investors at the time). By implementing changes such as employees would be required to implement a new safety procedure to get a promotion and that no task was worth doing in an unsafe manner, the CEO had instilled a habit of safety into the culture of the company. The ripple on effect of this was that by putting a focus on one aspect of the entire operation, this amended many of the things that were halting progress within the company. For example, instead of machinery breaking down catastrophically and needing long repair times, equipment was being regularly maintained as to keep it in safe working order and producing more material. Workers that were needing to take lots of time off were now able to keep working now that they were not falling ill to injury's or sickness from the workplace. This lead to Alcoa producing some of its best results in history and is a great example how focusing on one element of a business can have such a ripple effect. Michael Phelps Early on in Michael Phelps life, a coach realized Michael's potential as an athlete in swimming. This coach through instilling habits into Michael's routine, would go on to produce one of the most athletic swimmers of all time. By getting Michael to follow the exact same process (routine) every time before a race, this made it naturally for Michael to compete so strongly. Michael watched a video of the perfect race, every single time before a race, listened to the same pump up music through his headphones, stood up and waved to the crowd the exact same way every time such that it had became a habit to swim. Something that he didn't actively have to think about to do. Conclusion I personally absolutely loved this book and it's appreciation for that not every person works the same way. By presenting the reader with a multitude of case studies and allowing the reader to determine what might be effective in their life for themselves, this doesn't instruct the reader to learn how their habits work but rather allows them to see the world through a different lens. You can find this book at: https://www.amazon.com.au/Power-Habit-What-Life-Business/dp/081298160X","tags":"Book Reviews","url":"https://jackmckew.dev/book-review-the-power-of-habits.html","loc":"https://jackmckew.dev/book-review-the-power-of-habits.html"},{"title":"Automatically Generate Documentation with Sphinx","text":"Document code automatically through docstrings with Sphinx This post goes into how to generate documentation for your python projects automatically with Sphinx! First off we have to install sphinx into our virtual environment. Pending on your flavour, we can do any of the following 1 2 3 pip install sphinx conda install sphinx pipenv install sphinx Once you have installed sphinx, inside the project (let's use the directory of this blog post), we can create a docs folder in which all our documentation will live. 1 2 mkdir docs cd docs Ensuring to have our virtual environment with sphinx installed active, we run sphinx-quickstart , this tool allows us to populate some information for our documentation in a nice Q&A style. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 Welcome to the Sphinx 2 .3.1 quickstart utility. Please enter values for the following settings ( just press Enter to accept a default value, if one is given in brackets ) . Selected root path: . You have two options for placing the build directory for Sphinx output. Either, you use a directory \"_build\" within the root path, or you separate \"source\" and \"build\" directories within the root path. > Separate source and build directories ( y/n ) [ n ] : y The project name will occur in several places in the built documentation. > Project name: SphinxDemo > Author name ( s ) : Jack McKew > Project release [] : If the documents are to be written in a language other than English, you can select a language here by its language code. Sphinx will then translate text that it generates into that language. For a list of supported codes, see https://www.sphinx-doc.org/en/master/usage/configuration.html#confval-language. > Project language [ en ] : Creating file . \\s ource \\c onf.py. Creating file . \\s ource \\i ndex.rst. Creating file . \\M akefile. Creating file . \\m ake.bat. Finished: An initial directory structure has been created. You should now populate your master file . \\s ource \\i ndex.rst and create other documentation source files. Use the Makefile to build the docs, like so: make builder where \"builder\" is one of the supported builders, e.g. html, latex or linkcheck. Now let's create an example package that we can write some documentation in. 1 2 mkdir sphinxdemo cd sphinxdemo Then we create 3 files inside our example package: 1 __init__.py download 1 version = \"0.1.1\" 1 __main__.py download 1 2 3 4 5 6 from .file_functions import get_files_in_folder if __name__ == \"__main__\" : py_files = get_files_in_folder ( \".\" , extension = \".py\" ) print ( py_files ) 1 file_functions.py download 1 2 3 4 5 6 7 8 9 10 11 12 13 import os def get_files_in_folder ( path , extension ): f = [] for ( dirpath , dirnames , filenames ) in os . walk ( path ): if extension : for filename in filenames : if filename . endswith ( extension ): f . append ( filename ) else : f . extend ( filenames ) return f If you are using VS Code to use packages with debugging, change your launch.json with the following: \"configurations\": [ { \"name\": \"Python: Module - sphinxdemo\", \"type\": \"python\", \"request\": \"launch\", \"module\": \"sphinxdemo. __main__ \" } To add documentation within our source code, we use docstrings. There are many available styles of docstrings out there, my personal preference is Google Docstring Style . We need to enable the napoleon sphinx extensions in docs/conf.py for this style to work. The resulting documented code will look like: 1 __init__.py download 1 2 3 4 5 6 \"\"\" Initialisation file for package sphinxdemo-with-docs Declare any package wide variables here \"\"\" version = \"0.1.1\" 1 __main__.py download 1 2 3 4 5 6 7 8 9 10 11 12 \"\"\" Main runtime for sphinxdemo-with-docs package __main__.py file used within package to work with `python -m` functionality. Prints out list of all *.py files within current directory when run \"\"\" from .file_functions import get_files_in_folder if __name__ == \"__main__\" : py_files = get_files_in_folder ( \".\" , extension = \".py\" ) print ( py_files ) 1 file_functions.py download 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \"\"\" Functions for parsing folders for files. \"\"\" import os def get_files_in_folder ( path , extension ): \"\"\" Prints all files in folder, if an extension is given, will only print the files with the given extension Args: path (string): folder to recursively search through for specific extensions extension (string): extension of file type to filter by Returns: list: list of all filenames within path with matching extension \"\"\" f = [] for ( dirpath , dirnames , filenames ) in os . walk ( path ): if extension : for filename in filenames : if filename . endswith ( extension ): f . append ( filename ) else : f . extend ( filenames ) return f Now at a minimum our source code is documented, now to present these docstrings in a format that we can share with others (html). First we need to set the sphinx configuration, the file which contains this (we generated with sphinx-quickstart) is located in docs/source/conf.py . We are going to utilise the following sphinx extensions (they are all in-built into sphinx): sphinx.ext.autodoc sphinx.ext.napoleon sphinx.ext.viewcode sphinx.ext.autosummary Our conf.py file for sphinx's configuration results in: Sphinx Configuration File conf.py download 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 # Configuration file for the Sphinx documentation builder. # # This file only contains a selection of the most common options. For a full # list see the documentation: # https://www.sphinx-doc.org/en/master/usage/configuration.html # -- Path setup -------------------------------------------------------------- # If extensions (or modules to document with autodoc) are in another directory, # add these directories to sys.path here. If the directory is relative to the # documentation root, use os.path.abspath to make it absolute, like shown here. # import os import sys sys . path . insert ( 0 , os . path . abspath ( \"../..\" )) # -- Project information ----------------------------------------------------- project = \"SphinxDemo\" copyright = \"2020, Jack McKew\" author = \"Jack McKew\" # -- General configuration --------------------------------------------------- # Add any Sphinx extension module names here, as strings. They can be # extensions coming with Sphinx (named 'sphinx.ext.*') or your custom # ones. extensions = [ \"sphinx.ext.autodoc\" , \"sphinx.ext.napoleon\" , \"sphinx.ext.viewcode\" , \"sphinx.ext.autosummary\" , ] # Add any paths that contain templates here, relative to this directory. templates_path = [ \"_templates\" ] autosummary_generate = True # List of patterns, relative to source directory, that match files and # directories to ignore when looking for source files. # This pattern also affects html_static_path and html_extra_path. exclude_patterns = [] # -- Options for HTML output ------------------------------------------------- # The theme to use for HTML and HTML Help pages. See the documentation for # a list of builtin themes. # html_theme = \"alabaster\" # Add any paths that contain custom static files (such as style sheets) here, # relative to this directory. They are copied after the builtin static files, # so a file named \"default.css\" will overwrite the builtin \"default.css\". html_static_path = [ \"_static\" ] We must also set our index.rst (restructured text) with what we want to see in our documentation. Documentation Index File index.rst download 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 .. SphinxDemo documentation master file , created by sphinx - quickstart on Tue Feb 4 20 : 05 : 16 2020 . You can adapt this file completely to your liking , but it should at least contain the root `toctree` directive . Welcome to SphinxDemo ' s documentation ! ====================================== .. toctree :: : maxdepth : 4 : caption : Contents : .. rubric :: Modules .. autosummary :: : toctree : generated sphinxdemo_with_docs . __init__ sphinxdemo_with_docs . __main__ sphinxdemo_with_docs . file_functions Indices and tables ================== * : ref : `genindex` * : ref : `modindex` * : ref : `search` To generate individual pages for our modules, classes and functions, we define separate templates, these are detailed here: autosummary templates Next we navigate our docs directory, and finally run: 1 make html This will generate all the stubs for our documentation and compile them into HTML format.","tags":"Python","url":"https://jackmckew.dev/automatically-generate-documentation-with-sphinx.html","loc":"https://jackmckew.dev/automatically-generate-documentation-with-sphinx.html"},{"title":"3D Terrain in Python","text":"{% notebook 2020/3d-terrain/notebooks/3d-terrain.ipynb %}","tags":"Python","url":"https://jackmckew.dev/3d-terrain-in-python.html","loc":"https://jackmckew.dev/3d-terrain-in-python.html"},{"title":"Intro To GeoPandas","text":"{% notebook 2020/intro-to-geopandas/notebooks/intro-to-geopandas.ipynb %}","tags":"Python","url":"https://jackmckew.dev/intro-to-geopandas.html","loc":"https://jackmckew.dev/intro-to-geopandas.html"},{"title":"Migrating from Wordpress to Pelican","text":"For some time now I have been wanting to move away from Wordpress, due to my specific case of wanting to embed custom HTML and JavaScript code snippets to enable interactive data visualisation. Furthermore my previous workflow of posts was disjointed in which I would develop the code in a Jupyter notebook, sometimes even writing the post in markdown within the notebook, then copying all of this out of the notebook into a Wordpress post and fiddling around with formatting for much too long. What tipped me over the edge was when I was looking back on previous posts (as this blog is mainly for storing previous projects, concepts and ideas), I was finding that I would go through the post and then have no idea whatsoever on where the project actually lived, this had to be fixed. I started noticing more and more people online had moved to Github Pages , which is primarily used with Jekyll. This rabbit hole went on as follows: Static served websites (generate HTML pages and serve them) Numerous static site generators: Jekyll, Hugo, VuePress Pelican So on Blogging with Jupyter notebooks Concept of CI/CD Travis CI Netlify What I settled on was a bit of a concoction of services, such that I can both get my feet wet with these new tools and still stay in the land of snakes (Python). Pelican + Travis CI + Netlify + Github Before we get into all 4 services in conjunction, let's separate and step through the process for each of them. Pelican Right off the bat, the first milestone I wanted to hit was to be able to generate a locally hosted static site from a single post converted to markdown. Luckily, there is an exact guide for going through this process in the documentation for Pelican and using the tool pelican-quickstart. http://docs.getpelican.com/en/3.6.3/quickstart.html Themes The next step was to decide on a theme for the website, while the intentions were to develop a theme from scratch, I shall leave this for a later date. An easy way of previewing themes was the website: http://www.pelicanthemes.com/ Which lets you scroll through the various themes, and even links to the repository on github for the theme if you wish to use it. The theme I decided on was Flex by Alexandre Vicenzi . Apply the the theme was as simple as cloning the repo (or using git submodules ), and adding one line of code in pelicanconf.py (generated automatically by pelican-quickstart). 1 THEME = \"./themes/Flex\" Plugins Admittedly, I just tried out all the plugins in the Pelican Plugins Repository until I found the combination that works for me, this ended up being: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 PLUGINS = [ \"sitemap\" , \"better_codeblock_line_numbering\" , \"better_code_samples\" , \"bootstrapify\" , \"deadlinks\" , \"more_categories\" , \"neighbors\" , \"pelican-ert\" , \"liquid_tags.notebook\" , \"liquid_tags.include_code\" , \"representative_image\" , \"share_post\" , 'show_source' , 'tipue_search' , \"dateish\" , \"post_stats\" , \"render_math\" , \"autostatic\" , \"clean_summary\" ] For tipue_search in particular, as this wasn't supported by the theme yet, I created a pull request on the original repository, with the functionality integrated https://github.com/alexandrevicenzi/Flex/pull/193 . Wordpress Import Now that I had the skeleton of the website set up, I needed to bring in all the existing posts from wordpress. By following another guide within the Pelican documentation, this was a relatively simple task http://docs.getpelican.com/en/3.6.3/importer.html . However, I did spend the time to go through and edit each markdown to remove redundant 'wordpress' formatting tags manually. Linking to Content As one of the main tasks of this project was to consolidate articles with the content/code/analysis in one spot, initially in development following the guide in http://docs.getpelican.com/en/3.6.3/content.html . 1 2 3 4 5 6 7 8 9 content ├── articles │ └── article.md ├── images │ └── han.jpg ├── pdfs │ └── menu.pdf └── pages └── test.md I ended up with a structure like above, which annoyed me a bit as now the content was in one place, but still divided into 3 folders with little-to-no link between them, my goal was to have the structure like: 1 2 3 4 5 6 7 8 9 10 11 content ├── articles │ ├── test-article │ | ├── img │ │ | └── icon.png │ │ | └── photo.jpg │ | ├── notebooks │ │ | └── test-notebook.ipynb │ │ └── article.md └── files └── archive.zip By using the plugins autostatic & liquid_tags , I was able to achieve this structure. Travis CI To be honest, I was actually surprised at how easy it was to turn Travis CI and that I could spin up a virtual machine, install all the dependencies and re-build the website. However, I had a lot of trouble trying to get Travis CI to push back to the repository such that Netlify could build from it. This was later remedied by setting a repository secret variable on Travis CI as I couldn't get the secret token encrypted by Travis CI CLI (Ruby application). In essence, all that was needed was a .travis.yml file in the root directory which ended up like this: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 language: python branches: only: - master install: - pip install --upgrade pip - pip install -r requirements.txt script: - pelican content deploy: provider: pages skip_cleanup: true github_token: $GITHUB_TOKEN keep_history: true local_dir: output on: branch: master Netlify Admittedly, I feel as if I'm not using Netlify for all it can do. Essentially, all for this project, it just detects a change in the gh-pages branch (for Github Pages), and redeploys the website out to a custom domain. Github Github is the repository location for all the code, and I use Git for version control and interaction with the repository. All I need to do now to create a new post is: Push a new markdown file (and any other linked content) to the master branch of the repository, This will fire up Travis CI to build the site with Pelican for me, Travis CI will then push the created site to the gh-pages branch of the repository, Netlify will detect the change and process the new site, The new site is deployed with updated posts!","tags":"Software Development","url":"https://jackmckew.dev/migrating-from-wordpress-to-pelican.html","loc":"https://jackmckew.dev/migrating-from-wordpress-to-pelican.html"},{"title":"Making Executables Installable with Inno Setup","text":"Following on from last week's post on making executable GUIs with Gooey, this post will cover how to make the executables we have packaged up into installable files so our users can run them easily. Once we have created the executable file for our GUI (which will be located in the dist folder: Now we are going to use a program called Inno Setup, which can be downloaded from: http://www.jrsoftware.org/isinfo.php. After you've installed Inno Setup, run these commands: Select create a new script file using the Script Wizard Fill in the application information Leave defaults Select the *.exe file found in the dist folder Select shortcut choices Add any license or information files Select install mode Select the languages Provide compiler settings and icon for installable Leave default Compile new script Share around the executable installer! Once installed, it will now act and behave like any other software installed on your computer!","tags":"Software Development","url":"https://jackmckew.dev/making-executables-installable-with-inno-setup.html","loc":"https://jackmckew.dev/making-executables-installable-with-inno-setup.html"},{"title":"Making Executable GUIs with Python, Gooey & Pyinstaller","text":"Today we will go through how to go from a python script to packaged executable with a guided user interface (GUI) for users. First off we still start by writing the scripts that we would like to share with others to be able to use, especially for users that may be uncomfortable in a programming environment and would feel at home with a GUI. My personal favourite part about Gooey , is that you are essentially creating a command line interface (CLI) tool, which Gooey then uses to generate a GUI. This eliminates having two separate code bases to facilitate CLI & GUI users, which can be very painful at times. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def print_file_name ( path , filesize ): \"\"\" Inputs: path (str): filepath to file selected filezize (bool): whether to print the file size or not Prints file name of file from path given and if filesize is true then will print the total size of the file in bytes \"\"\" print ( os . path . basename ( path )) if filesize : print ( f \"File size: { os . path . getsize ( path ) } bytes\" ) def get_files_in_folder ( path , extension ): \"\"\" Inputs: path (str): path to folder selected extension (str): extension to filter by Prints all files in folder, if an extension is given, will only print the files with the given extension \"\"\" f = [] for ( dirpath , dirnames , filenames ) in os . walk ( path ): if extension : for filename in filenames : if filename . endswith ( extension ): f . append ( filename ) else : f . extend ( filenames ) return f The 2 functions defined above are for getting information of selected files, or returning a list of files found within a folder (and subfolders). Now to use Gooey , we need to define a 'main' function for parsing the arguments for the GUI to generate controls. As Gooey is based on the argparse library, if you have previously built CLI tools with argparse, the migration to Gooey is quite simplistic. However as there is always edge cases, ensure to check your tools functionality once you have developed it. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 @Gooey ( optional_cols = 2 , program_name = \"Gooey Executable with Pyinstaller\" ) def parse_args (): prog_descrip = 'Pyinstaller example with Gooey' parser = GooeyParser ( description = prog_descrip ) sub_parsers = parser . add_subparsers ( help = 'commands' , dest = 'command' ) first_parser = sub_parsers . add_parser ( 'file' , help = 'This function prints the chosen file name' ) first_parser . add_argument ( 'file_path' , help = 'Select a random file' , type = str , widget = 'FileChooser' ) first_parser . add_argument ( '--file-size' , help = 'Do you want to print the file size?' , action = 'store_true' ) second_parser = sub_parsers . add_parser ( 'folder' , help = 'This funtion prints all files in a folder' ) second_parser . add_argument ( 'folder_path' , help = 'Select a folder' , type = str , widget = 'DirChooser' ) second_parser . add_argument ( '--file-type' , help = 'Specify file type with .jpg' , type = str ) args = parser . parse_args () return args By using the Gooey decorator we are able to define many different layout options for our GUI. Since we are trying to enable users to use multiple scripts which are different and separate, I personally like to the optional columns layout, but there are many other types of layouts which can be seen here: https://github.com/chriskiehl/Gooey#layout-customization . Following this we create our argument parsing function, and in which we define parsers, subparsers and add the arguments. This post will not be covering how to write CLIs, but it is on the list for future posts. To complete the script, we need to put in the functionality at startup. 1 2 3 4 5 6 if __name__ == '__main__' : conf = parse_args () if conf . command == 'file' : print_file_name ( conf . file_path , conf . file_size ) elif conf . command == 'folder' : print ( get_files_in_folder ( conf . folder_path , conf . file_type )) By embedding the command names within the arguments we are able to use a variety of functions which may or may not be interconnected. Once this file is run will generate the following: Which are fully embedded within the windows file explorer system for selecting files, folders, etc. Now to package this GUI as an executable, we use PyInstaller . By following Chris Kiehl's (Developer of Gooey) instructions on using Pyinstaller and Gooey: https://chriskiehl.com/article/packaging-gooey-with-pyinstaller . All we need to is create a build.spec file within our directory and run pyinstaller build.spec. This will then generate a build folder and a dist folder within your current directory. The build folder will contain all the files used in generating the executable, which is found within the dist folder. The code in it's entirety is: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 from gooey import Gooey , GooeyParser import os @Gooey ( optional_cols = 2 , program_name = \"Gooey Executable with Pyinstaller\" ) def parse_args (): prog_descrip = 'Pyinstaller example with Gooey' parser = GooeyParser ( description = prog_descrip ) sub_parsers = parser . add_subparsers ( help = 'commands' , dest = 'command' ) first_parser = sub_parsers . add_parser ( 'file' , help = 'This function prints the chosen file name' ) first_parser . add_argument ( 'file_path' , help = 'Select a random file' , type = str , widget = 'FileChooser' ) first_parser . add_argument ( '--file-size' , help = 'Do you want to print the file size?' , action = 'store_true' ) second_parser = sub_parsers . add_parser ( 'folder' , help = 'This funtion prints all files in a folder' ) second_parser . add_argument ( 'folder_path' , help = 'Select a folder' , type = str , widget = 'DirChooser' ) second_parser . add_argument ( '--file-type' , help = 'Specify file type with .jpg' , type = str ) args = parser . parse_args () return args def print_file_name ( path , filesize ): \"\"\" Inputs: path (str): filepath to file selected filezize (bool): whether to print the file size or not Prints file name of file from path given and if filesize is true then will print the total size of the file in bytes \"\"\" print ( os . path . basename ( path )) if filesize : print ( f \"File size: { os . path . getsize ( path ) } bytes\" ) def get_files_in_folder ( path , extension ): \"\"\" Inputs: path (str): path to folder selected extension (str): extension to filter by Prints all files in folder, if an extension is given, will only print the files with the given extension \"\"\" f = [] for ( dirpath , dirnames , filenames ) in os . walk ( path ): if extension : for filename in filenames : if filename . endswith ( extension ): f . append ( filename ) else : f . extend ( filenames ) return f if __name__ == '__main__' : conf = parse_args () if conf . command == 'file' : print_file_name ( conf . file_path , conf . file_size ) elif conf . command == 'folder' : print ( get_files_in_folder ( conf . folder_path , conf . file_type )) If you run into an error on Windows with the alert \"Failed to execute script pyi_rth_pkgres\", install the dev version of pyinstaller pip install https://github.com/pyinstaller/pyinstaller/archive/develop.zip This was noted in this issue on github: https://github.com/pyinstaller/pyinstaller/issues/2137","tags":"Python","url":"https://jackmckew.dev/making-executable-guis-with-python-gooey-pyinstaller.html","loc":"https://jackmckew.dev/making-executable-guis-with-python-gooey-pyinstaller.html"},{"title":"Hands On Machine Learning Chapter 3","text":"Chapter 3 is focusing in on classification systems. As brought up earlier, most common supervised machine learning tasks are regression (predicting values) and classification (predicting classes). This chapter goes through the 'Hello World' of classification tasks, the MNIST dataset. The MNIST dataset is a set of 70,000 images of handwritten digits written by high school students and employees of the US Census Bureau. Thankfully each image is also labelled with the digit it represents. Chapter 3 also introduces one of my personal favourite ways of evaluating classification performance, a confusion matrix. A confusion matrix is built up of rows and columns, rows representing the actual classification and columns representing the predicted classification . In a perfect classifier, the diagonal from left to right will be full of numbers ( true positives (TP) and true negatives (TN) and every where else will be 0. Whenever there is a number to the upper right of the diagonal, this represents any false positives (FP), while the lower left of the diagonal, representing false negatives (FN). Another way to assess the performance is to use the accuracy of the positive predicts, called the precision of the classifier. $$ \\frac{TP}{TP + FP} $$ Another metric that goes hand-in-hand with precision is the recall of a classifier. Which is the ratio of true positives that are correctly classified. $$ \\frac{TP}{TP+FN} $$ Or you can combine both precision and recall into a single metric, namely the F1 score . The F1 score is the harmonic mean of precision and recall. The harmonic mean gives much more weight to the low values, meaning the F1 score will only be high if the recall and precision are high. $$ \\frac{TP}{TP+\\frac{FN+FP}{2}} $$ Precision/Recall Tradeoff As above, when comparing precision and recall, you cannot have 100% of either, instead it is a trade off. With a precision of 100%, means all the samples classified as positive are true positives, however there may be a lot more that are now false positives. With a recall of 100%, all samples classified will include all of the true positives, however now all the false positives are included. Deciding the trade off comes down to the application. For example, if you wanted to create a classifier that detects websites that are safe for kids, you would prefer a classifier that rejects many good websites (low recall), but keeps only safe ones (high precision). On the other hand, if you wanted to create a classifier that detects threats in messages, it is probably fine to have a 25% precision, as long as it has 99% recall; meaning the authorities will get a few false alerts, but almost all threats will be identified.","tags":"Machine Learning","url":"https://jackmckew.dev/hands-on-machine-learning-chapter-3.html","loc":"https://jackmckew.dev/hands-on-machine-learning-chapter-3.html"},{"title":"Book Review: Principles by Ray Dalio","text":"I've just finished reading Ray Dalio's book Principles , and wanted to use this space to jot down some of my personal takeaways and thoughts. To give some background, Ray Dalio, founder of Bridgewater Assosciates, one of the largest hedge funds in the world, wrote this book around the unique principles that he discovered through his over forty year career that led to his and Bridgewater's success. Dalio believes that it is these principles, that are the reason behind whatever success he has had. Before I get into the thick of it, I would recommend this book to anyone that is feeling they are lacking some direction in their life. One of the organizational techniques I've picked up lately is Bullet Journaling , and one of the takeaways I got from the book was to start prioritizing my to-dos, in comparison to what I do now which is just dump it all onto the paper. The book is broken into two major sections: Life Principles and Work Principles, followed by descriptions of the tools developed to work alongside the principles to help keep the concept of an idea meritocracy intact. The concept of an idea meritocracy, is where you can have a community/company in which the best idea wins, no matter who/where it came from. This is in comparison to regular company structures where the decisions come from top down, which has its pros/cons as well. Life Principles On page 246, there is a summary of the life principles which are fully detailed in the preceding chapter. While I won't go into detail on Ray's perspective of the points, however I'll give a brief description on my personal takeaway. Please note the points are listed in no particular order. 1.1 - Be a hyperrealist - you need to understand, accept and work with reality to be able to appreciate and move forward with things effectively 1.3 - Radical transparency - while this can be a tough habit to develop, it pays itself off time and time again for both yourself and the people around you 1.4 - Look to nature to learn how reality works - if you haven't noticed in my blog posts, I am extremely curious about the natural way of evolution and look to bring it's methods into my everyday life and projects. I believe that Dalio also appreciates the power of evolution and is integrating it into his work (as he later describes) 2 - 5 Step process to get what you want out of life Have clear goals Identify and don't tolerate the problems that stand in the way of our achieving those goals Accurately diagnose the problems to get at their root causes Design plans that will get you around them Do what's necessary to push these designs through to results 3 - Radical open mind - something that I have blogged about in the past, open mindedness, you can find that post here: https://jackmckew.dev/episode-3-open-mind.html 4.5 - Getting the right people in the right roles in support of your goal is the key to succeeding at whatever you choose to accomplish 5.10 - Believability weight your decision making - I believe when gathering opinions and thoughts on something, you must weight the opinions based upon how 'believable' the source is Work Principles Dalio believes that culture is the most important part of any organization. I also share the same perspective that any company consists of two major components: culture and people. If the culture isn't right, the right people won't stay. 1.1 - Realize you have nothing to fear from knowing the truth - while sometimes harsh, having more information can always be beneficial if looked at from the right perspective 3.1 - Recognize that mistakes are a natural part of the evolutionary process - keeping with the theme of evolution is one of the greatest things in nature, it must be accepted that mistakes are apart of it all and are inevitable 3.2 - Don't worry about looking good, worry about achieving your goals - this is one that I try to embrace more and more each time I do something, admittedly at times I will withhold from sharing with others from how much progress has been made, by not sharing, everyone is losing out from sharing the learning experience as well 3.4 - Remember to reflect when you experience pain - it is always a good idea to look back on an experience that didn't go to plan or badly to make sure you adapt and evolve for next time 5.2 - Find the most believable people possible to disagree with you and try to understand their reasoning - a great way at validating your reasoning behind something is to ask someone else to poke holes through it to see if it still stands I believe that the way that you perceive the points in this book is dependent on the stage of life that you are at personally. However, even reading through them now, while I can't resonate with them at this point in time, I am glad that they have planted the seed for later on, which may be of great assistance. That is why I believe you should read this book.","tags":"Book Reviews","url":"https://jackmckew.dev/book-review-principles.html","loc":"https://jackmckew.dev/book-review-principles.html"},{"title":"Intro to Games in Python with Pyglet","text":"Recently, I've been researching ways that I could run a 2D simulation (and hopefully 3D) with many moving pieces and there was a desire to make it interactive as well. I stumbled through many visualisation frameworks such as: p5 pygame plotly panda3d bokeh many others Eventually, through the motivation of another side project (looking into training neural networks to learn how to play games) and inspired by this video from Code Bullet https://www.youtube.com/watch?v=r428O_CMcpI ; I decided on attempting to use Pyglet to do these simulations. While the aforementioned simulations won't be covered in this post, this post aims to demonstrate how I adapted the in-depth tutorial on the Pyglet website (which goes through how to recreate asteroids in Pyglet) to generate vector based objects which can crash into each other. First off as always, start by setting up a virtual environment with your preferred method ( Anaconda or follow my workflow ), since Pyglet has no external dependencies, all you need to do is install the pyglet package. I won't go through all the code in the example, and how it works, I will only go through what I changed in the case to get where I wanted to go. To begin, and make things a bit easier, I downloaded the pyglet-master repository from GitHub ( https://github.com/pyglet/pyglet ) so I didn't have to create and copy the file contents one by one. After going through the different versions with the examples > game folder, I decided all I required was the simple functionality of collision and any further into developing the game wasn't needed for this stage, so I copied out the version 3 folder. If we run 'asteroid.py' from within the version 3 folder, we are met with this screen Now since all I am trying to do is generate multiple objects (which will be shown with the player symbol to indicate direction), I can comment out the lines which give the lives, score, title and interactive player. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # Set up the two top labels # score_label = pyglet.text.Label(text=\"Score: 0\", x=10, y=575, batch=main_batch) # level_label = pyglet.text.Label(text=\"Version 3: Basic Collision\", # x=400, y=575, anchor_x='center', batch=main_batch) # Initialize the player sprite # player_ship = player.Player(x=400, y=300, batch=main_batch) # Make three sprites to represent remaining lives # player_lives = load.player_lives(2, main_batch) # Make three asteroids so we have something to shoot at # asteroids = load.asteroids(3, player_ship.position, main_batch) asteroids = load . asteroids ( 100 ,( window_width // 2 , window_height // 2 ), main_batch ) # Store all objects that update each frame in a list # game_objects = [player_ship] + asteroids game_objects = asteroids # Tell the main window that the player object responds to events # game_window.push_handlers(player_ship.key_handler) Now that we've done that, we need to modify the asteroids generator function to use the player sprite. In load.py, you can change simply the img argument to the player image sprite reference like so: 1 new_asteroid = physicalobject . PhysicalObject ( img = resources . player_image , x = asteroid_x , y = asteroid_y , batch = batch ) Now if we run this, the animation will look a little off, because the objects won't be traveling the direction in the direction that the sprite is pointing. This is due to the existing velocity calculation being a random number for both the X and Y component. To make the player sprites move in the direction they are rotated in, and maintain the existing codebase, we will need to convert from polar notation to cartesian . To do this, we add an extra 2 functions into 'util.py' which will do this for us: 1 2 3 4 5 6 7 8 9 def cart2pol ( x , y ): rho = math . sqrt ( x ** 2 + y ** 2 ) phi = math . arctan2 ( y , x ) return ( rho , phi ) def pol2cart ( rho , phi ): x = rho * math . sin ( math . radians ( phi )) y = rho * math . cos ( math . radians ( phi )) return ( x , y ) Note the use of radians in pol2cart, this is due to the affect of quadrants and trigonometric functions . I won't go into detail, but it won't behave like you expect it to. Now to get our player sprites moving in the direction they are rotated, update the code which generates the 'asteroids' to utilise our new function: 1 2 3 new_asteroid . rotation = random . randint ( 0 , 360 ) new_asteroid . velocity_speed = random . random () * 40 new_asteroid . velocity_x , new_asteroid . velocity_y = util . pol2cart ( new_asteroid . velocity_speed , new_asteroid . rotation ) Now when we go and run our main file again, we will met with a screen like this: Where the player sprites will float around in the direction they are pointing, until they crash into another sprite, causing both of them to disappear. This is a quick intro to Pyglet, I am hoping to expand on this simulation and am positive I will be doing further write ups with it in the future.","tags":"Python","url":"https://jackmckew.dev/intro-to-games-in-python-with-pyglet.html","loc":"https://jackmckew.dev/intro-to-games-in-python-with-pyglet.html"},{"title":"Introduction to Pytest & Pipenv","text":"Unit tests in general are good practice within software development, they are typically automated tests written to ensure that a function or section of a program (a.k.a the 'unit') meets its design and behaves as intended. This post won't go into testing structures for complex applications, but rather just a simple introduction on how to write, run and check the output of a test in Python with pytest. As this post is on testing, I also thought it might be quite apt for trialing out a difference package for dependency management. In the past I've used anaconda, virtualenv and just pip, but this time I wanted to try out pipenv. Similar to my post Python Project Workflow where I used virtualenv, you must install pipenv in your base Python directory, and typically add the Scripts folder to your path for ease later on. Now all we need to do is navigate to the folder and run: 1 pipenv shell This will create a virtual environment somewhere on your computer (unless specified) and create a pipfile in the current folder. The pipfile is a file that essentially describes all the packages used within the project, their version number & so on. This is extremely useful when you pick it back up later on and find where you were at or if you wish to share this with others, they can generate their own virtual environment simply from the pipfile with: 1 pipenv install -- dev Enough about pipenv, let's get onto trying out pytest. For this post I will place both my function and it's tests in the same file, however, from my understanding it's best practice to separate them, specifically keeping all tests within an aptly named 'tests' directory for your project/package. First off let's define the function we intend to test later: 1 2 def subtract ( number_1 , number_2 ): return number_1 - number_2 Now we want to test if our function returns 1 if we give it number_1 = 2 and number_2 = 1: 1 2 3 4 import pytest def test_subtract (): assert subtract ( 2 , 1 ) == 1 To run this test, open the pipenv shell like above in the directory of the file where you've written your tests and run: 1 pytest file_name . py This will output the following: Each green dot represents a single test, and we can see that our 1 test passes in 0.02 seconds. To get more information from pytest, use the same command with -v (verbose) option: 1 pytest file_name . py - v Now we might want to check that it works for multiple cases, to do this we can use the parametrize functionality of pytest like so: 1 2 3 4 5 6 7 8 9 10 11 12 13 import pytest def subtract ( number_1 , number_2 ): return number_1 - number_2 @pytest . mark . parametrize ( 'number_1, number_2, expected' , [ ( 2 , 1 , 1 ), ( 5 , 1 , 4 ), ( 6 , 2 , 4 ), ( - 2 , 1 , - 3 ), ]) def test_subtract ( number_1 , number_2 , expected ): assert expected == subtract ( number_1 , number_2 ) Once run with the verbose command, we get the output: Hopefully this post is a gentle introduction to what unit testing can be in Python.","tags":"Python","url":"https://jackmckew.dev/introduction-to-pytest-pipenv.html","loc":"https://jackmckew.dev/introduction-to-pytest-pipenv.html"},{"title":"Inheritance in Python","text":"As Python is a high level, general purpose programming language, which supports users to define their own types using classes, which are most often following the concept of object-oriented programming. Object-oriented programming is a type of software design in which users not only define the data type (eg, int) of a data structure, but also the types of functions that can be applied. Object-oriented programming is built up of a lot of concepts, to name a few: Inheritance Abstraction Class Encapsulation so on This post will cover an introduction to the concept of inheritance using Python and the animal kingdom. First off, we are going to start by defining our 'base' class (also known as abstract class) of our Animal with common properties: 1 2 3 4 5 6 7 8 9 10 11 12 class Animal (): def __init__ ( self , name = 'Animal' ): self . name = name def family ( self ): print ( \"Animal Kingdom\" ) def speak ( self ): raise Exception ( \"Not implemented yet (define speak)\" ) def eat ( self ): raise Exception ( \"Not implemented yet (define eat)\" ) Now that we have our base class, we can define a subclass 'Dog' that will be able to speak if we define the function inside, but we can also see that it derives from it's parent class 'Animal' by printing out it's family. 1 2 3 4 5 6 7 8 9 10 11 class Dog ( Animal ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) def speak ( self ): print ( \"Woof!\" ) dog = Dog ( \"Jay\" ) dog . speak () dog . family () Which will print out: 1 2 Woof ! Animal Kingdom See my post on dunders (double underscores) to get a better understanding of how the __init__ function is working: https://jackmckew.dev/dunders-in-python.html Now we can define any subclass which can derive from our parent class 'Animal', or even more we can derive a class from 'Dog' and it will have all it's properties: 1 2 3 4 5 6 7 class JackRussell ( Dog ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) dog_2 = JackRussell ( 'Jeff' ) dog_2 . speak () dog_2 . family () Which will also print: 1 2 Woof ! Animal Kingdom Now what if we wanted to specify the family that all of our dog classes are, we can do this by overriding their parent class (similar to how we are overriding the speak function): 1 2 3 4 5 6 7 8 9 class Dog ( Animal ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) def family ( self ): print ( \"Mammal\" ) def speak ( self ): print ( \"Woof!\" ) Which then when we run both the below code: 1 2 3 4 5 6 7 8 9 10 11 dog = Dog ( \"Jay\" ) dog . speak () dog . family () class JackRussell ( Dog ): def __init__ ( self , name = 'Animal' ): super () . __init__ ( name = name ) dog_2 = JackRussell ( 'Jeff' ) dog_2 . speak () dog_2 . family () We will now get: 1 2 3 4 Woof ! Mammal Woof ! Mammal You should now be comfortable in understanding how inheritance works. Normally, it's best practice to inherit only from a single parent class when creating subclasses. As multiple inheritance makes your programs less complicated and easier to manage. However, for large programs, it is very difficult to avoid multiple inheritance.","tags":"Python","url":"https://jackmckew.dev/inheritance-in-python.html","loc":"https://jackmckew.dev/inheritance-in-python.html"},{"title":"Dunders in Python","text":"A 'dunder' (double underscores) in Python (also known as a magic method) are the functions within classes having two prefix and suffix underscores in the function name. These are normally used for operator overloading (eg, __init__, __add__, __len__, __repr__, etc). For this post we will build a customized class for vectors to understand how the magic methods can be used to make life easier. First of all before we get into the magic methods, let's talk about normal methods. A method in Python is a function that resides in a class. To begin with our Vector class, we initialise our class and give it a function, for example: 1 2 3 4 class Vector (): def say_hello (): print ( \"Hello! I'm a method\" ) Now to call the method, we simply call the function name along with the Vector instance we wish to use: 1 Vector . say_hello () This will print: 1 Hello! I ' m a method Now for our vector class, we want to be able to initialise it with certain constants or variables for both the magnitude and direction of our vector. We use the __init__ magic method for this, as it is invoked without any call, when an instance of a class is created. 1 2 3 class Vector (): def __init__ ( self , * args ): self . values = args Now when we create an instance of our Vector class, we can give it certain values that it will store in a tuple: 1 2 3 vector_1 = Vector ( 1 , 2 , 3 ) print ( vector_1 ) Which will print: 1 < __main__ . Vector object at 0x03E90530 > But to us humans, this doesn't mean much more than we know what the name of the class is of that instance. What we really want to see when we call print on our class is the values inside it. To do this we use the __repr__ magic method: 1 2 3 4 5 6 7 8 9 class Vector (): def __init__ ( self , * args ): self . values = args def __repr__ ( self ): return str ( self . values ) vector_1 = Vector ( 1 , 2 , 3 ) print ( vector_1 ) Which will print: 1 ( 1 , 2 , 3 ) This is exactly what we want! Now what if we wanted to create a Vector, but we weren't sure what values we wanted to give it yet. What would happen if we didn't give it any values? Would it default to (0,0) like we would hope? 1 2 3 empty_vector = Vector () print ( empty_vector ) Which will print: 1 () Not exactly how we need it, so we would need to run a check when the class is being initialized, to ensure that there are values being provided: 1 2 3 4 5 6 7 8 class Vector (): def __init__ ( self , * args ): if len ( args ) == 0 : self . values = ( 0 , 0 ) else : self . values = args def __repr__ ( self ): return str ( self . values ) Which when initialise an empty instance of our Vector now, it will create a (0,0) vector for us! Now what if we wanted to be able to check how many values were inside our vector class? To do this we can use the __len__ magic method>: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 class Vector (): def __init__ ( self , * args ): if len ( args ) == 0 : self . values = ( 0 , 0 ) else : self . values = args def __repr__ ( self ): return str ( self . values ) def __len__ ( self ): return len ( self . values ) vector_1 = Vector ( 1 , 2 , 3 ) print ( vector_1 ) print ( len ( vector_1 )) Which will print: 1 2 ( 1 , 2 , 3 ) 3 Hopefully this post has given you insight into how dunders/magic methods could be used to super power your classes and make life much easier! You can find more information and examples about dunders in Python at: < https://docs.python.org/3/reference/datamodel.html#special-method-names","tags":"Python","url":"https://jackmckew.dev/dunders-in-python.html","loc":"https://jackmckew.dev/dunders-in-python.html"},{"title":"Python Project Workflow","text":"This post will go through my personal preference on project structure and workflow for creating a new project and an insight how I work on my projects from a development point of view. I will go from the very start as if I did not have Python/Git installed on my machine whatsoever. First of all, we need to get Python! Head over to https://www.python.org/downloads/ to get the version of Python you need (or default to the latest Python 3 stable release). For version control in my projects, I also like to use Git so, head on over to https://git-scm.com/downloads to download Git for your operating system. Now once these are installed (if you put them in the default location), Python will default to be located in: C:\\Users\\Jack\\AppData\\Local\\Programs\\Python\\Python37-32. For the next few steps to ensure we are setting up virtual environments for our projects open command prompt here if you are on windows. This will look something like this: The 'cd' command in windows (and other OS) stands for change directory, follow this with a path and you will be brought to that directory. Next whenever I first install Python I like to update pip to it's latest release, to do this use the command in this window: 1 python -m pip install --upgrade pip With pip upgraded to it's current release, it's time to install some very helpful packages for setting up projects: virtualenv and cookiecutter. To install these navigate to the the Scripts folder within the current directory with cd ('cd Scripts') and run 'pip.exe install virtualenv cookiecutter', pip will then work it's magic and install these packages for you. If you take a peek into the Scripts folder now in your Python directory, it'll look a little like this: Now something that I personally like to do is add this folder to your system environment variables in Windows so it's much easier to run any packages in your root Python installation on your PC. To do this: type in 'system environment' into the search command select environment variables from the bottom right corner edit system (or user) path variable browse and select the Scripts directory in your Python installation If you chose to do this step, you will now be able to create virtual environments and cookiecutter templates without having to specify the directory to the executables. It's now time to create a project from scratch. So navigate to where you like to keep your projects (mostly mine is in Documents\\Github\\) but you can put them anywhere you like. Now run command prompt again (or keep the one you have open) and navigate to the dedicated folder (or folders) using cd. For most of my projects lately being of data science in nature, I like to use the cookiecutter-data-science template which you can find all the information about here: https://drivendata.github.io/cookiecutter-data-science/ . To then create a project it is as simple as running: 1 cookiecutter https://github.com/drivendata/cookiecutter-data-science Provide as much information as you wish into the questions and you will now have a folder created wherever you ran the command with all the relevant sections from the template. Whenever starting a new Python project, my personal preference is to keep the virtual environment within the directory, however this is not always a normal practice. To create a virtual environment for our Python packages, navigate into the project and run (if you added Scripts to your Path): 1 virtualenv env This will then initialise a folder within your current directory to install a copy of Python and all it's relevant tools with a folder ('env'). Before we go any further, this is the point that I like to initialise a git repository. To do this, run git init from your command line from within the project directory. Now to finish off the final steps of the workflow that will affect the day-to-day development, I like to use pre-commit hooks to reformat my with black and on some projects check for PEP conformance with flake8 on every commit to my projects repository. This is purely a personal preference on how you would like to work, others like to use pytest and more to ensure their projects are working as intended, however I am not at that stage just yet. To install these pre-commits into our workflow, firstly initialise the virtual environment from within our project by navigating to env/Scripts/activate.bat. This will activate your project's Python package management system and runtime, following this you can install packages from pip and otherwise. For our pre-commits we install the package 'pre-commit': 1 pip install pre-commit Following this to set up the commit hooks create a '.pre-commit-config.yaml' within your main project directory. This is where we will specify what hooks we would like to run before being able to commit. Below is a sample .pre-commit-config.yaml that I use in my projects: 1 2 3 4 5 6 7 8 9 10 11 12 13 repos: - repo: https://github.com/ambv/black rev: stable hooks: - id: black language_version: python3.7 - repo: https://github.com/pre-commit/pre-commit-hooks rev: v2.3.0 hooks: - id: flake8 - id: check-yaml - id: end-of-file-fixer - id: trailing-whitespace Now to install these, activate your virtual environment like above, navigate to the project directory and run 'pre-commit install'. This will install the pre-commit hooks within your git directory. Before going any further, I highly recommend to run 'pre-commit run --all-files' to both ensure pre-commit is working as expected and check if there is any project specific settings you may have to set. On the default cookiecutter data science template with the settings as per above this will show on the pre-commit run (after you have staged changes in git (use git add -A for all)): We can see a different opinions in code formatting appearing already from flake8's output. The black code formatter in Python's code length is 88 characters , not 79 like PEP8. So we will add a pyproject.toml to the project directory where we can specify settings within the black tool: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 [ tool.black ] line-length = 79 include = '\\.pyi?$' exclude = ''' /( \\.git | \\.hg | \\.mypy_cache | \\.tox | \\.venv | \\.docs | _build | buck-out | build | dist )/ ''' For any flake8 specific settings (such as error codes to ignore), we can set a .flake8 file in the project directory as well, which may look like: 1 2 3 4 5 [ flake8 ] ignore = E203, E266, E501, W503, F403, F401 max-line-length = 88 max-complexity = 18 select = B,C,E,F,W,T4,B9 Finally we are able to run a commit to our project!","tags":"Python","url":"https://jackmckew.dev/python-project-workflow.html","loc":"https://jackmckew.dev/python-project-workflow.html"},{"title":"Linear Regression: Under the Hood with the Normal Equation","text":"Let's dive deeper into how linear regression works. Linear regression follows a general formula: $$ \\hat{y} = \\theta_0 + \\theta_1x_1 + \\theta_2x_2 + \\cdots + \\theta_nx_n $$ Where \\(\\hat{y}\\) is the predicted value, \\(n\\) is the number of features, \\(x_i\\) is the \\(i&#94;{th}\\) feature value and \\(\\theta_n\\) is the \\(n&#94;{th}\\) model parameter. This function is then vectorised which speeds up processing on a CPU, however, I won't go into that further. How does the linear regression model get 'trained'? Training a linear regression model means setting the parameters such that the model best fits the training data set. To be able to do this, we need to be able to measure how good (or bad) the model fits the data. Common ways of measuring this are: Root Mean Square Error (RMSE) Mean Absolute Error (MAE) R-Squared Adjusted R-Squared many others From here on, we will refer to these as the cost function, and the objective is to minimise the cost function. The Normal Equation To find the value of \\(\\theta\\) that minimises the cost function, there is a mathematical equation that gives the result directly, named the Normal Equation $$ \\hat{\\theta} = (X&#94;T \\cdot X)&#94;{-1}\\cdot X&#94;T \\cdot y $$ Where \\(\\hat{\\theta}\\) is the value of \\(\\theta\\) that minimises the cost function and \\(y\\) (once vectorised) is the vector of target values containing \\(y&#94;{(1)}\\) to \\(y&#94;{(m)}\\) . For example if this equation was run on data generated from this formula: 1 2 3 4 import numpy as np X = 10 * np . random . rand ( 100 , 1 ) y = 6 + 2 * X + np . random . rand ( 100 , 1 ) Now to compute \\(\\hat{\\theta}\\) with the normal equation, we can use the inv() function from NumPy's Linear algebra module: 1 2 X_b = np . c_ [ np . ones (( 100 , 1 )), X ] theta_best = np . linalg . inv ( X_b . T . dot ( X_b )) . dot ( X_b . T ) . dot ( y ) With the actual function being \\(y = 6 + 2x_0 + noise\\) , and the equation found: 1 2 array ([[ 5.96356419 ], [ 2.00027727 ]]) Since the noise makes it impossible to recover the exact parameters of the original function now we can use \\(\\hat{\\theta}\\) to make predictions: 1 y_predict = X_new_b . dot ( theta_best ) With y_predict being: 1 2 [[ 5.96356419 ] [ 9.96411873 ]] The equivalent code using Scikit-Learn would look like: 1 2 3 4 5 from sklearn.linear_model import LinearRegression lin_reg = LinearRegression () lin_reg . fit ( X , y ) print ( lin_reg . intercept_ , lin_reg . coef_ ) print ( lin_reg . predict ( X_new )) And it finds: 1 2 3 [ 5.96356419 ] [[ 2.00027727 ]] [[ 5.96356419 ] [ 9.96411873 ]] Using the normal equation to train your linear regression model is linear in regards to the number of instances you wish to train on, meaning you will need to be able to fit the data set in memory. There are many other ways of to train a linear regression, some which are better suited for large number of features, these will be covered in later posts.","tags":"Data Science","url":"https://jackmckew.dev/linear-regresssion-under-the-hood-with-the-normal-equation.html","loc":"https://jackmckew.dev/linear-regresssion-under-the-hood-with-the-normal-equation.html"},{"title":"Intro to Web Scraping","text":"Following on from last weeks post where we analysed the amount of repeated letters within current New Zealand town names . There was still one part of that analysis that really bugged me, and if you noticed it was from the data set that was used was using the European town names not the original Maori names. This post will be dedicated to introducing web scraping where we will extract the Maori names and run a similar analysis to present an interactive graph. As like previously, let's take a look at the interactive graph before getting into how it was created. {% notebook 2019/intro-to-web-scraping/notebooks/scraping_analysis.ipynb cells[-1:] %} Similarly with most of my posts of this nature, we always begin by getting the data. To find a data set that gives us as many Maori town or place names as possible proved to be quite challenging, but luckily for Maori Language week NZhistory.gov.nz posted a table of a 1000 Maori place names, their components and the meaning. This data can be found: https://nzhistory.govt.nz/culture/maori-language-week/1000-maori-place-names . Unlike last time however with our world city names from Kaggle , this data isn't nicely supplied to us in an Excel format. While it may be possible to directly copy-paste from the website into a spreadsheet, I think this is a great way to ease into web scraping. What is Web Scraping Web scraping, web harvesting or web data extraction is the process of extracting data from websites. To do this in Python, while there is multiple ways to achieve this (requests + beautiful soup, selenium, etc), my personal favourite package to use is Scrapy . While it may be daunting to begin with from a non object-oriented basis, you will soon appreciate it more once you've begun using it. Initially the premise around the Scrapy package is to create 'web spiders'. If we take a look of the structure of the first example on the Scrapy website we get an understanding on how to structure our web spiders when developing: 1 2 3 4 5 6 7 8 9 10 11 import scrapy class BlogSpider ( scrapy . Spider ): name = 'blogspider' start_urls = [ 'https://blog.scrapinghub.com' ] def parse ( self , response ): for title in response . css ( '.post-header>h2' ): yield { 'title' : title . css ( 'a ::text' ) . get ()} for next_page in response . css ( 'a.next-posts-link' ): yield response . follow ( next_page , self . parse ) First of all we can see that the custom spider is essentially an extension of the scrapy.Spider class. It is to be noted that the name and start_urls variables (which are apart of the class) are special in the sense the scrapy package uses them as configuration settings. When it comes to web scraping, if you have had experience using HTML, CSS and/or Javascript, this experience will become extremely useful; that is not to say it is not possible without experience, it's just a learning curve. Following on we can see a function for parsing (also specially named) in which there are 2 loops, the first for loop is going to loop through all title's marked as headers (specifically h2) and return a dictionary with the text in the heading. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class NameSpider ( scrapy . Spider ): name = 'names' start_urls = [ 'https://nzhistory.govt.nz/culture/maori-language-week/1000-maori-place-names/' ] def parse ( self , response ): def extract_from_table ( table_row , table_col ): return response . xpath ( f \"//tr[ { table_row } ]//td[ { table_col } ]//text()\" ) . get () for i in range ( 2 , 1000 ): yield { 'Place Name' : extract_from_table ( i , 1 ), 'Components' : extract_from_table ( i , 2 ), 'Meaning' : extract_from_table ( i , 3 ) } Now that we have created our spider that looks through each row of the table on the webpage (more information on determining this can be found: https://docs.scrapy.org/en/latest/intro/tutorial.html ). It's time to run the spider and take a look at the output. To run a spider you go into the directory from the command line and run 'scrapy crawl \\<spider name>' and to store an output at the same time 'scrapy crawl \\<spider name> -o filename.csv -t csv. Now similar to the previous post, we run a similar analysis and plot with Bokeh! 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 import pandas as pd import collections from collections import OrderedDict import operator import matplotlib.pyplot as plt import numpy as np import math from bokeh.io import show , output_file from bokeh.plotting import figure from bokeh.models import ColumnDataSource from bokeh.models.tools import HoverTool names_df = pd . read_csv ( 'names.csv' , header = 0 , sep = ',' , quotechar = '\"' ) nz_names = names_df [ 'Place Name' ] . tolist () nz_dict = { i : 0 for i in nz_names } letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' lcount = dict ( OrderedDict ([( l , 0 ) for l in letters ])) for name in nz_names : nz_dict [ name ] = dict ( OrderedDict ([( l , 0 ) for l in letters ])) city_dict = nz_dict [ name ] for c in name : if c . upper () in letters : city_dict [ c . upper ()] += 1 total_df = pd . DataFrame . from_dict ( nz_dict ) total_df = total_df . T max_letters_cities = total_df . idxmax () . tolist () lettercounts = total_df . loc [ total_df . idxmax ()] . max () . tolist () maxletters = dict ( OrderedDict ([( l , 0 ) for l in letters ])) for i , l in enumerate ( letters ): maxletters [ l ] = max_letters_cities [ i ] maxletters [ l ] = ( lettercounts [ i ]) summary_df = pd . DataFrame () scale = 1 summary_df [ 'Word_Name' ] = total_df . idxmax () summary_df [ 'Count' ] = total_df . loc [ total_df . idxmax ()] . max () source = ColumnDataSource ( summary_df ) output_file ( \"letter_count.html\" ) hover = HoverTool () hover . tooltips = [ ( 'Word' , '@Word' ) ] p = figure ( x_range = summary_df . index . tolist (), plot_height = 250 , title = \"Letter Counts\" , toolbar_location = None ) p . vbar ( x = 'index' , top = 'Count' , width = 0.9 , source = source ) p . add_tools ( hover ) p . xgrid . grid_line_color = None p . y_range . start = 0 show ( p )","tags":"Python","url":"https://jackmckew.dev/intro-to-web-scraping.html","loc":"https://jackmckew.dev/intro-to-web-scraping.html"},{"title":"Looking for Patterns in City Names & Interactive Plotting","text":"Recently, I was traveling around New Zealand, and noticed in the Maori language they use letters back to back a lot like in the original Maori name for Stratford (\"whakaahurangi\"). So as any normal person does, I thought, well what town has the most repeated letters, and the idea for this blog post was born. Before we get into the nitty gritty, here is the output of the analysis! {% notebook 2019/looking-for-patterns-in-city-names-and-interactive-plotting/notebooks/NZ_City_Letter_Analysis.ipynb cells[24:25] %} Firstly, we have to find a dataset of all the town names, and I found a database of all world cities names hosted on Kaggle here: https://www.kaggle.com/max-mind/world-cities-database . Get the data! 1 2 3 # data source https://www.kaggle.com/max-mind/world-cities-database cities_df = pd . read_csv ( './data/worldcitiespop.csv' , header = 0 , sep = ',' , quotechar = '\"' ) cities_df = cities_df [ cities_df [ 'Country' ] == \"nz\" ] After inspecting the data of this data set, we're able to filter out to look at just New Zealand with the prefix of \"nz\" in the Country column. It must be noted that this data set represents the names of the towns currently, and not the original Maori names (more on this will be covered in a later post). Now we want to extract the town names out of the dataframe with the ones we want to analyze. For ease later on, we will extract this as a dictionary, such that we can assign the value of each to the count of each letter. 1 2 nz_cities = cities_df [ cities_df [ 'Country' ] == \"nz\" ][ 'AccentCity' ] . tolist () nz_dict = { i : 0 for i in nz_cities } Now we will create an ordered dictionary with the help from the collections package which will store the values of the count for each letter in the town name. 1 2 letters = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ' lcount = dict ( OrderedDict ([( l , 0 ) for l in letters ])) Now it's time for the data crunch. To count how many times a letter repeats in a town name we follow these steps: we create a for loop, to loop through all the city names in the table, initialise an ordered dictionary similar to above for each city in the value field of that town's dictionary entry loop through each letter in the town name check if the letter appears in our letter dictionary (mainly to not count spaces), Then if the letter does appear, increment the value for that letter by 1. This results in a dictionary for each town name, with the count of repeated letters. 1 2 3 4 5 6 for city in nz_cities : nz_dict [ city ] = dict ( OrderedDict ([( l , 0 ) for l in letters ])) city_dict = nz_dict [ city ] for c in city : if c . upper () in letters : city_dict [ c . upper ()] += 1 Hooray! Now we have all the data we need broken down and ready for analysis. To help ease the analysis and make it more readable for a human, we convert from our nested dictionaries to a pandas dataframe and transpose it such that we have the town name as the index, the letters as the column and the count of that letter as the values. 1 2 total_df = pd . DataFrame . from_dict ( nz_dict ) total_df = total_df . T Now we want to find which of these names have the maximum count for any particular letter and store it in a summary dataframe. It is to be noted that we could use the pivot function with aggregate types, however, I have not figured a nice way to do this yet. If you do know a nicer way to determine this, please let me know. 1 2 3 4 summary_df = pd . DataFrame () scale = 1 summary_df [ 'City_Name' ] = total_df . idxmax () summary_df [ 'Count' ] = total_df . loc [ total_df . idxmax ()] . max () Now by using the equivalent of an index-match in excel which you can read more about here ( https://towardsdatascience.com/name-your-favorite-excel-function-and-ill-teach-you-its-pandas-equivalent-7ee4400ada9f ). Admittedly, we could've made the join earlier, but since I use index-match so often in Excel, I wanted to learn how to do the same in pandas. This is achieved by using the map function (which is the equivalent of the index), but by using the index of another dataframe as the argument (the match function), we can rejoin the data set by matching the city name from our original data set. 1 2 summary_df [ 'Latitude' ] = summary_df [ 'City_Name' ] . map ( cities_df . set_index ([ 'AccentCity' ])[ 'Latitude' ] . to_dict ()) * scale summary_df [ 'Longitude' ] = summary_df [ 'City_Name' ] . map ( cities_df . set_index ([ 'AccentCity' ])[ 'Longitude' ] . to_dict ()) * scale Now we have a dataframe that contains: an index of the letters, the town name with the most repeated letters, the count of the letters within the name, the longitude and latitude of the town For plotting with Bokeh on a basemap, we need to convert from longitude & latitude to easting and northing. To do this we use the pyproj package to make this very simple. 1 2 3 4 5 6 7 def LongLat_to_EN ( long , lat ): try : easting , northing = transform ( Proj ( init = 'epsg:4326' ), Proj ( init = 'epsg:3857' ), long , lat ) return easting , northing except : return None , None This function can be used to generate the easting and northing for every town from it's longitude & latitude and add it to the dataframe. 1 summary_df [ 'E' ], summary_df [ 'N' ] = zip ( * summary_df . apply ( lambda x : LongLat_to_EN ( x [ 'Longitude' ], x [ 'Latitude' ]), axis = 1 )) Finally, it's time to plot our findings on a map. Before we initialise the map in Bokeh , for most plots, data tables and more in Bokeh , we need to put it in the ColumnDataSource form. We also initialise the interactivity when the user hovers over the data points on the plot. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 source = ColumnDataSource ( data = dict ( longitude = list ( summary_df [ 'E' ]), latitude = list ( summary_df [ 'N' ]), sizes = list ( summary_df [ 'Count' ] * 3 ), lettercount = list ( summary_df [ 'Count' ]), city_name = list ( summary_df [ 'City_Name' ]), letters = list ( summary_df . index ))) hover = HoverTool ( tooltips = [ ( \"Repeated Letter\" , \"@letters\" ), ( \"City Name\" , \"@city_name\" ), ( \"Count\" , \"@lettercount\" ) ]) Finally time for the plot! Now admittedly, I haven't found an easy way to find the limits of the graph, so this was made with a lot of trial and error (If you know a better way, please let me know!). 1 2 3 4 5 6 7 8 9 p = figure ( x_range = ( 20000000 , 17900000 ), y_range = ( - 6000000 , - 4000000 ), x_axis_type = \"mercator\" , y_axis_type = \"mercator\" , tools = [ hover , 'wheel_zoom' , 'save' ]) p . add_tile ( CARTODBPOSITRON ) p . circle ( x = 'longitude' , y = 'latitude' , size = 'sizes' , source = source , line_color = \"#FF0000\" , fill_color = \"#FF0000\" , fill_alpha = 0.05 )","tags":"Python","url":"https://jackmckew.dev/looking-for-patterns-in-city-names-interactive-plotting.html","loc":"https://jackmckew.dev/looking-for-patterns-in-city-names-interactive-plotting.html"},{"title":"PyCon AU 2019 Sunday In Summary","text":"This is a follow on from my last post PyCon AU 2019 Saturday In Summary . Day 2 The real costs of Open Source Sustainability \\@vmbrasseur The key takeaway that I got from this talk, was the typical reaction for problems which are far away from yourself or out of your control is to donate money. Vicky spoke about how sometimes money is not the solution to problems, specifically, for open source projects. Ways you can contribute can be summed up using the term Time, Talent, Treasure. Time: you can donate your time to help a cause, Talent: you can donate your skills and talents, Treasure: you can donate your treasures. Shipping your first Python package and automating future publishing \\@chriswilcox47 Packaging has always been a bit of enigma to me, and Chris Wilcox did an excellent job at explaining not only the structure behind a package, but also how to ship a package. One thing that I have noticed to make sure is to ensure your project structure is in place, and cookie cutter helps with this. Tox & Nox can be used to automate testing of your package over multiple versions and distributions of Python, so you can reassure your users that the package will work no matter the environment. It's dark and my lights aren't working (an asyncio success story) \\@jim_mussared This was one of the talks that really resonated with my previous experience in my thesis project working with the ESP8266. Jim gave a very funny and relatable talk on the experience of using Zigbee communications to link the lights in a new home. The universe as balls and springs: molecular dynamics in Python \\@lilyminium Jupyter notebooks can not only be used for developing, but also as presentations. Lily gave an in-depth talk about the analysis of molecular dynamics, presenting from a jupyter notebook which showed off the power of interactive visualizations making a very complex topic, simple and easy to understand. \"Git hook[ed]\" on images & up your documentation game \\@veronica_hanus As another person that appreciates visual cues to what changes were made in the past, I can definitely see why using Pyppeteer to hook a screenshot onto a git commit can make a massive difference on going back to the commit history and be able to see exactly what changes were made. Sunday Lightning Talks Personally, I really appreciated the talk on https://www.growstuff.org/ , a self proclaimed 'Tinder for Potatos'. Where users can put their plants they are growing & get a progress bar! Then they can interact with other growers, so possibly exchange and grow both the community and the plants. https://youtu.be/q2VmIUaOS9o?t=9 A few of the lightning talks really demonstrated how welcoming the Python & software community is. From Fashion at PyConAU 2019 showing how people can be their true self around a welcoming community to learning what it is like to be Jewish at a conference . To quote \\@UrcherAus , \"And can come out, presenting as female in public for the very first time, and we say to you, ‘welcome to #pyconau , I love your outfit'\". All the great ideas people gave me Learning that Google has a monolithic \"Monorepo\", where they store all of their projects in one repository to alleviate the problems of maintaining multiple repositories for varying projects that all depend on each other. Finding out that Blender has native support for Python scripting, and produces amazing renders. Very much so looking forward to finding some time to try out Blender and see if I can integrate Python and Blender. Watch this space for a future post on this topic! My to-do list after Pycon Day 2 ~~Write this blog post!~~ (Day 1), Look into tilemill (Day 1), ~~Understand~~ ~~mutable and immutable~~ ~~better (Day 1),~~ ~~Learn what~~ ~~super does in Python~~ ~~(Day 1),~~ Try make some generative art (Day 1), Looking into papermill + jupyterlab (Day 1), Have a go at using XGBoost and text (Day 1), Get (much) better at testing with pytest (Day 1), ~~Look into structuring python projects better~~ (Day 1), Move this website to static html with netlify and more (Day 1), ~~Look into~~ ~~singularity~~ ~~(Day 1),~~ ~~Look into~~ ~~GitFlow Workflow~~ ~~(Day 1),~~ Have a go at using Blender (Day 2), Try make and give a presentation with Jupyter (Day 2), Try making a plot in Plotly (Day 2), Look into Binder for distributing code (Day 2), Listen to Python Bytes podcasts (Day 2). Talks to catch up on \"Extracting tabular data from PDFs with Camelot & Excalibur\" - Vinayak Mehta , \"Using Dash by Plotly for Interactive Visualisation of Crime Data\" - Leo Broska , \"Using Python, Flask and Docker To Teach Web Pentesting\" - Zain Afzal, Carey Li , \"cuDF: RAPIDS GPU-Accelerated Dataframe Library\" - Mark Harris , \"3D Rendering with Python\" - Andrew Williams , Machine Learning and Cyber Security - Detecting malicious URLs in the haystack , Tunnel Snakes Rule! Bringing the many worlds of Python together , \"Goodbye Print Statements, Hello Debugger!\" - Nina Zakharenko , \"Insights into Social Media Data using Entropy Theory\" - Mars Geldard .","tags":"Python","url":"https://jackmckew.dev/pycon-au-2019-sunday-in-summary.html","loc":"https://jackmckew.dev/pycon-au-2019-sunday-in-summary.html"},{"title":"PyCon AU 2019 Saturday In Summary","text":"My first ever conference, learning things I'd never even think of, meeting lots of new people and making my to-do list full of new things to learn. All this happened over the weekend at PyConAU 2019 . This post is dedicated to all the fantastic people I met that gave me new perspectives on python programming and all the amazing talks I had the pleasure of attending. Please note that all the talks written about here were only the ones I was able to attend, there were many other amazing talks that I didn't get the opportunity to go to (will list the follow up ones later in the post) and would recommend to go through the youtube playlist of all the talks found here: https://www.youtube.com/user/PyConAU . A link to all the talks and descriptions can also be found in the headings. To make this post more digestible for the reader (you!), I have broken into parts which are linked here: Day 1 - How to communicate with businesses, metaclasses in python, making generative art, python applications in engineering, refactoring a large scale OSS project and the antipodean approach; Day 1 - Lightning talks; All the great ideas people gave me; My further to-do list following day 1. Day 1 Creating Lasting Change \\@aurynn Day 1 kicked off with a keynote talk from aurynn , who spoke about the lessons learned from talking to your boss. My personal key takeaways were: One word slides directs your focus to the talk rather than distracting, Communicating with people outside your discipline, interest area, etc is made much easier if you put whatever the topic is from their perspective, particularly in the workplace, putting things in terms of risk as this is what matters to businesses. It's Pythons All The Way Down: Python Types & Metaclasses Made Simple \\@judy2k Now classes are admittedly one of my weak points, so what better to do then go straight to metaclasses! My personal key takeaways were: If you run dir() on a type, it'll tell you all the capabilities of that type (eg, dir(int) returns [...,'__add__',...]); Complex numbers and functions have their own type in python; Descriptors override attribute access; There are two types of descriptors (data and non-data) where data descriptors are mutators; Metaclasses can be used as blueprints for generating classes; You can ensure classes are made appropriately with metaclasses. Pretty vector graphics - Playing with SVG in Python Amanda J Hogan By feeding strings of text (which are instructions), you can generate graphics with SVG, mix this with loops and you get generative art! Art being generally a very visual process, watch the video to get a better understanding, personally I liked this one: Python Applications in Infrastructure Planning and Civil Engineering Ben Chu As someone that works for a large engineering firm, this talk resonated with the possibilities of using python to automate jobs and get better results. Firstly, Ben spoke about using Jupyter notebooks to make interactive reports for the environmental teams to utilise for their analysis and using papermill to export these into different formats (excel, pdf, etc). Ben also used python to automate the verification stages of a proposed rail corridor location. By using requests & beautiful soup to scrape the NSW development application website along with a machine learning classification algorithm ( XGBoost ) for the developments impact on the rail corridor. Finally plotting this on an interactive map for the rail designers to use. How I auto-refactored a huge OSS project to use pytest \\@craigds2 Craig gave a great talk about how he used PyBowler, Pytest and importantly pytest-sugar to automatically refactor existing testing framework for a huge open source project GDAL . This post has inspired me to do more testing & refactoring on my code as I develop things! The Antipodes \\@brandon_rhodes Personally this talk really resonated with a habit that I have been trying to employ in my life recently. The basic principle being behind the meaning of the idea of antipodean s , someone standing on the exact other side of the planet from you. I thought this was an amazing segway for moving to a new framework for structuring communications. As most of us do, whenever replying to communication from someone, we normally start with me, me, me, me, you. For example, if someone asked us to make a decision, typically & personally, I would start the reply with stating why I had got to the decision, finally ending the message with the decision and the next steps. A technique that I have recently started employing on my messaging is writing as I normally would and before hitting send, moving what matters most to the reader (the decision) to the top, followed by next steps and then going through all the reasons why I possibly had made that decision. Saturday Lightning Talks Personally, I thought the most interesting lightning talk was about procedurally generating planets, modelling them in 3D and then trying to estimate if the climate on them https://youtu.be/AJqcxEzRdSY?t=140 . What I think was the crowd favourite, was the History and Politics of Australian supermarkets and their mergers . Definitely worth the watch. All the great ideas people gave me Shoutout to \\@davidjb_ who made me aware of modern static html sites with using Pelican and Netlify . By using a repository to store all the content, you can use these tools in combination to make an automated workflow for a CMS (Content management system) for deploying a website. Will definitely be looking into this for this very website! Shoutout to a guy (who doesn't have socials) that I met a pub that made me aware of Singularity , an alternative to docker, will have to do further research and testing on this one, so watch this space! Further to this, I was also made aware of the Gitflow Workflow , as using Git is admittedly one of my other weakpoints, so will definitely be trying to bring this principle into my development pipeline. My to-do list after Pycon Day 1 Write this blog post! Look into tilemill , Understand mutable and immutable better, Learn what super does in Python , Try make some generative art, Looking into papermill + jupyterlab , Have a go at using XGBoost and text, Get (much) better at testing with pytest , Look into structuring python projects better, Move this website to static html with netlify and more, Look into singularity , Look into GitFlow Workflow .","tags":"Python","url":"https://jackmckew.dev/pycon-au-2019-saturday-in-summary.html","loc":"https://jackmckew.dev/pycon-au-2019-saturday-in-summary.html"},{"title":"Hands On Machine Learning Chapter 2","text":"Chapter 2 is an end to end machine learning project, in which you pretend to be a recently hired data scientist for a real estate company. It cannot be emphasized enough that when learning about machine learning or any topic for that matter, it is best to actually experiment with real-world data and scenarios. Firstly my personal opinion on how a machine learning (or data science) project is structured is a series of steps: Get an understanding of the expected goal or outcome (eg frame the problem), Get an understanding of the current process (if there is one), Get the data behind the problem (or what you expect will be useful for solving the problem), Explore and visualize the data to gain insights, Prepare/massage the data ready for input into algorithms or models, Select a model/algorithm and train it, Tune your model/algorithm to the best you can, Present the solution to the original stakeholder (take the stakeholder on a journey), Launch, monitor and maintain your system. I believe, that if you follow these steps at a minimum, you will find success with your data science/machine learning projects. This methodology also applies for any type of project and can be enhanced with tweaks where you see fit. Anyway, back to Chapter 2, it is very much so reinforced that you should select an appropriate way of scoring performance of your algorithms/models (otherwise you can't compare them effectively). For regression tasks, generally the preferred performance measure is RMSE (Root Mean Square Error), but this may not always be the case depending on the context of the problem. For example, if the data set has many outliers (or outlier groups), it may prove beneficial to consider MAE (Mean Absolute Error). Assumptions are in my opinion, the downfall of any collaborative project if they are not transparent or communicated. A practice that I personally do and recommend doing is to try your best to document every assumption you may make in a project, such that anyone later on can pick up where you were and understand why you chose to do something a certain way. As per Chapter 1, it is again reinforced to split your data set up into a training set, a testing set and a validation set; albeit a more practical example of this concept in action. Whereas you use the K-fold validations with GridSearchCV to understand the best performing hyper parameters for your algorithm/model. Personally, in Chapter 2, the most difficult part to understand is around the pipeline for preparing data ready for use in algorithms/models. Pipelines are essentially a sequence of steps that need to be completed in order before the data is ready. Stemming from the Scikit-learn design principles, I found this the best way to understand the possible steps in a data preparation pipeline: Estimators Any object that estimates parameters based on a data set is known as an Estimator. For example, if you had a data set with lots of missing values, you could estimate what to fill these gaps with an imputer, then you could choose to use the median of the dataset if appropriate. Transformers Any object that transformers a data set is known as a Transformer. For example, if you wanted to now fill those gaps in the data set previously mentioned with the mean, you would use a transformer to 'insert' the median wherever empty values were found. Predictors Any object that is capable of making predictions given a dataset is known as a Predictor. For example, a linear regression model is a predictor, using one feature to extrapolate another feature.","tags":"Machine Learning","url":"https://jackmckew.dev/hands-on-machine-learning-chapter-2.html","loc":"https://jackmckew.dev/hands-on-machine-learning-chapter-2.html"},{"title":"Python and OCR","text":"This post will demonstrate how to extract the text out of a photo, whether it being handwritten, typed or just a photo of text in the world using Python and OCR (Optical Character Recognition). While this is something that humans do particularly well at distinguishing letters, it is a form of semi-structured data. OCR just like humans also has it's limitations, for example, if you were trying to read someone with really difficult handwriting, it could be a big challenge. In this post, we will use the Tesseract engine (an open source Google project) to undertake the OCR process for us. First of all, as always, we must create a new virtual environment for our project to live in or use a package manager such as Anaconda (as explained in my post Episode - 8: Anaconda . Once initialized, we want to install a few packages to help us on our quest for OCR. Both Pillow and PyTesseract , if you are using Anaconda like I did, you will want to specifically use pip, not conda, to install these packages. Further to this, you will need to install the binary of the Tesseract-OCR engine, which installation instructions can be found: https://github.com/UB-Mannheim/tesseract/wiki . Now we are finally ready to test the engine and see if we can extract text out of an image, first of all we will start with a 'well' written example, the 'logo' of this website! Of course, we have still yet to write any code, so naturally, that is the next step. As always in a python project, you will need to import all the dependencies of the project, in this case, it will be Image from the PIL (pillow) package, and pytesseract (the python wrapper around the Tesseract Engine). 1 2 from PIL import Image import pytesseract Now that we have our dependencies loaded, it's time to check out the documentation behind Pillow and pytesseract to know how to operate the tools, consider these an instruction manual. The documentation for these tools can be found: Pillow , PyTesseract . Luckily for us, the developers have made this so simple it could be a one liner: 1 print ( pytesseract . image_to_string ( Image . open ( 'images/example.png' ))) Which outputs in the console from the example image above: JACK MCKEW'S\\ BLOG\\ Python enthusiast, electrical engineer and\\ tinkerer Great! We can confirm that the text that the tesseract engine detected, is in fact, exactly what the example we gave it was. However, let's go a bit out of the way to make this a function such that it can be called more easily with the filepath to the image as a string. 1 2 3 4 5 6 7 8 9 10 from PIL import Image import pytesseract def ocr_convert_to_text ( filename ): text = pytesseract . image_to_string ( Image . open ( filename )) return text extracted_text = ocr_convert_to_text ( 'images/example.png' ) print ( extracted_text ) Now we have a function that we can call with a file path to easily convert our images to text. Now let's give the tesseract engine a bit of challenge with a full page of handwritten text: Ad Bb Cc Da Fe FEF Ge Hh Ii IS RR lt Hm We\\ 00 PP Ag Rr SsT# Uu Vv Ww Xx 44 Le\\ Aa BS (36 72 Re bebe nme #% Ua ti ke\\ At Au Hee Bo In Fn Le Sim \\ \\(y Rep Ha Wy\\ Ye Unu Uppy bb otn tx 79 Ww 2A\\ Sr be Liki 4\\ IR AS67890\\ so cool! New \\|neerndtinas release of mast\\ famous APP for exhorting Printed text 40\\ handwritten \"Sinyak\\ PacK mY box with. five dozen uguor Jugs\\ Don't 62 @. Earn \\\\) & Put 12 Jar.\\ Ingredienes: Zuss, Chis, CAR Lid.\\ (Sécrez info), kndw tb. 3\\ Wo xr dA h-(H4+F 060 Cheah]\\ ChiP & Dae.\" fava ys ie m4 mind.\\ Jackdaws uve m4 bi? sPhinx Of quare 2.\\ The five boxing withrds JumP quick.\\ How vexiegi quick date 2ebras sump!\\ {0.0} ainsa & crepim pa. bau! -) Using the same code, we were able to determine most of the text out of the picture that the tesseract engine was given. Obviously this is not perfect, but it is a whole lot easier than typing it all in by hand. For a bit of another challenge and to demonstrate the capabilities, let's try some Australian number plates: (CSE) XcB-962 (66M-059\\ X2ZH:709) EEH:133) (GAA729) Obviously this can and has had a big impact on the way people can utilize images to make their life easier, from scanning in your handwritten notes at school and converting straight on to the computer, to being able to add all the contact information in your phone from a business card. How can OCR help your life at work or at home? Please let me know in the comments..","tags":"Python","url":"https://jackmckew.dev/python-and-ocr.html","loc":"https://jackmckew.dev/python-and-ocr.html"},{"title":"Python and Data Security (Hashing Algorithms)","text":"Data security is becoming more and more prevalent in today's society than ever before. We must make a conscious effort to secure both our physical lives, but also our digital lives as well. With data privacy, sharing of information and access control becoming integrated into most people's life in some way or another. Since this topic is so wide and deep, this will most likely become a series of posts as I am passionate around data security and enjoy getting stuck right into the math behind it. This post will be around hashing algorithms but future topics will include: Hashing Algorithms (this post), Modular Arithmetic and why it's used, Securely sharing keys, Methods of encryption, Methods of data security, Analyzing security weaknesses, Many more. As above, this post is dedicated to hashing algorithms and how to interface with them with Python for data security. What is a Hashing Algorithm The sole purpose of a hashing algorithm is to generate a safe hash which in turn raises the questions of what is a hash and what makes it safe? A hash is a value computed from a base input number using a hashing function. With a hashing function being: A hash function is any function that can be used to map data of arbitrary size onto data of a fixed size. https://en.wikipedia.org/wiki/Hash_function The hashing algorithm is intrinsically designed to be a one-way function, meaning it is impractical to revert. Although, as history has shown, as computing advances are made hashing algorithms are becoming compromised. A prime example of this being the MD5 algorithm, which was designed and used a cryptographic hash function (data security), but is now so simply reverse, that it is used for verifying data transfers. There are certain characteristics around what the perfect or ideal hash function for data security should possess: Easy/speed of computation, Impossible/impractical to regenerate source data/message (brute force as only option), Unique hashes for data (also known as hash collisions when there are duplicate hashes), Any change is source data should change the hash value (known as the avalanche effect). What is hashing used for in practice Hashing algorithms for data security in the real world is used in a variety of situations from ensuring files were successfully delivered correctly or to store sensitive/private information. If you are reading this, I can almost guarantee that you have some interface with a hashing algorithm right now! Whether it be how you're password is stored to indexing data in a database. Using hashes with Python This will be a simple use-case of a hashing algorithm using Python to securely convert passwords and how to verify against them (storing the hashed data is it's own beast in itself). Please note I will be utilizing the passlib package which contains over 30 password hashing algorithms, as well as a framework for managing existing password hashes. First of all we must select a hashing algorithm to use, to help with this from the team at passlib they have provided a basic guideline of questions : Does the hash need to be natively supported by your operating system's crypt() api,in order to allow inter-operation with third-party applications on the host? If yes, the right choice is either bcrypt for BSD variants,or sha512_crypt for Linux; since these are natively supported. If no, continue... Does your hosting provider allow you to install C extensions? If no, you probably want to use pbkdf2_sha256 ,as this currently has the fastest pure-python backend. If they allow C extensions, continue... Do you want to use the latest & greatest, and don't mind increased memory usage when hashing? argon2 is a next-generation hashing algorithm,attempting to become the new standard. It's design has been being slightly tweaked since 2013, but will quite likely become the standard in the next few years. You'll need to install the argon2_cffi support library. If you want something secure, but more battle tested, continue... The top choices left are bcrypt and pbkdf2_sha256 . Both have advantages, and their respective rough edges; though currently the balance is in favor of bcrypt (pbkdf2 can be cracked somewhat more efficiently). If choosing bcrypt, we strongly recommend installing the bcrypt support library on non-BSD operating systems. If choosing pbkdf2, especially on python2 \\< 2.7.8 and python 3 \\< 3.4, you will probably want to install fastpbk2 support library. From this, we will use the argon2 hashing algorithm. As normal, it is best practice to set up a virtual environment (or conda environment) and install the dependencies, in this case passlib. First of all, import the hashing algorithm you wish to use from the passlib package: 1 from passlib.hash import argon2 Following importing the hashing algorithm, to hash the password in our case is very simple and we can have a peak at what the output hash looks like: 1 2 3 hash = argon2 . hash ( \"super_secret_password\" ) print ( hash ) \\ \\(argon2i\\\\) v=19\\ \\(m=102400,t=2,p=8\\\\) NqY05lyrtdb6v/ee03pvrQ\\$mvLTquN71JPjuC+S9QNXYA The first section (\"\\ \\(argon2i\\\\) v=19\\ \\(m=102400,t=2,p=8\\\\) \") is the header information, showing the parameters that the algorithm used to generate the hash. While this seems as if it would make the algorithm easier to break, imagine a scenario where every password is hashed using an hashing algorithm with randomised parameters; verifying passwords would be a nightmare. Let's further break down what this represents: \\$argon2i - the variant of Argon2 algorithm being used, \\$v=19 - the version of Argon2 being used, \\$m=102400,t=2,p=8 - the memory (m), iterations (t) and parallelism (p) parameters being used, \\$NqY05lyrtdb6v/ee03pvrQ - the base64-encoded salt (added randomness), using standard base64 encoding and no padding, \\$mvLTquN71JPjuC+S9QNXYA - the base64-encoded hashed password (derived key), using standard base64 encoding and no padding. If we run this again, we can check that the outputs are completely different due to the randomly generated salt. 1 2 3 hash = argon2 . hash ( \"super_secret_password\" ) print ( hash ) \\ \\(argon2i\\\\) v=19\\ \\(m=102400,t=2,p=8\\\\) 8f4/x7hXitGacy6F8N67dw\\$/jPKQ98vLQCxkboxRlHa/g Now that we've generated our new passwords, stored them away in a secure database somewhere, using a secure method of communication somehow, our user wants to login with the password they signed up with (\"super_secret_password\") and we have to check if this is the correct password. To do this with passlib, it is as simply as calling the .verify function with the plaintext and the equivalent hash which will return a boolean value determining whether of not the password is correct or not. 1 print ( argon2 . verify ( \"super_secret_password\" , hash )) True Hooray! Our password verification system works, now we would like to check that if the user inputs a incorrect password that our algorithm returns correctly (false). 1 print ( argon2 . verify ( \"user_name\" , hash )) False Conclusion Hopefully this has given you some insight into what hashing algorithms are, how they are used and how to use them with Python. They can both be an extremely powerful tool for securing data, however, must always be revisited later on down the track as advancements are made and your system may now be compromised.","tags":"Python","url":"https://jackmckew.dev/python-and-data-security-hashing-algorithms.html","loc":"https://jackmckew.dev/python-and-data-security-hashing-algorithms.html"},{"title":"Hands On Machine Learning Chapter 1","text":"I've recently been making my way through the book \"Hands-On Machine Learning with Scikit-Learn and Tensorflow\", and thought I will put a summary of the chapter as a post, along with my personal answers to each of the chapter's exercises. The book in particular is published by O'Reilly and can be found https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/ . Chapter 1 is around defining when and where to apply machine learning to a problem, as it is not always the best approach to solving a problem. Following, making sure to be aware of the strengths and weaknesses of each 'type' of machine learning systems. Types of machine learning systems can be broken into three broad categories: Is the model trained with human supervision? (supervised, unsupervised, semisupervised and reinforcement learning) Does the model learn incrementally on the fly or not? (online or batch learning) Does the model work by simply comparing new vs known data or detect patterns to build a prediction? (instance based or model based learning) The book then goes into detail around these, I will not as many resources around these topics are abundantly available on the internet. Chapter 1 also goes onto to detail the importance of defining the problem, 'clean' data, training vs testing data and comparing different techniques. From here on are the chapter 1 exercise questions, with my personal answer, and the book's answer. 1. How would you define Machine Learning? My answer: Self-sufficiently improving on a technique.\\ Book's answer: Machine Learning is about building systems that can learn from data. Learning means getting better at some task, given some performance measure. 2. Can you name four types of problems where it shines? My answer: Processes which either involve: many complex steps, steps that require 'tuning', ever-changing systems based on variables or to gain insight on a problem from a new perspective.\\ Book's answer: Machine Learning is great for complex problems for which we have no algorithmic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). 3. What is a labeled training set? My answer: A data set with the associated desired answer.\\ Book's answer: A labeled training set is a training set that contains the desired solution (a.k.a. alabel) for each instance. 4. What are the two most common supervised tasks? My answer: Regression and classification.\\ Book's answer: The two most common supervised tasks are regression and classification. 5. Can you name four common unsupervised tasks? My answer: Association rule learning, anomaly detection, simplification and clustering.\\ Book's answer: Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning. 6. What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains? My answer; Reinforcement learning.\\ Book's answer: Reinforcement Learning is likely to perform best if we want a robot to learn to walk in various unknown terrains since this is typically the type of problem that Reinforcement Learning tackles. It might be possible to express the problem as a supervised or semisupervised learning problem, but it would be less natural. 7. What type of algorithm would you use to segment your customers into multiple groups? My answer: k-neighbour clustering.\\ Book's answer: If you don't know how to define the groups, then you can use a clustering algorithm (unsupervised learning) to segment your customers into clusters of similar customers. However, if you know what groups you would like to have, then you can feed many examples of each group to a classification algorithm (supervised learning), and it will classify all your customers into these groups . 8. Would you frame the problem of spam detection as a supervised learning problem or an unsupervised learning problem? My answer: Supervised or semisupervised.\\ Book's answer: Spam detection is a typical supervised learning problem: the algorithm is fed many emails along with their label (spam or not spam). 9. What is an online learning system My answer: A system that learns incrementally on the fly from new data.\\ Book's answer: An online learning system can learn incrementally, as opposed to a batch learning system. This makes it capable of adapting rapidly to both changing data and autonomous systems, and of training on very large quantities of data. 10. What is out-of-core learning? My answer: Whenever the data set is too large to fit on a single machine.\\ Book's answer: Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer's main memory. An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn from these minibatches. 11. What type of learning algorithm relies on a similarity measure to make predictions? My answer: Instance based learning (comparison of new vs old).\\ Book's answer: An instance-based learning system learns the training data by heart; then, when given a new instance, it uses a similarity measure to find the most similar learned instances and uses them to make predictions. 12. What is the difference between a model parameter and a learning algorithm's hyperparameter? My answer: A model parameter directly influences and influenced by the way the model behaves, while a hyperparameter is dictates how the model should behave (eg, learn fast or slow).\\ Book's answer: A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). 13. What do model-based learning algorithms search for? What is the most common strategy they use to succeed? How do they make predictions? My answer: Relationships or trends within the data. Regression is used to find a possible solution to fit to the data and predictions are then extrapolated.\\ Book's answer: Model-based learning algorithms search for an optimal value for the model parameters such that the model will generalize well to new instances. We usually train such systems by minimizing a cost function that measures how bad the system is at making predictions on the training data, plus a penalty for model complexity if the model is regularized. To make predictions, we feed the new instance's features into the model's prediction function, using the parameter values found by the learning algorithm. 14. Can you name four of the main challenges in Machine Learning? My answer: Quality, quantity, irrelevant sections and incorrectly modeled.\\ Book's answer: Some of the main challenges in Machine Learning are the lack of data, poor data quality, nonrepresentative data, uninformative features, excessively simple models that underfit the training data, and excessively complex models that overfit the data 15. If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions? My answer: Overfitted or underfitted to the data. Simplify the model, get more useful data and/or reduce noise.\\ Book's answer: If a model performs great on the training data but generalizes poorly to new instances, the model is likely overfitting the training data (or we got extremely lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. What is a test set and why would you want to use it? My answer: A test set is used to understand how your model interacts with unseen data without having to collect new information.\\ Book's answer: A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 17. What is the purpose of a validation set? My answer: To understand how accurate the system interfaces with unseen data.\\ Book's answer: A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters. 18. What can go wrong if you tune hyperparameters using the test set? My answer: The system has been specifically setup to perform under these conditions and may perform unexpectedly in new situations.\\ Book's answer: If you tune hyperparameters using the test set, you risk overfitting the test set, and the generalization error you measure will be optimistic (you may launch a model that performs worse than you expect). 19. What is cross-validation and why would you prefer it to a validation set? My answer: By dividing the training set further into categories, then trained and validated against combinations of other categories.\\ Book's answer: Cross-validation is a technique that makes it possible to compare models (for model selection and hyperparameter tuning) without the need for a separate vali‐ dation set. This saves precious training data.","tags":"Machine Learning","url":"https://jackmckew.dev/hands-on-machine-learning-chapter-1.html","loc":"https://jackmckew.dev/hands-on-machine-learning-chapter-1.html"},{"title":"Parallel Processing in Python","text":"Parallel processing is a mode of operation where the task is executed simultaneously in multiple processors in the same computer. The purpose of this is intended to reduce the overall processing time, however, there is often overhead between communicating processes. For small tasks, the overhead is detrimental to the length of processing, increasing the overall time taken. For this post we will be using the multiprocessing package in Python. Multiprocessing is apart of the standard library within Python and is a package that supports spawning processes using an API similar to the threading module (also apart of the standard library). The main benefit of the multiprocessing package, is that it disregards the global interpreter lock (GIL), by using sub processes instead of threads. The number of processors or threads in your computer dictates the maximum number of processes you can run at a time. To add flexibility to your program when it may be run across multiple machines, it is good practice to make use of the cpu_count() function apart of the multiprocessing, as shown below (please note f strings were only introduced in Python 3.6). 1 2 import multiprocessing as mp print ( f \"Maximum number of processes: { mp . cpu_count () } \" ) In parallel processing, there are two types of execution: Synchronous and Asynchronous. Synchronous meaning where the processes are completed in the same order in which it was started, such that, the output is (normally) in order. While asynchronous means the processes can be in any order, and while the output can be mixed, is usually computed faster. Within multiprocessing there are 2 main classes that you will use for parallel processing: Pool & Process. The two classes are intended to be used in completely different scenarios, but still utilize parallel processing. Pool is beneficial for when you have a long list that need to be processed and combined back together at the end. Process is beneficial for when you need multiple functions running simultaneously, albeit not the same. The Pool Class The pool class has four methods that are particular useful: Pool.apply Pool.map Pool.apply_async Pool.map_async Before we tackle the asynchronous variants of the pool methods (async suffix). Here is a simple example using Pool.apply and Pool.map. We initialize the number of processes to however many is available or the maximum of the system. 1 2 3 4 5 6 def power_n_minus_1 ( value ): return value ** value - 1 if __name__ == '__main__' : pool = mp . Pool ( processes = mp . cpu_count ()) results = [ pool . apply ( power_n_minus_1 , args = ( x ,)) for x in range ( 1 , 5 )] print ( results ) With the results being: [1, 2, 9, 64] or 1\\&#94;0, 2\\&#94;1,3\\&#94;2,4\\&#94;3. This can also be achieved similarly with Pool.map. 1 2 3 4 5 6 def power_n_minus_1 ( value ): return value ** value - 1 if __name__ == '__main__' : pool = mp . Pool ( processes = mp . cpu_count ()) results = pool . map ( power_n_minus_1 , range ( 1 , 5 )) print ( results ) Both of these will lock the main program that is calling them until all processes in the pool are finished, use this if you want to obtain results in a particular order. However if you don't care about the order and want to retrieve results as soon as they finished, then use the async variant. 1 2 3 4 5 6 7 def power_n_minus_1 ( value ): return value ** value - 1 if __name__ == '__main__' : pool = mp . Pool ( processes = mp . cpu_count ()) outputs = [ pool . apply_async ( power_n_minus_1 , args = ( x ,)) for x in range ( 1 , 5 )] results = [ p . get () for p in outputs ] print ( results ) The Process Class The process class is the most basic approach to parallel processing from multiprocessing package. Here we will use a simple queue function to generate 10 random numbers in parallel. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 import multiprocessing as mp import random output = mp . Queue () def rand_number ( lower_limit , upper_limit , output ): output . put ( random . randint ( lower_limit , upper_limit )) if __name__ == \"__main__\" : processes = [ mp . Process ( target = rand_number , args = ( 1 , 101 , output )) for x in range ( 10 )] for p in processes : p . start () for p in processes : p . join () results = [ output . get () for p in processes ] print ( results ) With the result being: [76, 40, 76, 27, 64, 94, 30, 71, 70, 40]. By utilizing the multiprocessing package in Python or parallel computing concepts in general, you will now be able to dramatically increase computation times (for large processes).","tags":"Python","url":"https://jackmckew.dev/parallel-processing-in-python.html","loc":"https://jackmckew.dev/parallel-processing-in-python.html"},{"title":"Distributing Python Code","text":"This post will cover a way of distributing Python code such that is can be used by someone that does not have Python installed. One of the major drawbacks with Python that the gap is slowly being closed is how easy it is to distribute Python code. At a minimum, the computer that is to run the code must have the Python compiler (or equivalent). Now while this has been progressively included in more operating systems as a default (May update of Windows being the latest), you must still develop as such that is not present on the users' PC. For this post, I will show you a basic piece of code to demonstrate how it will be packaged and distributed to your users. To show a basic dialog box on the screen with the following code: 1 2 3 import ctypes ctypes . windll . user32 . MessageBoxW ( 0 , \"Hello Windows!\" , \"PyInstaller Example\" , 1 ) Which shows the user with this dialog box: Now to package this code into an executable (.exe), there are multiple packages out there that are possible to use, some examples of these are: cx_freeze py2exe PyInstaller For this post, I will use PyInstaller as it is what I am most familiar with, please get in touch with me if you believe any other package is better suited. I have created an environment in anaconda named \"pyinstall\", in which I have installed PyInstaller with the command \"conda install -c conda-forge pyinstaller\", which includes Python 3.7.3 due to anaconda's packaging system (thereby including ctypes from the standard library). Now to use the PyInstaller package, just open Anaconda Prompt (or cmd if anaconda.exe is in your PATH). Navigate to where the python code is stored, and run the command \"pyinstaller \\<name_of_program>.py. See below for an example: This will create a build & dist folder within the directory you navigated to, which contains the python application and all the required files will be put inside the dist folder which will be shipped to the user later on. There are many other settings that you can use to customize how your package gets built and more, but I won't go into that in this post. Now if we go into the dist folder and find the .exe (which will have the same name as your python file unless you change this setting). Once you hit run, you'll be met by this screen: Now you can send this executable to anyone (although most antivirus will stop you) and it will run on their PC!","tags":"Python","url":"https://jackmckew.dev/distributing-python-code.html","loc":"https://jackmckew.dev/distributing-python-code.html"},{"title":"Python Decorators Explained","text":"Python decorators are one of the most difficult concepts in Python to grasp, and subsequently a lot of beginners struggle. However they help to shorten code and make it more 'Pythonic'. This post is going to go through some basic examples where decorators can shorten your code. Firstly you have to understand functions within python: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def hello ( name = 'Jack' ): return \"Hello \" + name print ( hello ()) # output: 'Hello Jack' greeting = hello # assign a function to a variable, with no parentheses as we are not calling it print ( greeting ()) # output: 'Hello Jack' del hello print ( hello ()) # output: NameError print ( greeting ()) # output: 'Hello Jack' As we can see above we can give functions default arguments (the string 'Jack' for the name variable in hello). Assign functions to variables (ensuring the parentheses are not included otherwise we would be assigning to the returning value from the function. Remove previous functions now that we have 'copied' the function over. Now to take the next step into functions within Python, by defining functions within functions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 def hello ( name = 'Jack' ): print ( \"You're now inside the hello() function\" ) def greeting (): return \"Now you are in the greeting() function\" def welcome (): return \"Now you are in the welcome() function\" print ( greet ()) print ( welcome ()) print ( \"You are now back in the hello() function\" hello () # outputs: \"You're now inside the hello() function\" # \"Now you are in the greeting() function\" # \"Now you are in the welcome() function\" # \"You are now back in the hello() function\" welcome () # output: NameError: name 'welcome' is not defined Now we can make nested functions (functions within functions), the next step is, functions returning functions. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 def hello ( name = 'Jack' ): def greeting (): return \"Now you are in the greeting() function\" def welcome (): return \"Now you are in the welcome() function\" if ( name == 'Jack' ): return greeting else : return welcome returned_function = hello () print ( returned_function ) # output: <function greeting at 0x7f2143c01500> # This clearly shows that the returned function is the greeting() function within the hello() function print ( returned_function ()): # output: \"Now you are in the greeting() function\" From earlier, we know that if we don't include the parentheses then the function does not executed. Another extension of the way this is formatted is that we can now call hello()() which outputs \"Now you are in the greeting() function\". 1 2 3 4 5 6 7 8 9 10 def hello ( name = \"Jack\" ): return \"Hello \" + name def preFunction ( function ): print ( \"This is the prefunction function\" ) print ( hello ()) preFunction ( hello ) # output: \"This is the prefunction function\" # \"Hello Jack\" Now you have all the knowledge to learn what decorators really are, they let you execute code before and after a function. The code above is actually a decorator, but let's make it more usable. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 def new_decorator ( function ): def functionWrapped (): print ( \"This is the pre function\" ) function () print ( \"This is the post function\" ) return functionWrapped def function_requiring_decoration (): print ( \"I need some decorations!\" ) function_requiring_decoration () # output: \"I need some decorations\" function_requiring_decoration = new_decorator ( function_requiring_decoration ) # Now our function is wrapped by functionWrapped() function_requiring_decoration () # output: \"This is the pre function\" # \"I need some decoration\" # \"This is the post function\" Now you've made a decorator! We've just used what we learned previously to modify it's behaviour in one way or another. Now to make it even more concise we can just the @ symbol. Here is how we could have used the previous code with @ symbol. 1 2 3 4 5 6 7 8 9 10 11 @new_decorator def function_requiring_decoration (): print ( \"I need some decorations!\" ) function_requiring_decoration () # output: \"This is the pre function\" # \"I need some decoration\" # \"This is the post function\" # The @ operator is a short way of saying: function_requiring_decoration = new_decorator ( function_requiring_decoration ) Hopefully now you are ready to go and explore the world of -decorators within Python, they can be used quite powerfully and allow for you to reuse code and extend capabilities. Some of the best examples for decorators are for authentication or logging, however I will not cover them as they are extensively documented over the internet.","tags":"Python","url":"https://jackmckew.dev/python-decorators-explained.html","loc":"https://jackmckew.dev/python-decorators-explained.html"},{"title":"Explained: Voltage Drop","text":"Voltage drop is a electrical phenomenon in that wires carrying current always have resistance, or impedance to the current flow. Voltage drop is defined as the amount of loss that occurs through part of or all of a circuit due to resistance/impedance. The most well known analogy for explaining voltage, current and voltage drop is a hose carrying water. In the garden hose, the water pressure is the voltage, the amount of water flowing is the current and the type and size of the hose makes up the resistance. Thus meaning that voltage drop is the loss of water pressure from the supply end of the hose to the output. When designing electrical systems within Australia and New Zealand, we are required to design to Australian standards. For voltage drop, the relevant standards as AS/NZS3000 (Wiring Rules) and AS/NZS3008 (Cable Selection). Where AS/NZS3000 nominates the limits to conform to (5% maximum from point of supply) and AS3008 dictates multiple ways that voltage drop can be calculated. For this post, I will demonstrate a simplified method that is outlined in AS3000 Table C7 where it specifies 'Am per %Vd' (Amp meters per % voltage drop) for each cable size: Cable Conductor Size Single Phase (230V) Am per %Vd Three Phase (400V) Am per %Vd 1mm&#94;2&#94; 45 90 1.5mm&#94;2&#94; 70 140 2.5mm&#94;2&#94; 128 256 4mm&#94;2&#94; 205 412 6mm&#94;2&#94; 306 615 10mm&#94;2&#94; 515 1034 16mm&#94;2&#94; 818 1643 25mm&#94;2&#94; 1289 2588 35mm&#94;2&#94; 1773 3560 50mm&#94;2&#94; 2377 4772 70mm&#94;2&#94; 3342 6712 95mm&#94;2&#94; 4445 8927 For example, a 50m run of 10mm\\&#94;2&#94; cable carrying 3 phase 32A will result in 5% drop: 32A * 50m = 1600 / 1034 = 1.5%. In future posts, I will go into the various ways that AS/NZS3008 demonstrates ways of calculating voltage drop.","tags":"Engineering","url":"https://jackmckew.dev/explained-voltage-drop.html","loc":"https://jackmckew.dev/explained-voltage-drop.html"},{"title":"What is MongoDB?","text":"Recently after looking for a different flavour of database apart from MySQL (which is what I am personally use to), I had always heard about MongoDB. So after some investigation, I found that MongoDB has a platform MongoDB University to familiarize yourself with their product. I completed their very first introductory course M001: MongoDB Basics last week, I found it very gentle in the introduction to database management and exploring data sets. This post is dedicated my take on the course and the key takeaways from my point of view. The course is broken into multiple chapters in which a chapter is released each week for the duration of the course. For example, the basics course was broken up into: Intro to MongoDB, Mongo Compass and Basic Queries, Create, Read, Update and Delete (CRUD) operations and more, MongoDB queries. Following all the chapters, you are faced with a final exam which tests if you were participating/listening in the earlier chapters. If you are concerned that you may struggle, this final exam is made up of a few multiple choice questions based on querying the data sets used in the chapters. MongoDB is a open source document-oriented database program, classified as a NoSQL database and utilizes JSON-like documents with a schema. They also provide a tool to help sift through the database called 'Compass'. Personally, I really enjoy the functionality within Compass with plotting geographical data, presenting data type variances across the fields in a document and many other features. I found Compass one of the most appealing features as someone that constantly seeks to gain insight from data. Queries within MongoDB are structured like a dictionary in Python, where the field in the document is passed the key and the criteria is the value. For example, a basic query to return all documents within a MongoDB database with score equal to 7 would be: 1 { score : 7 } As a mainly Python developer, I found this to be very appealing as I find myself using dictionaries constantly when writing Python code, and by MongoDB using this format makes for an easy connection between the two. CRUD operations, are the fundamentals on actually using a database usefully. Through the Mongo shell you are able to add documents to the MongoDB database through JSON, XML, etc data formats. Projections within MongoDB are used to specify or restrict the fields to return with the filtered documents if you are specifically looking at a few fields within a densely populated document. In addition to the way queries are structured for filtering documents, it is also possible to use one of the many query or projection operators to further filter the documents. For example a query to return all documents with a score greater than 7 would be: 1 { score : { $ gte : 7 }} This sums up all of the takeaways from the M001 course for MongoDB that I found. I look forward to taking more of the courses on MongoDB university to gain a greater understanding and be able to utilise MongoDB across some of my projects.","tags":"Software Development","url":"https://jackmckew.dev/what-is-mongodb.html","loc":"https://jackmckew.dev/what-is-mongodb.html"},{"title":"Efficient Frontier for Balancing Portfolios","text":"Following last 2 weeks' posts ( Python for the Finance Industry & Portfolio Balancing with Historical Stock Data ), we now know how to extract historical records on stock information from the ASX through an API, present it in a graph using matplotlib, and how to balance a portfolio using randomly generated portfolios. This post is to demonstrate a method in balancing portfolios that does not depend on generating random portfolios, but rather mathematically determining the extremities of boundaries for effective portfolios using the SciPy optimize function (similar to that of Excel's 'solver' ). Returning to last weeks' post when the budget allocations to assets were determined from randomly generated portfolios, it was presented on the graph below: From this plot, it can be visualized that it forms an arch line between the yellow and red crosses. This line is called the efficient frontier . The efficient frontier represents the set of optimal portfolios that offer the highest expected return for a defined level of risk or the lowest risk for a given level of expected return. Simply this means, all the dots (portfolios) to the right of the line will give you a higher risk for the same returns. First of all we must mathematically determine the portfolio with the maximum Sharpe ratio as the greater a portfolio's Sharpe ratio, the better it's risk-adjusted performance. Sharpe ratio is calculated using the formula below: To find the maximum of the Sharpe Ratio programmatically we follow these steps: Firstly, define the formula as the function neg_sharpe_ratio (take note that to find the maximum of function in SciPy , we use the minimize function with an inverse sign), In the max_sharpe_ratio function, define arguments to be passed into the SciPy minimize function: neg_sharpe_ratio: function to be minimized, num*[1/num_assets]: initial guess which is evenly distributed array of values, Arguments that are to be passed into the objective function (neg_sharpe_ratio), Method of Sequential Lease Squares Programming, there are many others which can be seen here, Bounds: between 0% and 100% of our budget allocation, Constraints: given as a dictionary, 'eq' type for equality and 'fun' for the anonymous function which limits the total summed asset allocation to 100% of the budget. The result from the minimize function is returned as a OptimizeResult type. 1 2 3 4 5 6 7 8 9 10 11 12 def neg_sharpe_ratio ( weights , average_returns , covariance_matrix , risk_free_rate ): returns , volatility = portfolio_performance ( weights , average_returns , covariance_matrix ) return - ( returns - risk_free_rate ) / volatility def max_sharpe_ratio ( average_returns , covariance_matrix , risk_free_rate ): num_assets = len ( average_returns ) args = ( average_returns , covariance_matrix , risk_free_rate ) constraints = ({ 'type' : 'eq' , 'fun' : lambda x : np . sum ( x ) - 1 }) bound = ( 0 , 1 ) bounds = tuple ( bound for asset in range ( num_assets )) result = sco . minimize ( neg_sharpe_ratio , num_assets * [ 1 / num_assets ,], args = args , method = 'SLSQP' , bounds = bounds , constraints = constraints ) return result Similarly to the maximum sharpe ratio we do the same for determining the minimum volatility portfolio programmatically. We minimise volatility by trying different weightings on our asset allocations to find the minima. 1 2 3 4 5 6 7 8 9 10 11 12 13 def portfolio_volatility ( weights , average_returns , covariance_matrix ): return portfolio_performance ( weights , average_returns , covariance_matrix )[ 1 ] def min_variance ( average_returns , covariance_matrix ): num_assets = len ( average_returns ) args = ( average_returns , covariance_matrix ) constraints = ({ 'type' : 'eq' , 'fun' : lambda x : np . sum ( x ) - 1 }) bound = ( 0.0 , 1.0 ) bounds = tuple ( bound for asset in range ( num_assets )) result = sco . minimize ( portfolio_volatility , num_assets * [ 1. / num_assets ,], args = args , method = 'SLSQP' , bounds = bounds , constraints = constraints ) return result As above, we can also draw a line which depicts the efficient frontier for the portfolios for a given risk rate. Below some functions are defined for computing the efficient frontier. The first function, efficient_return is calculating the most efficient portfolio for a given target return, and the second function efficient frontier is compiling the most efficient portfolio for a range of targets. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def efficient_return ( average_returns , covariance_matrix , target ): num_assets = len ( average_returns ) args = ( average_returns , covariance_matrix ) def portfolio_return ( weights ): return portfolio_performance ( weights , average_returns , covariance_matrix )[ 0 ] constraints = ({ 'type' : 'eq' , 'fun' : lambda x : portfolio_return ( x ) - target }, { 'type' : 'eq' , 'fun' : lambda x : np . sum ( x ) - 1 }) bounds = tuple (( 0 , 1 ) for asset in range ( num_assets )) result = sco . minimize ( portfolio_volatility , num_assets * [ 1. / num_assets ,], args = args , method = 'SLSQP' , bounds = bounds , constraints = constraints ) return result def efficient_frontier ( average_returns , covariance_matrix , returns_range ): efficients = [] for ret in returns_range : efficients . append ( efficient_return ( average_returns , covariance_matrix , ret )) return efficients Now it's time to plot the efficient frontier on the graph with the randomly selected portfolios to check if they have been calculated correctly. It is also an opportune time to check if the maximum Sharpe ratio and minimum volatility portfolios have been calculated correctly by comparing them to the previously randomly determined portfolios. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def display_efficient_frontier ( average_returns , covariance_matrix , num_portfolios , risk_free_rate ): results , weights = generate_portfolios ( num_portfolios , average_returns , covariance_matrix , risk_free_rate ) max_sharpe = max_sharpe_ratio ( average_returns , covariance_matrix , risk_free_rate ) max_sharpe_return , max_sharpe_volatility = portfolio_performance ( max_sharpe [ 'x' ], average_returns , covariance_matrix ) max_sharpe_allocations = allocations_ef ( max_sharpe . x , stocks_df ) . T print ( \"MAX SHARPE RATIO \\n \" ) print ( \"Return: {0:.2f} \" . format ( max_sharpe_return )) print ( \"Volatility: {0:.2f} \" . format ( max_sharpe_volatility )) print ( max_sharpe_allocations ) min_vol = min_variance ( average_returns , covariance_matrix ) min_vol_return , min_vol_volatility = portfolio_performance ( min_vol [ 'x' ], average_returns , covariance_matrix ) min_vol_allocations = allocations_ef ( min_vol . x , stocks_df ) . T print ( \" \\n MINIMUM VOLATILITY \\n \" ) print ( \"Return: {0:.2f} \" . format ( min_vol_return )) print ( \"Volatility: {0:.2f} \" . format ( min_vol_volatility )) print ( min_vol_allocations ) an_vol = np . std ( returns ) * np . sqrt ( 253 ) an_rt = average_returns * 253 for i , txt in enumerate ( stocks_df . columns ): print ( txt , \":\" , \"Annuaised return\" , round ( an_rt [ i ], 2 ), \", Annualised volatility:\" , round ( an_vol [ i ], 2 )) plt . figure ( figsize = ( 10 , 7 )) plt . scatter ( results [ 0 ,:], results [ 1 ,:], c = results [ 2 ,:], cmap = 'YlGnBu' , marker = 'o' , s = 10 , alpha = 0.3 ) plt . colorbar () plt . scatter ( max_sharpe_volatility , max_sharpe_return , marker = 'X' , color = 'r' , s = 400 , label = 'Maximum Sharpe ratio' ) plt . scatter ( min_vol_volatility , min_vol_return , marker = 'X' , color = 'y' , s = 400 , label = 'Minimum volatility' ) target = np . linspace ( min_vol_return , max ( an_rt ), 50 ) efficient_portfolios = efficient_frontier ( average_returns , covariance_matrix , target ) plt . plot ([ p [ 'fun' ] for p in efficient_portfolios ], target , linestyle = '-.' , color = 'white' , label = 'efficient frontier' ) plt . title ( 'Calculated Portfolio Optimization based on Efficient Frontier' ) plt . xlabel ( 'Volatility' ) plt . ylabel ( 'Returns' ) plt . legend ( labelspacing = 0.8 ) def allocations_ef ( solution , stocks_df ): allocation = pd . DataFrame ( solution , index = stocks_df . columns , columns = [ 'allocation' ]) return allocation returns = stocks_df . pct_change () average_returns = returns . mean () covariance_matrix = returns . cov () num_portfolios = 25000 risk_free_rate = 0.01977 display_efficient_frontier ( average_returns , covariance_matrix , num_portfolios , risk_free_rate ) The surprising part is that the calculated result is very close to what we have previously simulated by picking from randomly generated portfolios. The slight differences in allocations between the simulated vs calculated are in most cases less than 1%, which shows how powerful randomly estimating calculations can be albeit sometimes not reliable in small sample spaces. Rather than plotting every randomly generated portfolio, we can plot the individual stocks on the plot with the corresponding values of each stock's return and risk. This way we can compare how diversification is lowering the risk by optimizing the allocations. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def display_efficient_frontier_selected ( average_returns , covariance_matrix , risk_free_rate ): max_sharpe = max_sharpe_ratio ( average_returns , covariance_matrix , risk_free_rate ) max_sharpe_return , max_sharpe_volatility = portfolio_performance ( max_sharpe [ 'x' ], average_returns , covariance_matrix ) max_sharpe_allocations = allocations_ef ( max_sharpe . x , stocks_df ) . T print ( \"MAX SHARPE RATIO \\n \" ) print ( \"Return: {0:.2f} \" . format ( max_sharpe_return )) print ( \"Volatility: {0:.2f} \" . format ( max_sharpe_volatility )) print ( max_sharpe_allocations ) min_vol = min_variance ( average_returns , covariance_matrix ) min_vol_return , min_vol_volatility = portfolio_performance ( min_vol [ 'x' ], average_returns , covariance_matrix ) min_vol_allocations = allocations_ef ( min_vol . x , stocks_df ) . T print ( \" \\n MINIMUM VOLATILITY \\n \" ) print ( \"Return: {0:.2f} \" . format ( min_vol_return )) print ( \"Volatility: {0:.2f} \" . format ( min_vol_volatility )) print ( min_vol_allocations ) an_vol = np . std ( returns ) * np . sqrt ( 253 ) an_rt = average_returns * 253 for i , txt in enumerate ( stocks_df . columns ): print ( txt , \":\" , \"Annuaised return\" , round ( an_rt [ i ], 2 ), \", Annualised volatility:\" , round ( an_vol [ i ], 2 )) plt . figure ( figsize = ( 10 , 7 )) plt . scatter ( an_vol , an_rt , marker = 'o' , s = 200 ) for i , txt in enumerate ( stocks_df . columns ): plt . annotate ( txt , ( an_vol [ i ], an_rt [ i ]), xytext = ( 10 , 0 ), textcoords = 'offset points' ) plt . scatter ( max_sharpe_volatility , max_sharpe_return , marker = 'X' , color = 'r' , s = 400 , label = 'Maximum Sharpe ratio' ) plt . scatter ( min_vol_volatility , min_vol_return , marker = 'X' , color = 'y' , s = 400 , label = 'Minimum volatility' ) target = np . linspace ( min_vol_return , max ( an_rt ), 50 ) efficient_portfolios = efficient_frontier ( average_returns , covariance_matrix , target ) plt . plot ([ p [ 'fun' ] for p in efficient_portfolios ], target , linestyle = '-.' , color = 'white' , label = 'efficient frontier' ) plt . title ( 'Calculated Portfolio Optimization based on Efficient Frontier' ) plt . xlabel ( 'Volatility' ) plt . ylabel ( 'Returns' ) plt . legend ( labelspacing = 0.8 ) display_efficient_frontier_selected ( average_returns , covariance_matrix , risk_free_rate ) From the plot above, the stock with the highest risk is BHP, which accompanies the highest returns. This shows that if the investor is willing to take the risk than they will be rewarded with the higher return. This concludes the 3 part series on Python in the finance industry, if there is any topics in particular you would like to see how software can integrate and improve a service/product please feel free to get in touch!","tags":"Python","url":"https://jackmckew.dev/efficient-frontier-for-balancing-portfolios.html","loc":"https://jackmckew.dev/efficient-frontier-for-balancing-portfolios.html"},{"title":"Portfolio Balancing with Historical Stock Data","text":"Following last weeks' post ( Python for the Finance Industry ). This post is to demonstrate a method of determining an optimized portfolio based on historical stock price data. First of all while attempting to tackle this problem, I stumbled across many very informative articles in which based on what I learned throughout reading them, and trying to replicate their findings with the ASX stocks' data. Ricky Kim ( Efficient Frontier Portfolio Optimisation in Python) Bernard Brenyah ( Markowitz's Efficient Frontier in Python) Now I will not be going into how Markowit'z Efficient Frontier Portfolio Optimization & Sharpe Ratios works as these techniques are extremely well documented across this internet and very easily found. This post will be for implementing these techniques in Python to apply them to an ASX based portfolio. Picking up from the end of the previous post, we had just plotted the percentage change over the time period for our stocks' data. For the sake of this post we will be using a technique called random optimization, where will be taking a number of random attempts and selecting the best one. Further posts will show a more detailed approach to this optimization problem. Now there are multiple steps before we get to the desired outcome of a balanced portfolio. Generate X number of 'random' portfolios, Rate their performance against one another, Pick the desired solution. To generate random portfolios, we define a function such that we can pass it differing variables as to tweak our outcomes in the future. 1 2 3 4 5 6 7 8 9 10 11 12 def generate_portfolios ( num_portfolios , average_returns , covariance_matrix , risk_free_rate ): results = np . zeros (( 3 , num_portfolios )) weights_record = [] for portfolio in range ( num_portfolios ): weights = np . random . random ( len ( companies ) - 1 ) weights /= np . sum ( weights ) weights_record . append ( weights ) returns , volatility = portfolio_performance ( weights , average_returns , covariance_matrix ) results [ 0 , portfolio ] = volatility results [ 1 , portfolio ] = returns results [ 2 , portfolio ] = ( returns - risk_free_rate ) / volatility return results , weights_record To step through this function: Define empty location for our portfolio performance results to be stored along with recording weights so we can extract them once selected, For each portfolio to be generated, give a random 'weighting' for each of the company that we have historical data on (eg, 23% NAB.AX), Even out the distribution of the weights such that the sum of the weightings is 100% (eg, total budget), Record the weightings generated in our memory location, Determine the performance of our randomly generated portfolio (more on that soon), Fill in the portfolio performance results for this generated portfolio and repeat for X number of portfolios. In step 5 above, we have to determine how to rank the generated portfolios against each other to work out how to filter our results. To do this, we calculate volatility of the portfolio using the following formula: Bernard Brenyah , whom I mentioned at the beginning of the post, has provided a clear explanation of how the above formula can be expressed in matrix calculation in one of his blog post s. In which we just take the matrix calculation and multiply by 253 for number of trading days in Australia. 1 2 3 4 5 def portfolio_performance ( weights , average_returns , covariance_matrix ): returns = np . sum ( weights * average_returns ) * 253 variance = np . dot ( weights . T , np . dot ( covariance_matrix , weights )) volatility = np . sqrt ( variance ) * np . sqrt ( 253 ) return returns , volatility Now that we have X number of randomly generated portfolios, all ranked against one another, it's time to plot so that our results can be visualized. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 def display_random_efficient_frontier ( average_returns , covariance_matrix , num_portfolios , risk_free_rate ): results , weights = generate_portfolios ( num_portfolios , average_returns , covariance_matrix , risk_free_rate ) max_sharpe_index = np . argmax ( results [ 2 ]) max_volatility = results [ 0 , max_sharpe_index ] max_return = results [ 1 , max_sharpe_index ] max_sharpe_allocations = allocations ( max_sharpe_index , weights , stocks_df ) . T print ( \"MAX SHARPE RATIO \\n \" ) print ( \"Return: {0:.2f} \" . format ( max_return )) print ( \"Volatility: {0:.2f} \" . format ( max_volatility )) print ( max_sharpe_allocations ) min_vol_index = np . argmin ( results [ 0 ]) min_volatility = results [ 0 , min_vol_index ] min_return = results [ 1 , min_vol_index ] min_vol_allocations = allocations ( min_vol_index , weights , stocks_df ) . T print ( \" \\n MINIMUM VOLATILITY \\n \" ) print ( \"Return: {0:.2f} \" . format ( min_return )) print ( \"Volatility: {0:.2f} \" . format ( min_volatility )) print ( min_vol_allocations ) plt . figure ( figsize = ( 10 , 7 )) plt . scatter ( results [ 0 ,:], results [ 1 ,:], c = results [ 2 ,:], cmap = 'YlGnBu' , marker = 'o' , s = 10 , alpha = 0.3 ) plt . colorbar () plt . scatter ( max_volatility , max_return , marker = 'X' , color = 'r' , s = 400 , label = 'Maximum Sharpe ratio' ) plt . scatter ( min_volatility , min_return , marker = 'X' , color = 'y' , s = 400 , label = 'Minimum volatility' ) plt . title ( 'Simulated Portfolio Optimization based on Efficient Frontier' ) plt . xlabel ( 'Volatility' ) plt . ylabel ( 'Returns' ) plt . legend ( labelspacing = 0.8 ) def allocations ( index , weights , stocks_df ): allocation = pd . DataFrame ( weights [ index ], index = stocks_df . columns , columns = [ 'allocation' ]) return allocation Using the above function 'display_random_efficient_frontier', this will determine our max sharpe ratio portfolio generated and the minimum volatility portfolio with their respective returns. Now it is entirely up to the trader on how much risk they are willing to take on board with their portfolio. With the settings below in conjunction with the previously defined functions and stock data to generate the portfolios (risk free rate determined from this website ). 1 2 3 4 5 6 7 returns = stocks_df . pct_change () mean_returns = returns . mean () cov_matrix = returns . cov () num_portfolios = 25000 risk_free_rate = 0.01977 display_random_efficient_frontier ( mean_returns , cov_matrix , num_portfolios , risk_free_rate ) With the two portfolios determined, the one gives us the best risk-adjusted (as long as the trader is prepared to take the risk) is the one with the maximum Sharpe ratio, allocating a 67% portion to WOW and 32% to BHP, as these stocks were quite volatile from the daily percentage change calculations. On the other hand, the minimum volatility portfolio is reflecting the more stable of the stocks from the daily percentage change calculations distributing portions over NAB and TLS due to their stability from the percentage change calculations and reducing the portion to WOW.","tags":"Python","url":"https://jackmckew.dev/portfolio-balancing-with-historical-stock-data.html","loc":"https://jackmckew.dev/portfolio-balancing-with-historical-stock-data.html"},{"title":"Python for the Finance Industry","text":"This is the first post in a series of posts dedicated for demonstrating how Python can be applied in the finance industry. Personally, the first thing that comes to mind when I think of the finance industry is the stock market. For fellow Australians, our main stock exchange is the Australian Securities Exchange (ASX). For those who are reading who are not familiar with stocks, there is a plethora of information around stocks across the internet. When it comes to using Python with stocks, the very first thing that you will require, is data. Thankfully, there are multitudes of services out there which provide this data through application programming interfaces (APIs). The data is provided through APIs in a few common formats: JSON, XML, CSV. For this post, I will be utilising the free service, Alpha Vantage, to request historical records of stock information on the ASX. For access to Alpha Vantage's API, head to http://www.alphavantage.co/support/#api-key and register for a free API key. There is also documentation around testing if your API key is operational on the Alpha Vantage website. Now that we have access to an API in which we can extract historical records of stock information in the ASX, it's time to manipulate and analyse the data. As in my previous post Episode 8 – Anaconda , I recommend setting up a virtual environment or anaconda environment to install & manage dependencies of external libraries. The packages required for this post in the series are: Pandas (For manipulating the data), Alpha_vantage (To access the historical records through an API), NumPy (For processing across the data), Matplotlib (For visualising and generate plots of the data). To import these libraries into our Python code the following\\ code is required: 1 2 3 4 import pandas as pd from alpha_vantage.timeseries import TimeSeries import matplotlib.pyplot as plt import numpy as np Now that we have imported the packages required to extract,\\ process and display the data. The first step is to extract the data in a useful\\ format from the Alpha Vantage API. First declare a list with all the companies ASX names with the suffix \".AX\" to denominate that it's from the ASX. After that initialise an empty pandas dataframe to be filled with the data to analyse. Now iterate over the list, calling a request through the API to request the data that is required. There are multiple formats of data to be extracted through the API which is detailed in the Alpha_Vantage documentation . For this post, I have used the get_daily function from the timeseries object in alpha_vantage to extract the daily information on a stock for the past 20 years, in particular, the closing value. 1 2 3 4 5 6 7 8 9 10 companies = [ 'NAB.AX' , 'WOW.AX' , 'TLS.AX' , 'BHP.AX' ] stocks_df = pd . DataFrame () for company in companies : data , meta_data = ts . get_daily ( symbol = company , outputsize = 'full' ) print ( data . head ()) stocks_df [ company ] = pd . Series ( data [ '4. close' ]) print ( stocks_df . head ()) Now that the dataframe is full of closing values for the companie's stock's closing values, it's time to begin processing. First of all, for any missing data or erroneous 0 values, the ffill() function is used to fill any missing value by propagating the last valid observation forward. After that, the timestamp on each row is forced to become the index of the dataframe and converted to a datetime type. 1 2 3 stocks_df = stocks_df . replace ( 0 , pd . np . nan ) . ffill () stocks_df . index = pd . to_datetime ( stocks_df [ \"date\" ]) stocks_df = stocks_df . drop ( \"date\" , axis = 1 ) Now that the data has gone through it's pre-processing phase, it's time to begin plotting some figures. To begin, a basic figure, plotting a single for each company's stock price over the past 20 years on a single line graph to enable comparison between the companies. 1 2 3 4 5 6 plt . figure ( figsize = ( 14 , 7 )) for column in stocks_df . columns . values : plt . plot ( stocks_df . index , stocks_df [ column ], lw = 3 , alpha = 0.8 , label = c ) plt . legend ( loc = 'upper left' , fontsize = 12 ) plt . ylabel ( 'price in $' ) plt . show () Another way to plot this data is to show it as the percentage change from the day before AKA daily returns. By plotting the data in this way, instead of showing the actual prices, the graph is showing the stocks' volatility. 1 2 3 4 5 6 7 8 returns = stocks_df . pct_change () plt . figure ( figsize = ( 14 , 7 )) for column in returns . columns . values : plt . plot ( returns . index , returns [ column ], lw = 3 , alpha = 0.8 , label = c ) plt . legend ( loc = 'upper left' , fontsize = 12 ) plt . ylabel ( 'daily returns' ) plt . show () Now that we have some insight to the stocks' data, the next post in this series will demonstrate a way to calculate a balanced portfolio from historical records using Modern Portfolio Theory.","tags":"Python","url":"https://jackmckew.dev/python-for-the-finance-industry.html","loc":"https://jackmckew.dev/python-for-the-finance-industry.html"},{"title":"How to Program an ESP8266 with MicroPython","text":"Following the previous two weeks of topics, Introduction to ESP32/ESP8266 and What is MicroPython? . I wrote an article on maker.pro in which I describe how to program the ESP8266 with MicroPython in detail.","tags":"Engineering","url":"https://jackmckew.dev/how-to-program-an-esp8266-with-micropython.html","loc":"https://jackmckew.dev/how-to-program-an-esp8266-with-micropython.html"},{"title":"What is MicroPython?","text":"From the MicroPython docs themselves \"MicroPython is a lean and efficient implementation of the Python 3 programming language that includes a small subset of the Python standard library and is optimised to run on microcontrollers and in constrained environments.\". But what does all this mean? Python 3 is one of the most widely used, easy to write/read programming languages in the world that is rapidly growing. By default Python comes with a ‘standard library' which includes basic functions such as if statements, loops, printing, etc. Where MicroPython comes in is that the standard library for Python might take up valuable space/computations to run as efficiently it does on a PC, so MicroPython is a slice of the standard library that is able to run more efficiently and take up less space on a microcontroller (RAM and space is crucial when working with microcontrollers). MicroPython also comes with an interactive REPL (Read-Evaluate-Print Loop), which is an often overlooked amazing feature of MicroPython. The REPL allows you to connect to a microcontroller, execute code quickly without the need to compile or upload code. Which gives immediate feedback on whether your program is working as intended. Differences between MicroPython & Python There obviously had to be some changes between Python and MicroPython to make it work efficiently on processors a fraction of the power, but what are they? If you are a beginner-intermediate Python programmer, you'll only run into trouble in very specific scenarios, which can be easily worked around. For example you cannot delete from a list with a step greater than 1. Sample Python Code 1 2 3 L = [ 1 , 2 , 3 , 4 ] del ( L [ 0 : 4 : 2 ]) print ( L ) You'd expect for the output here in Python normally to be: Python Output MicroPython Output [2,4] TypeError: object 'range' isn't a tuple or list However this can be easily worked around with an explicit loop for example: Sample MicroPython/Python Code 1 2 3 4 L = [ 1 , 2 , 3 , 4 ] for i in L : if ( i % 2 == 0 ): del ( L [ i ]) For more information on differences between Python (in particular CPython) and MicroPython you can find the MicroPython documentation here: http://docs.micropython.org/en/latest/genrst/index.html","tags":"Python","url":"https://jackmckew.dev/what-is-micropython.html","loc":"https://jackmckew.dev/what-is-micropython.html"},{"title":"Introduction to ESP32/ESP8266","text":"What is an ESP32/ESP8266 The ESP32 and ESP8266 are low-cost Wi-Fi modules, which are perfect for DIY Internet of Things (IoT) projects. They both come with general purpose input/output pins (GPIOs), support a variety of protocols such as SPI, I2C, UART and many more. The most attractive part of the ESP range is that they come with wireless networking included, separating them from their Arduino microcontroller counterparts. All in all, the ESP series allows you to easily control/monitor devices remotely using Wi-Fi for a very low price. ESP32 vs ESP8266 The ESP32 is the later ‘model' of the ESP8266. It added a whole suite of new functionality such as: touch sensitive pins, built-in temperature and hall effect sensors and upgraded from single core CPU to a dual core, faster Wi-FI, more GPIOs and now supports Bluetooth and BLE (Bluetooth Low Energy). While both boards are very low-cost, the ESP32 costs slightly more, the ESP8266 (here in Australia) costs around \\~\\ \\(10AU, and the ESP32 around \\~\\\\) 22AU. Flavours of ESP boards There are currently many different varieties of ESP flavours you can buy off the shelf, while if you are more into developing the board around your ESP module (the pictures above) you can simply just purchase the relevant ESP module, or if you are like me and don't want to bother soldering and developing your own board there is a solutions for you!\\ ESP32 Development Boards ESP32 Thing - Sparkfun The ESP32 Thing comes with all the functionalities to easily communication and program the ESP32 with your computer (including a on-board USB-Serial). It also features a LiPo charger, so your ESP32 project can use rechargeable batteries without having to solder any terminals and make it easy to replace/disconnect the battery pack. Espressif ESP32 Development Board - Developer Edition If you're not confident on soldering the header pins on the Sparkfun Thing board, then the Espressif board comes with that done for you! The header pins are also nicely spaced out so if you are a breadboard enthusiast, you can just plug and play on your breadboard and start connecting all your header wires. ESP8266 Development Boards NodeMCU The NodeMCU is my personal favourite ESP flavour board because it is friendly to your breadboard, has an on-board USB-Serial and can be powered by USB. This all means that you can test and develop your board straight out of the box without fiddling around with soldering pins, voltages or getting any extra components (except a Micro-usb cable). Adafruit Huzzah ESP8266 Breakout The Huzzah board is Adafruits answer to other development boards that weren't friendly to breadboards, didn't have on-board voltage regulators and weren't CE or FCC emitter certified. The Huzzah board comes with all these functionalities, although unlike the NodeMCU you will need to get a USB-Serial cable to able to program your Huzzah board.","tags":"Engineering","url":"https://jackmckew.dev/introduction-to-esp32-esp8266.html","loc":"https://jackmckew.dev/introduction-to-esp32-esp8266.html"},{"title":"Episode 17 – Networking Routing & Addressing","text":"Following last weeks post around network topologies, I believe the next topic to cover is routing and addressing. Routing is the process of selecting a path for traffic to flow through in a network while addressing is marking elements within a network. A real-world example of routing and addressing is the postal system, each element (person) is marked with an address (eg, a street address) and the mail makes it to that address from routing it from the sender to the receiver. While the goal for routing may be simple (\"go from sender to receiver in the most efficient/quickest way possible\"), the techniques used to achieve this can be very complex and confusing but when solutions are found that make a network work efficiently, it is a very rewarding experience for all users. Routing can be broken into three broad categories: Protocols – the medium that allows information to move through a network Algorithms – to determine paths between sender and receiver Databases – to store information that the algorithms determine The whole premise around routers in a network ( Networking Basics ) is that they will \"pass it on\", either to their smarter peers or in the correct direction. For example in a star/tree network, devices pass information to their closest ‘router' which then decides either to pass it directly to the correct address or to another router which may have a better idea on where the information is intended on going. Protocols In industry, some of the most common networking protocols are MODBUS and DNP3. Modbus being a de-facto standard for interconnecting electrical equipment and DNP3 (Distributed Network Protocol) commonly being used in the water/electric industries for their flexibility during outages or broken links in a network. Algorithms Routing tables is the most prevalent type of routing algorithms with their fixed nature meaning once the routing decisions for how information travels have been decided, they do not change. The other type of routing algorithms (which are much more exciting) are known as adaptive algorithms, which means the routing changes depending on: topology, delay, load, etc, to try and reach the most efficient path from sender to receiver. Databases Following algorithms, databases can either hold the entire routing table and a router looks up where it wants to go and it which path to take (similar to a bus timetable), or, forwarding tables (technically can be apart of routing tables as well) which detail the communications pathways to utilize for types of traffic.","tags":"Engineering","url":"https://jackmckew.dev/episode-17-networking-routing-addressing.html","loc":"https://jackmckew.dev/episode-17-networking-routing-addressing.html"},{"title":"Episode 16 - Networking Basics","text":"A network is defined as \"A network is a collection of computers, servers, mainframes, network devices, peripherals, or other devices connected to one another to allow the sharing of data\". There are various configurations of networks for specific design scenarios as represented in: Typical residential home networks are configured in a tree topology that is connected to the internet. This typically consists of a single router/modem that serves all the end-user devices on the network with internet connection. The router also acts as a gateway for the devices connected on the network to communicate with one another. Packets of data that are generated by the devices are encapsulated with destination routing information; which is passed to the router at the center which directs the data to it's destination in the network. For example, a user connects to the router to gain access to a wider network that is the internet to load this webpage. If there was a higher risk on losing the communications medium between two devices (cable failure), then bus would be at disadvantage here but ring might prove more beneficial although transmission would be slower as the network connection would be further away (go around the ring in the case of the picture above). Mesh Topology By explaining network topologies by comparing to a basic Wi-Fi network normally gets the message across. A mesh network can prove beneficial to areas in which a star network isn't covering the area you wish it to, for example, if you have 'dark' spots where you don't receive Wi-Fi signal, a mesh network might be better suited. A practical implementation of a mesh network can be seen in shopping centre's Wi-Fi networks where multiple routers are placed strategically such that you can walk around the entire property and not lose signal. Ring Topology While star is a very popular configuration of network, it however is not the most ideal configuration for some types of networks. For example, if you had a series of devices that all needed to talk to each other, even if one was to fail, then a ring/bus/mesh would be more applicable where there is always a path to everyone else if a device was to fall offline. Star Topology If you consider a home Wi-Fi network that doesn't connect out further (no internet connection) then you have a basic star network. The center of the star in this scenario would be the Wi-Fi router, you can still connect to the other devices but not outside of your network and all messages have to travel through the router. Tree Topology A tree topology is just creating multiple star networks off the back of another network. For example, if you considered the network that is your internet connection from the street (or satellite), then connecting to your modem (gateway) then furthering to your devices in your home, you have a basic tree network. Bus Topology Where devices are connected to a single medium (cable) to communicate with each other, you now have a bus network. A bus network proves it's advantages by less cabling than star networks, ease of installation of linear networks and works well with small networks. It is easy enough to add new devices to the network and if one fails (but the medium doesn't) then all devices can still communicate. Disadvantages arise when problems occur as it difficult to determine the cause of an issue on a bus network.","tags":"Engineering","url":"https://jackmckew.dev/episode-16-networking-basics.html","loc":"https://jackmckew.dev/episode-16-networking-basics.html"},{"title":"Episode 15 - What is a C.T?","text":"A C.T is the abbreviated form for a current transformer in electrical terms. It is a simple but effective use of magnetic circuits and transformer characteristics to monitor how power is behaving in a conductor. The C.T works by wrapping a coil of conductor around a core (typically silicon steel) shaped like a ring, the 'power' wire or the active is then passed through the core. When alternating current passes through the active conductor, this then generates a magnetic field around the wire, inducing a current proportional to the number of turns the wire is wrapped around the core. The C.T is a very widely used piece of equipment in instrument electrical/electronics as it allows for an indirect way of monitoring for the power flow unlike a 'flow' meter that must be a part of the pipework to directly measure flow. While it is still possible to monitor current in a 'flow' meter type fashion, it is far less risk to implement a C.T solution. Possibly the frequent implementation of C.Ts in everyday life is within power meter solutions. By attaching a C.T to the active conductor that is powering a piece of equipment, we are then able to measure the power the equipment is using in real time. This can then be further digitised and utilised in a network fashion to provide asset owners with real time energy usage of their equipment. It must be noted when using C.Ts on alternating current systems that the C.T must only have a single conductor pass through it (the active); if both the active and the neutral are passed through the C.T then (in an ideal world) the C.T will have an output of 0. If you also pass through the earth wire to the equipment, it is possible to measure earth leakage or earth fault current (provided the measurement can handle it).","tags":"Engineering","url":"https://jackmckew.dev/episode-15-what-is-a-c-t.html","loc":"https://jackmckew.dev/episode-15-what-is-a-c-t.html"},{"title":"Episode 14 - Types of Machine Learning","text":"With AI and Machine Learning becoming the buzzwords in technology for 2018 and the real world applications now maturing to show the benefits of this technology. It can be very confusing when first entering the world of AI and machine learning with new techniques coming out every other day in search of improving the technology. Hopefully this article will help break down the barriers of the jargon and explain the types of machine learning algorithms out in the wild simplistically. In general, there are 3 different broad categories that current machine learning algorithms fit into: Supervised learning Unsupervised learning Reinforcement learning Supervised Learning Most practical machine learning algorithms use supervised learning. Supervised learning is where you have one or more input variables (x) and output variable(s) (y), and you use an algorithm to learn the mapping function from the input to the output. 1 y = f(x) The end goal of this algorithm is to approximate the mapping function accurately such that then you have a new data input (x), you can predict what the result (y) for that data would be. The name supervised learning comes from the algorithm first learning from a training data set before we present the algorithm to a new data set. The training data set can be thought as the teacher who is supervising the learning process, and learning only stops when the algorithm reaches an acceptable level of performance on predicting the result. Unsupervised Learning Unsupervised learning is when you only have the input variable(s) (x) and no respective output (y). The end goal for unsupervised learning is to model the distribution or structure of the data in order to discover and learn about the data set. Unsupervised learning in contrast to supervised learning is where the omnipresent teacher in supervised learning is gone and there is no correct answers. The algorithm is left alone to discover and present the distribution/structure in the data that it determines. Reinforcement Learning Reinforcement learning is the third broad category that a machine learning algorithm can fall into, where the algorithm has the input variable(s) (x) and through interacting with the input data set receives rewards for performing favoured actions. Learning from interactions with the environment around us comes from our natural experiences in the world. For example, imagine you're a child in a room with a fire. You move closer to the fire and feel it's warmth and it makes you feel good, this is a positive reward; then you try a touch the fire and it burns you hand, this is a negative reward. Reinforcement learning is just a computational approach to learning from interactions to achieve the most favourable result, in our example, we learnt that being close to the fire is a positive thing but too close is a negative thing so our result is to maintain a sufficient distance away to be warm but no burnt.","tags":"Machine Learning","url":"https://jackmckew.dev/episode-13-types-of-machine-learning.html","loc":"https://jackmckew.dev/episode-13-types-of-machine-learning.html"},{"title":"Episode 13 - Lighting Design","text":"Before I started in a more buildings-focused electrical engineering position, I didn't think that much went into selecting lights for buildings. Once you first get started in lighting design, it is like opening a can of worms, there is so much detail that goes into lighting design, it's unfathomable. First of all, lighting design in Australia is dictated by AS1158. Not only does the Australian standard explicitly state illuminance requirements for rooms based on their task (eg 320 lux for office based tasks), it also clearly defines how to calculate these levels based on the environment. What really is lux? Lux is the SI derived unit of illuminance and luminous emittance, measuring luminous flux per unit area. It is equal to one lumen per square meter. Luminous flux? Lumens? What do all these terms mean? A lumen is the SI derived unit of luminous flux, which is a measure of the total amount of visible light emitted by a source. Now when it comes to designing lighting for a building/area, multiple large considerations must be taken. Once you have determined what tasks will be completed within the area you are designing, you must then go to AS1158 and determine the required lux requirements. Following this, you must ensure you have accurate dimensions of the area you are designing for, along with all reflectance (colour) of surfaces within the area. Once you have got all these parameters, it is time to begin modelling the area within any lighting design software package (eg AGI-32). Now the designer must select lights (luminaires) to be specified for the area. The designer must take into consideration the ceiling (if there is one) type, this will dictate how the luminaires are to be mounted, be it: surface, recessed, suspended or pole mounted. Most luminaire fitting manufacturers will provide photometric files (IES files) detailing how their respective lights would behave if they were installed in the design area. Once the designer has verified that the specified luminaires will meet required lux levels in the area, this design must be passed to the electrical designer as they must factor in how much power all the luminaires will require to operate and how the cable routes must be laid out to suit the luminaires on the site. Please note, that this is a very simplistic view at lighting design. Just like any area of work, there is an art to doing a proper job.","tags":"Engineering","url":"https://jackmckew.dev/episode-13-lighting-design.html","loc":"https://jackmckew.dev/episode-13-lighting-design.html"},{"title":"Episode 12 - What is Git?","text":"One of the biggest issues when working on any project regardless of what industry, discipline or context, as soon as a new 'version' of design or update comes along, the issue of version control appears. When this change(s) come along in the life cycle of a project, it is within everyone involved's best interests to maintain some type of version control, to manage and track changes between versions. When it comes to software development version control, the most well-known and commonly used system is Git. Git is a actively maintained open source project originally developed by Linus Torvalds (creator of the Linux OS kernel) in 2005. Multitudes of software projects depend on git for version control, including both commercial and open source. Git has a distributed architecture, meaning rather than having one single location for the full version history of the project, every developer's working copy of the project is also a location that can contain the full history. This also comes with the benefit that if one developer happens to lose the entire project, it can be restored from other developers (pending they have/are working on the latest version). Flexibility Git is flexible in several respects: efficiency at maintaining version control for both small and large projects, capable at handling numerous types of non-linear development workflows and compatibility with existing protocols and systems. Git supports branching and tagging (unlike other commonly used version control systems), and operations that affect branches and tags such as reverting or merging are stored as part of the change history. This level of tracking is not available in many other version control systems. Security The integrity of the source code of the project was identified as a top priority when Git was design. Cryptographically secure hashing algorithms are used to secure: file contents, file and directory relationships, versions, tags and commits. This defends the project's source files and change history against both malicious and accidental changes and ensures that all modifications is fully traceable. Use-case Bob wants to implement a new feature to the project he is working on in anticipation of the upcoming 3.0 release, and commits the new feature with descriptive messages. After being inspired by the new feature, he works on a second new feature and commits those modifications as well. Naturally, these two new features are logged as separate entities within the change history of the project. Bob then returns back to version 2.6 of the same project to fix a problem that only affects version 2.6. This allow Bob (and his team) to distribute that fix release (version 2.6.1), before version 3.0 is ready. Bob can then return to the upcoming version 3.0 'branch' to continue implementing new features. Since Bob has his own copy of the entire project, all of these changes can occur without an network access, thus being reliable and fast. To submit all these committed features and fixes to the remote copy of the project, it simply done by the use of the 'push' command.","tags":"Software Development","url":"https://jackmckew.dev/episode-12-what-is-git.html","loc":"https://jackmckew.dev/episode-12-what-is-git.html"},{"title":"Episode 11 - Power Quality Explained","text":"I've always lived by the rule that if you can't explain something to a 5 year old then you don't know it well enough. I was asked recently by some (non-electrical focused) colleagues on a handful of electrical terms and components. One of the biggest things that kept popping up that I found difficult to explain clearly was power quality and it's issues. So I decided why not dedicate a blog post about it and write a basic example power factor capacitor calculator in Python. Power quality is defined as \"the concept of powering and grounding sensitive electronic equipment in a manner suitable for the equipment with precise wiring system and other connected equipment\" by the IEEE (The Institute of Electrical and Electronics Engineers). In a simplistic view this is just trying to say that electrical equipment is to be installed/configured in a way that is operates as intended. Quality of power is not determined by the one who produces it, it's defined by the end user of the power. Eg, like a physical product, if you buy something from a store and it's poor quality, that's being defined by the end user. Similar to that of a physical product, quality of power can be lost in a variety of forms/ways. Issues with power quality can be categorized into three main categories: Harmonic voltages and currents Poor power factor Voltage instability Harmonics AC (Alternating Current) electricity is generated as a sinusoidal waveform, and harmonics are signals/waves whose frequency is a whole number multiple of the frequency of the reference signal/wave. To visualize this phenomenon, we can use packages like NumPy and Matplotlib, to calculate and plot our base signal and it's harmonics (I encourage you to run this code and change the harmonics to see what they look like). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 import numpy as np import matplotlib.pyplot as plt def Harmonic ( i ): x = np . linspace ( 0 , 2 * np . pi , 2000 ) y = [ 0 for _ in x ] for n in range ( 0 , i ): y += np . sin (( 2 * n + 1 ) * ( 2 * np . pi ) * ( x )) / ( 2 * n + 1 ) plt . plot ( x , y ) plt . grid () Harmonic ( 1 ) Harmonic ( 2 ) # Harmonic(3) # Harmonic(4) # Harmonic(5) plt . show () The example above shows us the base signal (fundamental frequency), and it's first harmonic (harmonic of 2 or twice as fast as the fundamental frequency). When these two signals are superimposed on each other, they produce a distorted waveform. Electrical equipment is designed to operate at the base signal (50Hz here in Australia), and typically does not cope with distorted wave like seen below when we superimpose a base signal with it's first harmonic. Luckily, these issues are now easily detected and rectified by harmonic analyzers and active/passive harmonic filters. Power Factor Power factor is a measure of how effectively power is being used in an electrical system, and is defined as the ratio of real (useful) to apparent (total) power. Real power (kW) is the power that actually powers the equipment to produce useful work (such as spinning a motor). It can also be called actual, active or working power. Reactive power (kVAR) is the power required by (some) equipment (eg, motors), to produce a magnetic field to enable the useful work to be produce. It's necessary to operate the equipment, however you don't see any result from the reactive power. Apparent power (kVA) is the vector sum of the real power (kW) and reactive power (kVAR) and is the total power supplied from the mains power required to produce the right amount of real power. Suppose you are running a store, you have to spend an amount of money X (cost) on buying products to sell in the future for a larger amount of money Y, meaning your profit will be P = Y - X. X is not lost money, without spending X you will not be able to make any profit P. The profit P is comparable to the active power, the earnings Y are the equivalent of apparent power and the initial cost X is the reactive power. Therefore, for a given power supply (kVA): The more cost you have (higher percentage of kVAR), the lower the ratio of kW (profit) to kVA (profit + cost), meaning a poorer power factor. The less cost you have (lower percentage of kVAR), the higher your ratio of kW (profit) to kVA (profit + cost) becomes, and the better you power factor. As your cost (kVAR) approaches zero, your power factor approaches 1 (unity). Voltage Instability A stable voltage is when every piece of equipment connected to a network is operating under normal condition without issues, however when a fault or disturbance (harmonics) occurs in this system, the voltage becomes unstable. Due to voltage instability, the electrical system's voltage may collapse, if the voltage is below acceptable limits. Voltage collapse may be a total or partial black, the terms voltage instability and voltage collapse are interchangeable. For example, if 10 generators are running to keep 10 machines working. Suddenly 3 of the generators run out of fuel, but the 10 machines keep going. This would cause a loss of generation, not being able to maintain the power required to keep all the machines working and consequentially since there is not enough power to share between any of the machines, all 10 machines will turn off, causing a total blackout. Capacitor Calculator - Python Correcting power factor from a lagging (\\<1) power factor, can be as simple as reducing reactive power (kVAR) in the system such that the ratio of real power (kW) to apparent power (kVA) is still as close to unity (1) as possible. Since motors require inductive or lagging power for magnetizing before useful work beings, this brings makes the power factor of the system lagging (\\<1). Capacitors provide capacitive or leading reactive power that cancels out the lagging power when used for power-factor improvement. The improved power factor changes the current requirements of the system, but not the one required by the motor. Using these formulas we can calculate just how big of a capacitor we require: Once we input all these required formulas, and our initial data points, we are now able to easily compute the required size of capacitor to amend power factor issues. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 from math import sqrt , pi real_power = 2.2 #Real power in kW current = 10 #Current in amps voltage = 240 #Voltage in volts frequency = 50 #Frequency in hertz corrected_pf = 0.95 #Target power factor #Calculate current power factor and apparent power current_pf = 1000 * real_power / ( voltage * current ) S_current = ( voltage * current ) / 1000 #Power factors greater than 1 will give imaginary Q_current, alert user try : #Calculate current reactive power Q_current = sqrt ( pow ( abs ( S_current ), 2 ) - pow ( real_power , 2 )) #Calculate target apparent power S_corrected = real_power / corrected_pf #Calculate required reactive power compensation Q_corrected = sqrt ( pow ( S_corrected , 2 ) - pow ( real_power , 2 )) #Calculate size of capacitor required for reactive power Q_c = Q_current - Q_corrected C_f = 1000 * Q_c / ( 2 * pi * frequency * voltage ) #Print results to user print ( \"Current power factor {0:.3f} \" . format ( current_pf )) print ( \"Current apparent power {0:.3f} kVA\" . format ( S_current )) print ( \"Current reactive power {0:.3f} kVAR\" . format ( Q_current )) print ( \"Capacitor required {0:.3f} Farads\" . format ( C_f )) except ValueError : print ( \"Current power factor > 1\" )","tags":"Engineering","url":"https://jackmckew.dev/episode-11-power-quality-explained.html","loc":"https://jackmckew.dev/episode-11-power-quality-explained.html"},{"title":"Episode 10 - Python Package Cheat Sheet","text":"One of the biggest skills in any career path comes solely from knowing where to look and what to look for when breaking down a problem. The same principle applies for Python programming. Since there are millions of different packages out there that all serve different purposes, it is often difficult to even know if there is a package out there that will solve your problem. I will be updating this table in the future as well as I personally find more and more solutions to my problems, and hope to share this insight with everyone. I will not be including packages from Python's standard library. Package Name Description Used For Pandas pandas is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language. Data Analysis Numpy NumPy is the fundamental package for scientific computing with Python. Data Analysis SciPy SciPy (pronounced \"Sigh Pie\") is a Python-based ecosystem of open-source software for mathematics, science, and engineering. Data Analysis Matplotlib Matplotlib is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Data Visualisation Spyder Spyder is a powerful scientific environment written in Python, for Python, and designed by and for scientists, engineers and data analysts. Data Visualisation/Data Analysis Folium folium builds on the data wrangling strengths of the Python ecosystem and the mapping strengths of the Leaflet.js library. Manipulate your data in Python, then visualize it in a Leaflet map via folium. Data Visualiation Bokeh Bokeh is an interactive visualization library that targets modern web browsers for presentation. Data Visualisation Camelot Camelot is a Python library that makes it easy for anyone to extract tables from PDF files! PDF Manipulation tqdm tqdm is a lightweight package for displaying progress bars within a console General Selenium Selenium automates browsers . That's it! What you do with that power is entirely up to you. Web Automation Beautiful Soup Beautiful Soup is a Python library for pulling data out of HTML and XML files. Web Scraping Scrapy An open source and collaborative framework for extracting the data you need from websites. In a fast, simple, yet extensible way. Web Scraping Requests Requests is an elegant and simple HTTP library for Python, built for human beings. Web Interaction Flask Flask is a microframework for Python based on Werkzeug, Jinja 2 and good intentions. And before you ask: It's BSD licensed ! Web Development Django Django is a high-level Python Web framework that encourages rapid development and clean, pragmatic design. Web Development Opencv OpenCV (Open Source Computer Vision Library) is released under a BSD license and hence it's free for both academic and commercial use. Image Analysis Pygame pygame ( the library ) is a Free and Open Source python programming language library for making multimedia applications like games built on top of the excellent SDL library. Game Development Pyinstaller PyInstaller freezes (packages) Python applications into stand-alone executables, under Windows, GNU/Linux, Mac OS X, FreeBSD, Solaris and AIX. Distribution cx_freeze cx_Freeze is a set of scripts and modules for freezing Python scripts into executables Distribution PyQt5 Qt is set of cross-platform C++ libraries that implement high-level APIs for accessing many aspects of modern desktop and mobile systems. GUI Development","tags":"Python","url":"https://jackmckew.dev/episode-10-python-package-cheat-sheet.html","loc":"https://jackmckew.dev/episode-10-python-package-cheat-sheet.html"},{"title":"Episode 9 - Web Enabled Universal Remote - Part 1","text":"I have a habit of misplacing all kinds of remotes within the house, TV, air conditioner, fans, etc, and having a different remote for everything can be quite annoying at times. So I decided to re-use some leftover components from a previous project to make a web enabled universal remote. Since most existing remotes use infrared to send the signal from the remote to the device, I figured it would be simple enough to create a infrared signal 'decoder' and then use a infrared diode to then replicate this signal back to the device. Next consideration was what do hardware is needed to get this project up and running. After researching a few other DIY remote control guides on the internet, I came up with a plan to use a wifi-enabled microcontroller together with an infrared receiver and an infrared diode. After rummaging through my spare hardware box, I happened to find a spare NodeMCU (ESP8266) that I could use for this project, this brings my part list to: Wifi-Enabled Microcontroller (NodeMCU) A resistor to dampen the diode signal (100 ohm) A transistor to boost the current from the NodeMCU so the diode signal gets to the device (2N222) Infrared receiver (TSOP4136) Infrared diode (L-7113F3BT) Now before connecting the entire circuit together, one should always test that components work in an expected way. To achieve this for the infrared receiver, a basic program to interface between the receiver and the microcontroller is needed. For a basic test, an LED would light up whenever the infrared is receiving a signal. By following the circuit diagram with the corresponding code for the NodeMCU, this test for the receiver should be reproduce-able at home, please note that for other infrared receivers you will need to check the pin outs. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #define ledPin D0 //Connection at GPIO16 (D0) for the builtin LED on the NodeMCU board #define inputPin D5 //Connection at GPIO14 (D5) for the infrared receiver int val = 0 ; // variable for reading the pin status void setup () { pinMode ( ledPin , OUTPUT ); // declare LED as output pinMode ( inputPin , INPUT ); // declare Infrared sensor as input } void loop () { val = digitalRead ( inputPin ); // read input value if ( val == HIGH ) { // check if the input is HIGH digitalWrite ( ledPin , LOW ); // turn LED OFF } else { digitalWrite ( ledPin , HIGH ); // turn LED ON } } In the code above, this defines the pins that the sensor and LED are connected to, checks if the sensor is receiving a signal and then switches the builtin LED accordingly. Since the microcontroller loops without delay and an infrared remote control sends signals very quickly with delay in between, the LED only flickers when a remote is aimed at it, however proving the component works as expected. Now that we have confirmed the receiver works as expected, we have to integrate and interface the infrared diode with the microcontroller such that we are able to send the decoded signals back at the device. Since the NodeMCU can only pass a maximum current of 12mA through the GPIO pins, this will not be enough for the infrared diode peak spectrum which occurs at 20mA. To boost the current up from 12mA to 20mA+, it is best to use a simple transistor, for this project I had some 2N2222 transistors lying around so decided to use them. The following circuit diagram shows how the infrared diode, transistor and microcontroller integrate together. Since the human eye cannot see the infrared diode turning on/off, this creates a challenge for testing this component before implementing the project. I did not create a test specifically for the diode, and will test whether it works correctly later on in some function testing of the project. This completes the hardware component of this project, the next part of this project will be the software. I am planning to utilise both docker and django on my home Raspberry Pi to act as a webserver that will issue commands to the microcontroller over a network to mimic the device's remote control.","tags":"Engineering","url":"https://jackmckew.dev/episode-9-web-enabled-universal-remote-part-1.html","loc":"https://jackmckew.dev/episode-9-web-enabled-universal-remote-part-1.html"},{"title":"Episode 8 - Anaconda","text":"Python is one of my favourite languages to develop in (if you haven't noticed yet). My favourite feature of Python is how easy it is to share your work with others and integrate other's code into your own projects. However as a project grows and gets older as time goes on it can be cumbersome to keep track of hundreds of dependencies that your project relies on to work. Even more so when all of these package dependencies are also being updated and changing functionality. One elegant solution that I always use when first starting a new project is to use Anaconda ( https://www.anaconda.com/). Anaconda is a free, easy-to-install package and environment manager for Python. It is very simple to use in that when you are starting a new project, you just need to create a new environment (within the Anaconda navigator) with the python version you wish to use and then activate it. Simple as that. 1 conda create --name new_environment_name python = 3 .5 In one single line, we have just created a new environment named \"new_environment_name\" and specified that this environment will use Python version 3.5. Now to activate the environment it is as simple as typing \"activate new_environment_name\". 1 activate new_environment_name Now to see what packages are contained within our newly created environment, or to ever see what packages and their versions are listed the command is: 1 conda list Now that we have created, activated and peeked inside our newly created environment we need to add some packages that we might use! This is as simple as the command \"conda install PACKAGENAME\", for example we might want to install matplotlib, a widely used data visualization package. Installing matplotlib into our environment is done by the command: 1 conda install matplotlib You will note that when this runs, it also asks to install all the dependencies that matplotlib relies on and will also notify you later when you have more packages that some might clash and need to be upgraded/downgraded so that all packages have a common version to work with. With regards to working with certain version numbers of packages within an Anaconda environment, to install a specific version of a package, or even if you know what the minimum requirement is, can by following the table below: Constraint Specification Result Fuzzy numpy=1.11 1.11.0, 1.11.1, 1.11.2, 1.11.18, etc Exact numpy==1.11 1.11.0 Greater than or equal to \"numpy>=1.11\" 1.11.0 or higher OR \"numpy=1.11.1|1.11.3\" 1.11.1 or 1.11.3 AND \"numpy>1.8,<2\" 1.8, 1.9, not 2.0 By following these simple constraint rules, it is very easy to manage package version to maintain dependencies within your project without tearing your hair out when packages update and break your project. Another major benefit of using Anaconda to manage your project's package dependencies is that when you're developing simultaneously with other projects and you may discover some bugs and wish to share them with your colleagues. To share all the dependencies (and their respective versions) with your colleague is as easy as generating an \"environment file\" and sharing the file with them so they have exactly the same environment as you. This is done by the following command: 1 conda env export > environment.yml Similarly, if you colleague sends you their \"environment file\", the command to reproduce their environment is (Please note that the name of the environment is encoded within the first line of the .yml file): 1 conda env create -f environment.yml In summary, Anaconda can be used to easily manage packages and dependencies across a project and fast track test/bug reproduction across multiple machines seamlessly. Personally, I would always advise to use a package manager across projects no matter the size.","tags":"Python","url":"https://jackmckew.dev/episode-8-anaconda.html","loc":"https://jackmckew.dev/episode-8-anaconda.html"},{"title":"Episode 7 – Planning","text":"With 2018 coming to an end, we welcome in the new year with the first episode of Code Fridays for 2018. Continuing with the theme of things starting a new, this episode is dedicated to a major factor or stage in any type of development, planning. Planning is one of the most crucial steps when beginning to tackle a project or even a problem. One of the most effective ways to deal with a problem coming from a time or financial perspective is prevention of the problem. By considering what problems may arise in a project's lifetime, the developer or designer can implement prevention before the problem comes to fruition. Being an effective planner can help you in all walks of life. Not only can one just plan for problems that may arise in a project, one can also plan to educate themselves with the knowledge to tackle unforeseen problems within a project. With an entire new year ahead of us all, I'd like to plan on what I'd like to learn in 2019 and in turn share what I learn with you all through this blog. Containerization – integrating containerization as mentioned in Episode 6 into projects Javascript – javascript is still an enigma to me at this point, I plan to work on beginner projects and hopefully integrate some of them to enhance this website Developing, deploying and maintaining an Android Application – I am currently midway through developing an Android application, in future episodes I will write tutorials on how to develop certain elements with an Android application to make it more interactive Big Data tools – Apache arrow and Hadoop are also new to me at this point, hoping to integrate these elements into projects that I'm currently working on This is only just a small taste of what I am planning to cover at the very least on my weekly updates in this blog so stay tuned throughout the whole year!","tags":"Principles","url":"https://jackmckew.dev/episode-7-planning.html","loc":"https://jackmckew.dev/episode-7-planning.html"},{"title":"Episode 6 - Containerization","text":"Recently I was researching into ways to more efficiently and effectively distribute software and I stumbled across containerization of applications. Containerization of application is when an application is run on an OS-level virtualization without spinning up an entire virtual machine for the application. Previously the way I had been distributing software I had been developing in my preferred language (Python) was by using PyInstaller ( https://pyinstaller.readthedocs.io/en/stable/). However I was running into issues with distributing a single executable throughout users, although since the software was only used by a small userbase at this stage, I was able to continue to use PyInstaller. I started researching containerization as in the future the software I will be developing will be used by a larger userbase. This will hopefully be more effective at managing versions and distributing updates to said userbase. Most other professionals in the software space have been constantly mentioning the use of Docker ( https://www.docker.com/), I am now integrating my projects into Docker and have had no issues thus far. By utilizing OS-level containerization this also allows the developer to run on any OS they wish. For multiple projects of mine, I had been intending to use influxDB ( https://www.influxdata.com/), however was limited to a strictly Windows only network. I see Docker as a solution to this problem, by being able to create a linux based container to run an instance of an influxDB that can be spun up within a Windows environment and communicate back to the Windows users in the network. Lastly, I'd like to wish a happy holidays to everyone reading and will be bringing more weekly content in the new year. Please do not hesitate to comment below if there is any topics/projects that you would like for me to research and write about my findings.","tags":"Software Development","url":"https://jackmckew.dev/episode-6-containerization.html","loc":"https://jackmckew.dev/episode-6-containerization.html"},{"title":"Episode 5 - Android Multi-Touch","text":"This week's episode of Code Fridays will go into detail on how to handle multi-touch inputs within Android. Firstly to handle the location on where the screen in being touched we need to create a class to handle the interaction. By creating a public class like Finger.java as can be seen below it contains 3 values: x_pos, y_pos and id. It is also useful to create a constructor so that other classes can easily construct the Finger class. 1 2 3 4 5 6 7 8 9 10 11 12 13 public class Finger { public float x_pos ; public float y_pos ; public int id ; Finger ( float init_x , float init_y , int init_id ) { x_pos = init_x ; y_pos = init_y ; id = init_id ; } } Now that we have a class to store our details on how each finger is touching the screen, we now need to interact with some base level Java. Firstly we need to extend a view within the Android application so that the application knows what boundaries to deal, in my test application, I've just used the entire screen as a view. After that an array is needed to store the data of multiple inputs touching the screen. I've used a TreeMap in this example as this allows for ease later on so that they are in order on how they were input, however this comes with a downside to this example as lifting a input in the middle of the order touched crashes the array, this will be fixed in a later episode. A paint is initialized for both the stroke paint for drawing lines between the touches and a paint for the text that is to come. Generic constructors for the view are also listed below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 public class TouchView extends View { private TreeMap < Integer , Finger > lineMap = new TreeMap <> (); @SuppressLint ( \"UseSparseArrays\" ) private HashMap < Integer , Path > fingerMap = new HashMap <> (); private Paint myPaint ; private Paint textPaint ; public TouchView ( Context context ) { super ( context ); init (); } public TouchView ( Context context , AttributeSet attrs , int defStyle ) { super ( context , attrs , defStyle ); init (); } public TouchView ( Context context , AttributeSet attrs ) { super ( context , attrs ); init (); } private void init () { myPaint = new Paint (); myPaint . setStyle ( Paint . Style . FILL_AND_STROKE ); myPaint . setStrokeWidth ( 5 ); myPaint . setColor ( Color . RED ); textPaint = new Paint (); textPaint . setTextSize ( 50 ); } Now that everything is initialized and ready to draw some graphics on the screen so that the application is interactive, now we have to interface with touch events. This is done by creating a new function within our View class, that takes in a MotionEvent on the View so that we can detect different types of touch events. Documentation on this can be found ( https://developer.android.com/training/graphics/opengl/touch#java ). 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 @SuppressLint ( \"ClickableViewAccessibility\" ) @Override public boolean onTouchEvent ( MotionEvent event ) { int action = event . getAction () & MotionEvent . ACTION_MASK ; switch ( action ) { case MotionEvent . ACTION_DOWN : { int id = event . getPointerId ( 0 ); fingerMap . put ( id , createCirPath ( event . getX (), event . getY (), id )); lineMap . put ( id , createFinger ( event . getX (), event . getY (), id )); break ; } case MotionEvent . ACTION_MOVE : { int touchCounter = event . getPointerCount (); for ( int t = 0 ; t < touchCounter ; t ++ ) { int id = event . getPointerId ( t ); fingerMap . remove ( id ); lineMap . remove ( id ); fingerMap . put ( id , createCirPath ( event . getX ( t ), event . getY ( t ), id )); lineMap . put ( id , createFinger ( event . getX ( t ), event . getY ( t ), id )); } } case MotionEvent . ACTION_POINTER_DOWN : { int id = event . getPointerId ( getIndex ( event )); fingerMap . put ( id , createCirPath ( event . getX ( getIndex ( event )), event . getY ( getIndex ( event )), getIndex ( event ))); lineMap . put ( id , createFinger ( event . getX ( getIndex ( event )), event . getY ( getIndex ( event )), getIndex ( event ))); break ; } case MotionEvent . ACTION_POINTER_UP : { int id = event . getPointerId ( getIndex ( event )); fingerMap . remove ( id ); lineMap . remove ( id ); break ; } case MotionEvent . ACTION_UP : { int id = event . getPointerId ( 0 ); fingerMap . remove ( id ); lineMap . remove ( id ); break ; } } invalidate (); return true ; } private int getIndex ( MotionEvent event ) { return ( event . getAction () & MotionEvent . ACTION_POINTER_INDEX_MASK ) >> MotionEvent . ACTION_POINTER_INDEX_SHIFT ; } private Finger createFinger ( float x , float y , int id ) { return new Finger ( x , y , id ); } Now that we've created a new Finger class inside our TreeMap by the order that the screen is touched in and we're removing that class when the screen input has been released, we are now ready to draw on the screen from our inputs. By iterating through the TreeMap, in each loop we know what the previous and what the next value in the array we can draw a circle for where the input is and a line between. This also allows us to determine whereabouts is the point in between these two points so we can write text. For this example, I've chosen to write the length of the distance between the two inputs to demonstrate that this can also be dynamic in nature. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 private Path createCirPath ( float x , float y , int id ) { Path p = new Path (); p . addCircle ( x , y , 50 , Path . Direction . CW ); return p ; } @Override protected void onDraw ( Canvas canvas ) { for ( Integer key : fingerMap . keySet ()) { Path p = fingerMap . get ( key ); canvas . drawPath ( p , myPaint ); } if ( lineMap . size () > 1 ) { Integer key = lineMap . firstKey (); for ( int i = 0 ; i < lineMap . size (); i = i + 1 ) { Finger start_fin = lineMap . get ( key ); if ( key + 1 != lineMap . size ()) { Integer new_key = lineMap . higherKey ( key ); Finger end_fin = lineMap . get ( new_key ); canvas . drawLine ( start_fin . x_pos , start_fin . y_pos , end_fin . x_pos , end_fin . y_pos , myPaint ); String lineText = \"Length: \" + new DecimalFormat ( \"#.##\" ). format ( Math . sqrt ( Math . pow ( end_fin . x_pos - start_fin . x_pos , 2 ) + Math . pow ( end_fin . y_pos - start_fin . y_pos , 2 ))); canvas . drawText ( lineText ,(( start_fin . x_pos + end_fin . x_pos ) / 2 ), (( start_fin . y_pos + end_fin . y_pos ) / 2 ), textPaint ); key = new_key ; } } } } In summary, it is quite simple to develop multi-touch interactions between the user and the application to enhance usability and interactivity. This is part of a application that I am developing at the moment and hope to share more insights into development as I progress on.","tags":"Android","url":"https://jackmckew.dev/episode-5-android-multi-touch.html","loc":"https://jackmckew.dev/episode-5-android-multi-touch.html"},{"title":"Episode 4 - Visualization","text":"In an ever growing world of data, every person perceives data in their own personalized way. This calls for data analysis to be visualized in a clear straightforward way so that it is accessible by anyone may come into contact with the system. By further making the data analysis system interactive, this adds an extreme amount of personalization to the analysis. Allowing the user to interact with the data set in their own way. With the help of python, it was simple to create an interactive map from a data set containing geographic co-ordinates allowing users to visually determine where they would like to select their data set from. This can then be embedding into any web browser or mobile device allowing for extreme flexibility and interactivity.","tags":"Python","url":"https://jackmckew.dev/episode-4-visualization.html","loc":"https://jackmckew.dev/episode-4-visualization.html"},{"title":"Episode 3 - Open Mind","text":"While it always may seem to be easiest to keep using what you've always used in the past, sometimes it pays off to keep an open mind about how you approach problems. Recently was asked to create a database with minute interval data from 600-700 data recording stations for up to the past 60 years, truly a lot of a data to handle. My first pass over was to use the python pandas module, with great success, however iterating over the data sets took around a week. By looking out for new ways to tackle problems, I was able to increase the speed 450 times faster by using dask to parallelize my data frames and multiprocessing allowing multiple workers to work across many cores of the PC. This meant going from around 60,000 rows per second to 1.5 million rows/second and 18 workers at one time. For the next version I am planning to investigate how to utilize influxDB and Apache Spark/Hadoop to try and optimize this process further.","tags":"Python","url":"https://jackmckew.dev/episode-3-open-mind.html","loc":"https://jackmckew.dev/episode-3-open-mind.html"},{"title":"Episode 2 - Kew-It","text":"Yesterday, I submitted my Electrical Engineering honours thesis. My project consisted of creating a hardware/software solution to schedule appliances in home to minimize energy costs through time of use pricing. The hardware is a \"black box\" that monitors power usage of appliances and logs this data through Wi-Fi to a database hosted locally. The software utilized an multi-objective evolutionary algorithm to then determine what the most beneficial time for each of the appliances to run. By using python for these computations, directly when the results are determined, a control strategy sends control messages back out to the \"black boxes\" to control the appliances automatically. By scheduling appliances in this manner, showed up to 50% reduction in cost of energy daily. As this can be scaled to any size of implementation, this project could show significant savings in cost of energy for any building/business. The project has an estimated payback period of 5 months, comparable to that of solar with 4-5 years.","tags":"Engineering","url":"https://jackmckew.dev/episode-2-kew-it.html","loc":"https://jackmckew.dev/episode-2-kew-it.html"},{"title":"Episode 1 - Optimization","text":"Recently I had to opportunity to optimize some workflows that involved heavy data processing, before the users were completing calculations/statistics by hand on up to 10 million rows in Excel, causing many complications (and crashes). With the use of Python this data analysis has been reduced to a matter of seconds speeding up workflows in some cases down from a whole working day to a matter of seconds allowing users to work on more important tasks and almost eliminating risk of human error.","tags":"Python","url":"https://jackmckew.dev/episode-1-optimization.html","loc":"https://jackmckew.dev/episode-1-optimization.html"}]};