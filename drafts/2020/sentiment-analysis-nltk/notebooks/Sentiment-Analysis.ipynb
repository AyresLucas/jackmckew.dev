{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis in Python with Vader\n",
    "\n",
    "Sentiment analysis is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Essentially just trying to judge the amount of emotion from the written words & determine what type of emotion. This post we'll go into how to do this with Python and specifically the package Vader <https://github.com/cjhutto/vaderSentiment>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\jackm\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\jackm\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import typing\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "(10605, 1)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "                                                      0\n5911  ...its stupidities wind up sticking in one's m...\n9945  Been there, done that, liked it much better th...\n7685  'Dragonfly' is a movie about a bus wreck that ...\n7081  Scorsese doesn't give us a character worth giv...\n744                       ...an inviting piece of film.",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5911</th>\n      <td>...its stupidities wind up sticking in one's m...</td>\n    </tr>\n    <tr>\n      <th>9945</th>\n      <td>Been there, done that, liked it much better th...</td>\n    </tr>\n    <tr>\n      <th>7685</th>\n      <td>'Dragonfly' is a movie about a bus wreck that ...</td>\n    </tr>\n    <tr>\n      <th>7081</th>\n      <td>Scorsese doesn't give us a character worth giv...</td>\n    </tr>\n    <tr>\n      <th>744</th>\n      <td>...an inviting piece of film.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Read in data here\n",
    "# https://nlp.stanford.edu/sentiment/code.html\n",
    "text_data = pd.read_table('original_rt_snippets.txt',header=None)\n",
    "display(text_data.shape)\n",
    "display(text_data.sample(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import english stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stopcorpus: typing.List = stopwords.words('english')\n",
    "print(stopcorpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase, and remove stop words\n",
    "\n",
    "def remove_links(text):\n",
    "    import re\n",
    "    return re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "text_data['translated_full_text'] = text_data['translated_full_text'].astype(str).apply(remove_links)\n",
    "\n",
    "def style_text(text:str):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_words(text_data:str,list_of_words_to_remove: typing.List):\n",
    "    return [item for item in text_data if item not in list_of_words_to_remove]\n",
    "\n",
    "text_data['cleaned_text'] = text_data['translated_full_text'].astype(str).apply(style_text)\n",
    "\n",
    "text_data['cleaned_text'] = text_data['translated_full_text'].astype(str).apply(lambda x: remove_words(x.split(),stopcorpus))\n",
    "\n",
    "def collapse_list_to_string(string_list):\n",
    "    return ' '.join(string_list)\n",
    "\n",
    "text_data['cleaned_text'] = text_data['cleaned_text'].apply(collapse_list_to_string)\n",
    "\n",
    "display(text_data['cleaned_text'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize cleaned text (stem words)\n",
    "\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "text_data['clean_lemmatized'] = text_data['cleaned_text'].astype(str).apply(lemmatize_text)\n",
    "\n",
    "text_data['clean_lemmatized'] = text_data['clean_lemmatized'].apply(collapse_list_to_string)\n",
    "\n",
    "display(text_data['clean_lemmatized'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.DataFrame(text_data[['full_text','clean_lemmatized','translated_full_text']].drop_duplicates())\n",
    "display(output_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "sid_analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(text:str, analyser,desired_type:str='pos'):\n",
    "    # Get sentiment from text\n",
    "    sentiment_score = analyser.polarity_scores(text)\n",
    "    return sentiment_score[desired_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Sentiment scores\n",
    "def get_sentiment_scores(df,data_column):\n",
    "    df[f'{data_column} Positive Sentiment Score'] = df[data_column].astype(str).apply(lambda x: get_sentiment(x,sid_analyzer,'pos'))\n",
    "    df[f'{data_column} Negative Sentiment Score'] = df[data_column].astype(str).apply(lambda x: get_sentiment(x,sid_analyzer,'neg'))\n",
    "    df[f'{data_column} Neutral Sentiment Score'] = df[data_column].astype(str).apply(lambda x: get_sentiment(x,sid_analyzer,'neu'))\n",
    "    df[f'{data_column} Compound Sentiment Score'] = df[data_column].astype(str).apply(lambda x: get_sentiment(x,sid_analyzer,'compound'))\n",
    "    return df\n",
    "\n",
    "output_df = get_sentiment_scores(output_df,'translated_full_text')\n",
    "output_df = get_sentiment_scores(output_df,'clean_lemmatized')\n",
    "display(output_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_excel(f'{location_name}-Sentiment-Analysis.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.plot.hist(subplots=True,grid=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{location_name}-sentiment-histograms.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "exclude_words = [\"http\",\"https\",\"error\"]\n",
    "\n",
    "exclude_wordWs.extend(stopcorpus)\n",
    "\n",
    "output_df['wordcloud'] = output_df['translated_full_text'].astype(str).apply(remove_links)\n",
    "\n",
    "output_df['wordcloud'] = output_df['wordcloud'].astype(str).apply(lambda x: x.lower())\n",
    "\n",
    "def remove_apostrophes(text):\n",
    "    text = text.replace(\"'\", \"\")\n",
    "    text = text.replace('\"', \"\")E\n",
    "    return text\n",
    "\n",
    "output_df['wordcloud'] = output_df['wordcloud'].astype(str).apply(remove_apostrophes)\n",
    "\n",
    "output_df['wordcloud'] = output_df['wordcloud'].astype(str).apply(lambda x: remove_words(x.split(),exclude_words))\n",
    "\n",
    "output_df['wordcloud'] = output_df['wordcloud'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "display(output_df['wordcloud'])\n",
    "\n",
    "wordcloud = WordCloud().generate(' '.join(output_df['wordcloud'].astype(str)))\n",
    "\n",
    "wordcloud.to_file(f\"{location_name}-wordcloud.png\")\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "\n",
    "plt.axis(\"off\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38164bitenvvenvcbacb4172fbf4d58a3add57776270394",
   "display_name": "Python 3.8.1 64-bit ('.env': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}