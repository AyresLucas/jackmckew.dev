<!DOCTYPE html>
<html lang="english">
<head>
          <title>Jack McKew's Blog - Linear Regression: Under the Hood with the Normal Equation</title>
        <meta charset="utf-8" />




    <meta name="tags" content="python" />

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://jackmckew.dev/">Jack McKew's Blog <strong>Python enthusiast, electrical engineer and tinkerer</strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
            <li><a href="/archives.html">Archives</a></li>
            <li><a href="/categories.html">Categories</a></li>
            <li><a href="/tags.html">Tags</a></li>
            <li><a href="/sitemap.xml">Sitemap</a></li>
            <li><a href="https://jackmckew.dev/pages/contact.html">Contact</a></li>
            <li><a href="https://jackmckew.dev/pages/cv-professional.html">CV/Professional</a></li>
            <li><a href="https://jackmckew.dev/category/android.html">Android</a></li>
            <li><a href="https://jackmckew.dev/category/book-reviews.html">Book Reviews</a></li>
            <li class="active"><a href="https://jackmckew.dev/category/data-science.html">Data Science</a></li>
            <li><a href="https://jackmckew.dev/category/engineering.html">Engineering</a></li>
            <li><a href="https://jackmckew.dev/category/engineering-python.html">Engineering, Python</a></li>
            <li><a href="https://jackmckew.dev/category/machine-learning.html">Machine Learning</a></li>
            <li><a href="https://jackmckew.dev/category/principles.html">Principles</a></li>
            <li><a href="https://jackmckew.dev/category/python.html">Python</a></li>
            <li><a href="https://jackmckew.dev/category/python-data-science.html">Python, Data Science</a></li>
            <li><a href="https://jackmckew.dev/category/python-engineering.html">Python, Engineering</a></li>
            <li><a href="https://jackmckew.dev/category/python-principles.html">Python, Principles</a></li>
            <li><a href="https://jackmckew.dev/category/software-development.html">Software Development</a></li>
            <li><a href="https://jackmckew.dev/category/software-development-data-science.html">Software Development, Data Science</a></li>
        </ul></nav><!-- /#menu -->
<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="https://jackmckew.dev/linear-regresssion-under-the-hood-with-the-normal-equation.html" rel="bookmark"
         title="Permalink to Linear Regression: Under the Hood with the Normal Equation">Linear Regression: Under the Hood with the Normal Equation</a></h2>
 
  </header>
  <footer class="post-info">
    <time class="published" datetime="2019-08-25T06:30:00+10:00">
      Sun 25 August 2019
    </time>
    <address class="vcard author">
      By           <a class="url fn" href="https://jackmckew.dev/author/jack-mckew.html">Jack McKew</a>
    </address>
    <div class="category">
        Category: <a href="https://jackmckew.dev/category/data-science.html">Data Science</a>
    </div>
    <div class="tags">
        Tags:
            <a href="https://jackmckew.dev/tag/python.html">python</a>
    </div>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    <p>Let's dive deeper into how linear regression works.</p>
<p>Linear regression follows a general formula:
$$
\hat{y} = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n
$$
Where $\hat{y}$ is the predicted value, $n$ is the number of features, $x_i$ is the $i^{th}$ feature value and $\theta_n$ is the $n^{th}$ model parameter. This function is then vectorised which speeds up processing on a CPU, however, I won't go into that further.</p>
<p>How does the linear regression model get 'trained'? Training a linear regression model means setting the parameters such that the model best fits the training data set. To be able to do this, we need to be able to measure how good (or bad) the model fits the data. Common ways of measuring this are:</p>
<ul>
<li>Root Mean Square Error (RMSE)</li>
<li>Mean Absolute Error (MAE)</li>
<li>R-Squared</li>
<li>Adjusted R-Squared</li>
<li>many others</li>
</ul>
<p>From here on, we will refer to these as the cost function, and the objective is to minimise the cost function.</p>
<h2>The Normal Equation</h2>
<p>To find the value of $\theta$ that minimises the cost function, there is a mathematical equation that gives the result directly, named the Normal Equation
$$
\hat{\theta} = (X^T \cdot X)^{-1}\cdot X^T \cdot y
$$
Where $\hat{\theta}$ is the value of $\theta$ that minimises the cost function and $y$ (once vectorised) is the vector of target values containing $y^{(1)}$ to $y^{(m)}$. </p>
<p>For example if this equation was run on data generated from this formula:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>

<span class="n">X</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p><img alt="10_6_2_gen" src="..\img\hands-on-machine-learning-chapter-4\10_6_2_gen.png"></p>
<p>Now to compute $\hat{\theta}$ with the normal equation, we can use the inv() function from NumPy's Linear algebra module:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="n">X_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">X</span><span class="p">]</span>
<span class="n">theta_best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_b</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_b</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_b</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>With the actual function being $y = 6 + 2x_0 + noise$, and the equation found:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="n">array</span><span class="p">([[</span> <span class="mf">5.96356419</span><span class="p">],</span>
       <span class="p">[</span> <span class="mf">2.00027727</span><span class="p">]])</span>
</pre></div>
</td></tr></table>

<p>Since the noise makes it impossible to recover the exact parameters of the original function now we can use $\hat{\theta}$ to make predictions:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="n">y_predict</span> <span class="o">=</span> <span class="n">X_new_b</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta_best</span><span class="p">)</span>
</pre></div>
</td></tr></table>

<p>With y_predict being:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="p">[[</span> <span class="mf">5.96356419</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">9.96411873</span><span class="p">]]</span>
</pre></div>
</td></tr></table>

<p><img alt="10_6_2_gen_solved" src="..\img\hands-on-machine-learning-chapter-4\10_6_2_gen_solved.png"></p>
<p>The equivalent code using Scikit-Learn would look like:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4
5</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="n">lin_reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lin_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lin_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lin_reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">lin_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">))</span>
</pre></div>
</td></tr></table>

<p>And it finds:</p>
<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="p">[</span> <span class="mf">5.96356419</span><span class="p">]</span> <span class="p">[[</span> <span class="mf">2.00027727</span><span class="p">]]</span>
<span class="p">[[</span> <span class="mf">5.96356419</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">9.96411873</span><span class="p">]]</span>
</pre></div>
</td></tr></table>

<p>Using the normal equation to train your linear regression model is linear in regards to the number of instances you wish to train on, meaning you will need to be able to fit the data set in memory.</p>
<p>There are many other ways of to train a linear regression, some which are better suited for large number of features, these will be covered in later posts.</p>
  </div><!-- /.entry-content -->
</section>
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>