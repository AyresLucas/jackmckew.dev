
<!DOCTYPE html>
<html lang="english">
<head>
  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="HandheldFriendly" content="True" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta name="robots" content="" />

  <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro|Source+Sans+Pro:300,400,400i,700" rel="stylesheet">

    <link rel="stylesheet" type="text/css" href="https://jmckew.com/theme/stylesheet/style.min.css">

  <link rel="stylesheet" type="text/css" href="https://jmckew.com/theme/pygments/github.min.css">
  <link rel="stylesheet" type="text/css" href="https://jmckew.com/theme/font-awesome/css/fontawesome.css">
  <link rel="stylesheet" type="text/css" href="https://jmckew.com/theme/font-awesome/css/brands.css">
  <link rel="stylesheet" type="text/css" href="https://jmckew.com/theme/font-awesome/css/solid.css">


    <link href="https://jmckew.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Jack McKew's Blog Atom">


    <link rel="shortcut icon" href="content/img/favicon.ico" type="image/x-icon">
    <link rel="icon" href="content/img/favicon.ico" type="image/x-icon">



<meta name="author" content="Jack McKew" />
<meta name="description" content="Let&#39;s dive deeper into how linear regression works. Linear Regression Linear regression follows a general formula: $$ \hat{y} = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n $$ Where $\hat{y}$ is the predicted value, $n$ is the number of features, $x_i$ is the $i^{th}$ feature value and $\theta_n$ is the $n^{th}$ model …" />
<meta name="keywords" content="python">


<meta property="og:site_name" content="Jack McKew's Blog"/>
<meta property="og:title" content="Linear Regression: Under the Hood with the Normal Equation"/>
<meta property="og:description" content="Let&#39;s dive deeper into how linear regression works. Linear Regression Linear regression follows a general formula: $$ \hat{y} = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n $$ Where $\hat{y}$ is the predicted value, $n$ is the number of features, $x_i$ is the $i^{th}$ feature value and $\theta_n$ is the $n^{th}$ model …"/>
<meta property="og:locale" content="en_US"/>
<meta property="og:url" content="https://jmckew.com/posts/linear-regresssion-under-the-hood-with-the-normal-equation/"/>
<meta property="og:type" content="article"/>
<meta property="article:published_time" content="2019-08-25 06:30:00+10:00"/>
<meta property="article:modified_time" content=""/>
<meta property="article:author" content="https://jmckew.com/author/jack-mckew.html">
<meta property="article:section" content="misc"/>
<meta property="article:tag" content="python"/>
<meta property="og:image" content="img/jm-photo.jpg">

  <title>Jack McKew's Blog &ndash; Linear Regression: Under the Hood with the Normal Equation</title>

</head>
<body>
  <aside>
    <div>
      <a href="https://jmckew.com">
        <img src="img/jm-photo.jpg" alt="" title="">
      </a>
      <h1><a href="https://jmckew.com"></a></h1>

<p>Python enthusiast, electrical engineer and tinkerer</p>
      <nav>
        <ul class="list">

            <li><a target="_blank" href="https://jmckew.com/pages/contact.html#contact">Contact</a></li>
            <li><a target="_blank" href="https://jmckew.com/pages/cv-professional.html#cv-professional">CV/Professional</a></li>

        </ul>
      </nav>

      <ul class="social">
          <li>
            <a  class="sc-twitter" href="https://twitter.com/Jac_McQ" target="_blank">
            <i class="fab fa-twitter">
            </i>
          </a></li>
          <li>
            <a  class="sc-linkedin" href="https://www.linkedin.com/in/jack-mckew/" target="_blank">
            <i class="fab fa-linkedin">
            </i>
          </a></li>
          <li>
            <a  class="sc-github" href="https://github.com/JackMcKew" target="_blank">
            <i class="fab fa-github">
            </i>
          </a></li>
      </ul>
    </div>


  </aside>
  <main>

    <nav>
      <a href="https://jmckew.com">    Home
</a>

      <a href="/archives.html">Archives</a>
      <a href="/tags.html">Tags</a>
      <a href="/sitemap.xml">Sitemap</a>

      <a href="https://jmckew.com/feeds/all.atom.xml">    Atom
</a>

    </nav>

<article class="single">
  <header>
      
    <h1 id="linear-regresssion-under-the-hood-with-the-normal-equation">Linear Regression: Under the Hood with the Normal Equation</h1>
    <p>
          Posted on Sun 25 August 2019 in <a href="https://jmckew.com/category/misc.html">misc</a>


        &#8226; 2 min read
    </p>
  </header>


  <div>
    <body><p>Let's dive deeper into how linear regression works.</p>
<h2>Linear Regression</h2>
<p>Linear regression follows a general formula:
$$
\hat{y} = \theta_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n
$$
Where $\hat{y}$ is the predicted value, $n$ is the number of features, $x_i$ is the $i^{th}$ feature value and $\theta_n$ is the $n^{th}$ model parameter. This function is then vectorised which speeds up processing on a CPU, however, I won't go into that further.</p>
<p>How does the linear regression model get 'trained'? Training a linear regression model means setting the parameters such that the model best fits the training data set. To be able to do this, we need to be able to measure how good (or bad) the model fits the data. Common ways of measuring this are:</p>
<ul>
<li>Root Mean Square Error (RMSE)</li>
<li>Mean Absolute Error (MAE)</li>
<li>R-Squared</li>
<li>Adjusted R-Squared</li>
<li>many others</li>
</ul>
<p>From here on, we will refer to these as the cost function, and the objective is to minimise the cost function.</p>
<h2>The Normal Equation</h2>
<p>To find the value of $\theta$ that minimises the cost function, there is a mathematical equation that gives the result directly, named the Normal Equation
$$
\hat{\theta} = (X^T \cdot X)^{-1}\cdot X^T \cdot y
$$
Where $\hat{\theta}$ is the value of $\theta$ that minimises the cost function and $y$ (once vectorised) is the vector of target values containing $y^{(1)}$ to $y^{(m)}$. </p>
<p>For example if this equation was run on data generated from this formula:</p>
<div class="highlight"><pre><span class="code-line"><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span></span>
<span class="code-line"></span>
<span class="code-line"><span class="n">X</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span></span>
<span class="code-line"><span class="n">y</span> <span class="o">=</span> <span class="mi">6</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span></span>
</pre></div>
<p><img alt="10_6_2_gen" src="..\img\hands-on-machine-learning-chapter-4\10_6_2_gen.png"/></p>
<p>Now to compute $\hat{\theta}$ with the normal equation, we can use the inv() function from NumPy's Linear algebra module:</p>
<div class="highlight"><pre><span class="code-line"><span></span><span class="n">X_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">100</span><span class="p">,</span><span class="mi">1</span><span class="p">)),</span><span class="n">X</span><span class="p">]</span></span>
<span class="code-line"><span class="n">theta_best</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_b</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_b</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_b</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span></span>
</pre></div>
<p>With the actual function being $y = 6 + 2x_0 + noise$, and the equation found:</p>
<div class="highlight"><pre><span class="code-line"><span></span><span class="n">array</span><span class="p">([[</span> <span class="mf">5.96356419</span><span class="p">],</span></span>
<span class="code-line">       <span class="p">[</span> <span class="mf">2.00027727</span><span class="p">]])</span></span>
</pre></div>
<p>Since the noise makes it impossible to recover the exact parameters of the original function now we can use $\hat{\theta}$ to make predictions:</p>
<div class="highlight"><pre><span class="code-line"><span></span><span class="n">y_predict</span> <span class="o">=</span> <span class="n">X_new_b</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">theta_best</span><span class="p">)</span></span>
</pre></div>
<p>With y_predict being:</p>
<div class="highlight"><pre><span class="code-line"><span></span><span class="p">[[</span> <span class="mf">5.96356419</span><span class="p">]</span></span>
<span class="code-line"> <span class="p">[</span> <span class="mf">9.96411873</span><span class="p">]]</span></span>
</pre></div>
<p><img alt="10_6_2_gen_solved" src="..\img\hands-on-machine-learning-chapter-4\10_6_2_gen_solved.png"/></p>
<p>The equivalent code using Scikit-Learn would look like:</p>
<div class="highlight"><pre><span class="code-line"><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span></span>
<span class="code-line"><span class="n">lin_reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span></span>
<span class="code-line"><span class="n">lin_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span></span>
<span class="code-line"><span class="k">print</span><span class="p">(</span><span class="n">lin_reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">,</span> <span class="n">lin_reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span></span>
<span class="code-line"><span class="k">print</span><span class="p">(</span><span class="n">lin_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_new</span><span class="p">))</span></span>
</pre></div>
<p>And it finds:</p>
<div class="highlight"><pre><span class="code-line"><span></span><span class="p">[</span> <span class="mf">5.96356419</span><span class="p">]</span> <span class="p">[[</span> <span class="mf">2.00027727</span><span class="p">]]</span></span>
<span class="code-line"><span class="p">[[</span> <span class="mf">5.96356419</span><span class="p">]</span></span>
<span class="code-line"> <span class="p">[</span> <span class="mf">9.96411873</span><span class="p">]]</span></span>
</pre></div>
<p>Using the normal equation to train your linear regression model is linear in regards to the number of instances you wish to train on, meaning you will need to be able to fit the data set in memory.</p>
<p>There are many other ways of to train a linear regression, some which are better suited for large number of features, these will be covered in later posts.</p></body>
  </div>
  <div class="tag-cloud">
    <p>
      <a href="https://jmckew.com/tag/python.html">python</a>
    </p>
  </div>





</article>

    <footer>
<p>&copy;  </p>
<p>    Built with <a href="http://getpelican.com" target="_blank">Pelican</a> using <a href="http://bit.ly/flex-pelican" target="_blank">Flex</a> theme
</p>    </footer>
  </main>




<script type="application/ld+json">
{
  "@context" : "http://schema.org",
  "@type" : "Blog",
  "name": " Jack McKew's Blog ",
  "url" : "https://jmckew.com",
  "image": "img/jm-photo.jpg",
  "description": ""
}
</script>

</body>
</html>